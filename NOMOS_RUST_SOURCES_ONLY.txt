# Nomos Testing Framework — Rust sources only — Wed Nov 26 12:50:34 CET 2025

════════════════════════════════════════════════
FILE: testing-framework/configs/src/common/kms.rs
────────────────────────────────────────────────
use groth16::fr_to_bytes;
use key_management_system::{
    backend::preload::KeyId,
    keys::{Key, secured_key::SecuredKey as _},
};

#[must_use]
pub fn key_id_for_preload_backend(key: &Key) -> KeyId {
    let key_id_bytes = match key {
        Key::Ed25519(ed25519_secret_key) => ed25519_secret_key.as_public_key().to_bytes(),
        Key::Zk(zk_secret_key) => fr_to_bytes(zk_secret_key.as_public_key().as_fr()),
    };
    hex::encode(key_id_bytes)
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/common/mod.rs
────────────────────────────────────────────────
pub mod kms;




════════════════════════════════════════════════
FILE: testing-framework/configs/src/lib.rs
────────────────────────────────────────────────
use std::{env, net::Ipv4Addr, ops::Mul as _, sync::LazyLock, time::Duration};

use nomos_core::sdp::ProviderId;
use nomos_libp2p::{Multiaddr, PeerId, multiaddr};

pub mod common;
pub mod nodes;
pub mod topology;

static IS_SLOW_TEST_ENV: LazyLock<bool> =
    LazyLock::new(|| env::var("SLOW_TEST_ENV").is_ok_and(|s| s == "true"));

pub static IS_DEBUG_TRACING: LazyLock<bool> = LazyLock::new(|| {
    env::var("NOMOS_TESTS_TRACING").is_ok_and(|val| val.eq_ignore_ascii_case("true"))
});

/// In slow test environments like Codecov, use 2x timeout.
#[must_use]
pub fn adjust_timeout(d: Duration) -> Duration {
    if *IS_SLOW_TEST_ENV { d.mul(2) } else { d }
}

#[must_use]
pub fn node_address_from_port(port: u16) -> Multiaddr {
    multiaddr(Ipv4Addr::LOCALHOST, port)
}

#[must_use]
pub fn secret_key_to_peer_id(node_key: nomos_libp2p::ed25519::SecretKey) -> PeerId {
    PeerId::from_public_key(
        &nomos_libp2p::ed25519::Keypair::from(node_key)
            .public()
            .into(),
    )
}

#[must_use]
pub fn secret_key_to_provider_id(node_key: nomos_libp2p::ed25519::SecretKey) -> ProviderId {
    ProviderId::try_from(
        nomos_libp2p::ed25519::Keypair::from(node_key)
            .public()
            .to_bytes(),
    )
    .unwrap()
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/nodes/executor.rs
────────────────────────────────────────────────
use std::{
    collections::HashSet,
    num::{NonZeroU64, NonZeroUsize},
    path::PathBuf,
    time::Duration,
};

use chain_leader::LeaderSettings;
use chain_network::{ChainNetworkSettings, OrphanConfig, SyncConfig};
use chain_service::{CryptarchiaSettings, StartingState};
use cryptarchia_engine::time::SlotConfig;
use key_management_system::keys::{Key, ZkKey};
use nomos_blend_service::{
    core::settings::{CoverTrafficSettings, MessageDelayerSettings, SchedulerSettings, ZkSettings},
    settings::TimingSettings,
};
use nomos_da_dispersal::{
    DispersalServiceSettings,
    backend::kzgrs::{DispersalKZGRSBackendSettings, EncoderSettings},
};
use nomos_da_network_core::protocols::sampling::SubnetsConfig;
use nomos_da_network_service::{
    NetworkConfig as DaNetworkConfig,
    api::http::ApiAdapterSettings,
    backends::libp2p::{
        common::DaNetworkBackendSettings, executor::DaNetworkExecutorBackendSettings,
    },
};
use nomos_da_sampling::{
    DaSamplingServiceSettings, backend::kzgrs::KzgrsSamplingBackendSettings,
    verifier::kzgrs::KzgrsDaVerifierSettings as SamplingVerifierSettings,
};
use nomos_da_verifier::{
    DaVerifierServiceSettings,
    backend::{kzgrs::KzgrsDaVerifierSettings, trigger::MempoolPublishTriggerConfig},
    storage::adapters::rocksdb::RocksAdapterSettings as VerifierStorageAdapterSettings,
};
use nomos_executor::config::Config as ExecutorConfig;
use nomos_node::{
    RocksBackendSettings,
    api::backend::AxumBackendSettings as NodeAxumBackendSettings,
    config::{
        blend::{
            deployment::{self as blend_deployment},
            serde as blend_serde,
        },
        deployment::{CustomDeployment, Settings as NodeDeploymentSettings},
        mempool::MempoolConfig,
        network::deployment::Settings as NetworkDeploymentSettings,
    },
};
use nomos_sdp::SdpSettings;
use nomos_time::{
    TimeServiceSettings,
    backends::{NtpTimeBackendSettings, ntp::async_client::NTPClientSettings},
};
use nomos_utils::math::NonNegativeF64;
use nomos_wallet::WalletServiceSettings;

use crate::{
    adjust_timeout,
    common::kms::key_id_for_preload_backend,
    topology::configs::{
        GeneralConfig, blend::GeneralBlendConfig as TopologyBlendConfig, wallet::WalletAccount,
    },
};

#[must_use]
#[expect(clippy::too_many_lines, reason = "TODO: Address this at some point.")]
pub fn create_executor_config(config: GeneralConfig) -> ExecutorConfig {
    let (blend_user_config, deployment_settings) = build_blend_service_config(&config.blend_config);
    ExecutorConfig {
        network: config.network_config,
        blend: blend_user_config,
        deployment: deployment_settings,
        cryptarchia: CryptarchiaSettings {
            config: config.consensus_config.ledger_config.clone(),
            starting_state: StartingState::Genesis {
                genesis_tx: config.consensus_config.genesis_tx,
            },
            // Disable on-disk recovery in compose tests to avoid serde errors on
            // non-string keys and keep services alive.
            recovery_file: PathBuf::new(),
            bootstrap: chain_service::BootstrapConfig {
                prolonged_bootstrap_period: config.bootstrapping_config.prolonged_bootstrap_period,
                force_bootstrap: false,
                offline_grace_period: chain_service::OfflineGracePeriodConfig {
                    grace_period: Duration::from_secs(20 * 60),
                    state_recording_interval: Duration::from_secs(60),
                },
            },
        },
        chain_network: ChainNetworkSettings {
            config: config.consensus_config.ledger_config.clone(),
            network_adapter_settings:
                chain_network::network::adapters::libp2p::LibP2pAdapterSettings {
                    topic: String::from(nomos_node::CONSENSUS_TOPIC),
                },
            bootstrap: chain_network::BootstrapConfig {
                ibd: chain_network::IbdConfig {
                    peers: HashSet::new(),
                    delay_before_new_download: Duration::from_secs(10),
                },
            },
            sync: SyncConfig {
                orphan: OrphanConfig {
                    max_orphan_cache_size: NonZeroUsize::new(5)
                        .expect("Max orphan cache size must be non-zero"),
                },
            },
        },
        cryptarchia_leader: LeaderSettings {
            transaction_selector_settings: (),
            config: config.consensus_config.ledger_config.clone(),
            leader_config: config.consensus_config.leader_config.clone(),
            blend_broadcast_settings:
                nomos_blend_service::core::network::libp2p::Libp2pBroadcastSettings {
                    topic: String::from(nomos_node::CONSENSUS_TOPIC),
                },
        },
        da_network: DaNetworkConfig {
            backend: DaNetworkExecutorBackendSettings {
                validator_settings: DaNetworkBackendSettings {
                    node_key: config.da_config.node_key,
                    listening_address: config.da_config.listening_address,
                    policy_settings: config.da_config.policy_settings,
                    monitor_settings: config.da_config.monitor_settings,
                    balancer_interval: config.da_config.balancer_interval,
                    redial_cooldown: config.da_config.redial_cooldown,
                    replication_settings: config.da_config.replication_settings,
                    subnets_settings: SubnetsConfig {
                        num_of_subnets: config.da_config.num_samples as usize,
                        shares_retry_limit: config.da_config.retry_shares_limit,
                        commitments_retry_limit: config.da_config.retry_commitments_limit,
                    },
                },
                num_subnets: config.da_config.num_subnets,
            },
            membership: config.da_config.membership.clone(),
            api_adapter_settings: ApiAdapterSettings {
                api_port: config.api_config.address.port(),
                is_secure: false,
            },
            subnet_refresh_interval: config.da_config.subnets_refresh_interval,
            subnet_threshold: config.da_config.num_samples as usize,
            min_session_members: config.da_config.num_samples as usize,
        },
        da_verifier: DaVerifierServiceSettings {
            share_verifier_settings: KzgrsDaVerifierSettings {
                global_params_path: config.da_config.global_params_path.clone(),
                domain_size: config.da_config.num_subnets as usize,
            },
            tx_verifier_settings: (),
            network_adapter_settings: (),
            storage_adapter_settings: VerifierStorageAdapterSettings {
                blob_storage_directory: "./".into(),
            },
            mempool_trigger_settings: MempoolPublishTriggerConfig {
                publish_threshold: NonNegativeF64::try_from(0.8).unwrap(),
                share_duration: Duration::from_secs(5),
                prune_duration: Duration::from_secs(30),
                prune_interval: Duration::from_secs(5),
            },
        },
        tracing: config.tracing_config.tracing_settings,
        http: nomos_api::ApiServiceSettings {
            backend_settings: NodeAxumBackendSettings {
                address: config.api_config.address,
                rate_limit_per_second: 10000,
                rate_limit_burst: 10000,
                max_concurrent_requests: 1000,
                ..Default::default()
            },
        },
        da_sampling: DaSamplingServiceSettings {
            sampling_settings: KzgrsSamplingBackendSettings {
                num_samples: config.da_config.num_samples,
                num_subnets: config.da_config.num_subnets,
                old_blobs_check_interval: config.da_config.old_blobs_check_interval,
                blobs_validity_duration: config.da_config.blobs_validity_duration,
            },
            share_verifier_settings: SamplingVerifierSettings {
                global_params_path: config.da_config.global_params_path.clone(),
                domain_size: config.da_config.num_subnets as usize,
            },
            commitments_wait_duration: Duration::from_secs(1),
            sdp_blob_trigger_sampling_delay: adjust_timeout(Duration::from_secs(5)),
        },
        storage: RocksBackendSettings {
            db_path: "./db".into(),
            read_only: false,
            column_family: Some("blocks".into()),
        },
        da_dispersal: DispersalServiceSettings {
            backend: DispersalKZGRSBackendSettings {
                encoder_settings: EncoderSettings {
                    num_columns: config.da_config.num_subnets as usize,
                    with_cache: false,
                    global_params_path: config.da_config.global_params_path,
                },
                dispersal_timeout: Duration::from_secs(20),
                retry_cooldown: Duration::from_secs(3),
                retry_limit: 2,
            },
        },
        time: TimeServiceSettings {
            backend_settings: NtpTimeBackendSettings {
                ntp_server: config.time_config.ntp_server,
                ntp_client_settings: NTPClientSettings {
                    timeout: config.time_config.timeout,
                    listening_interface: config.time_config.interface,
                },
                update_interval: config.time_config.update_interval,
                slot_config: SlotConfig {
                    slot_duration: config.time_config.slot_duration,
                    chain_start_time: config.time_config.chain_start_time,
                },
                epoch_config: config.consensus_config.ledger_config.epoch_config,
                base_period_length: config.consensus_config.ledger_config.base_period_length(),
            },
        },
        mempool: MempoolConfig {
            pool_recovery_path: "./recovery/mempool.json".into(),
        },
        sdp: SdpSettings { declaration: None },
        wallet: WalletServiceSettings {
            known_keys: {
                let mut keys = HashSet::from_iter([config.consensus_config.leader_config.pk]);
                keys.extend(
                    config
                        .consensus_config
                        .wallet_accounts
                        .iter()
                        .map(WalletAccount::public_key),
                );
                keys
            },
        },
        key_management: config.kms_config,

        testing_http: nomos_api::ApiServiceSettings {
            backend_settings: NodeAxumBackendSettings {
                address: config.api_config.testing_http_address,
                rate_limit_per_second: 10000,
                rate_limit_burst: 10000,
                max_concurrent_requests: 1000,
                ..Default::default()
            },
        },
    }
}

fn build_blend_service_config(
    config: &TopologyBlendConfig,
) -> (blend_serde::Config, NodeDeploymentSettings) {
    let zk_key_id =
        key_id_for_preload_backend(&Key::from(ZkKey::new(config.secret_zk_key.clone())));

    let backend_core = &config.backend_core;
    let backend_edge = &config.backend_edge;

    let user = blend_serde::Config {
        common: blend_serde::common::Config {
            non_ephemeral_signing_key: config.private_key.clone(),
            recovery_path_prefix: PathBuf::from("./recovery/blend"),
        },
        core: blend_serde::core::Config {
            backend: blend_serde::core::BackendConfig {
                listening_address: backend_core.listening_address.clone(),
                core_peering_degree: backend_core.core_peering_degree.clone(),
                edge_node_connection_timeout: backend_core.edge_node_connection_timeout,
                max_edge_node_incoming_connections: backend_core.max_edge_node_incoming_connections,
                max_dial_attempts_per_peer: backend_core.max_dial_attempts_per_peer,
            },
            zk: ZkSettings {
                secret_key_kms_id: zk_key_id,
            },
        },
        edge: blend_serde::edge::Config {
            backend: blend_serde::edge::BackendConfig {
                max_dial_attempts_per_peer_per_message: backend_edge
                    .max_dial_attempts_per_peer_per_message,
                replication_factor: backend_edge.replication_factor,
            },
        },
    };

    let deployment_settings = blend_deployment::Settings {
        common: blend_deployment::CommonSettings {
            num_blend_layers: NonZeroU64::try_from(1).unwrap(),
            minimum_network_size: NonZeroU64::try_from(1).unwrap(),
            timing: TimingSettings {
                round_duration: Duration::from_secs(1),
                rounds_per_interval: NonZeroU64::try_from(30u64).unwrap(),
                rounds_per_session: NonZeroU64::try_from(648_000u64).unwrap(),
                rounds_per_observation_window: NonZeroU64::try_from(30u64).unwrap(),
                rounds_per_session_transition_period: NonZeroU64::try_from(30u64).unwrap(),
                epoch_transition_period_in_slots: NonZeroU64::try_from(2_600).unwrap(),
            },
            protocol_name: backend_core.protocol_name.clone(),
        },
        core: blend_deployment::CoreSettings {
            scheduler: SchedulerSettings {
                cover: CoverTrafficSettings {
                    intervals_for_safety_buffer: 100,
                    message_frequency_per_round: NonNegativeF64::try_from(1f64).unwrap(),
                },
                delayer: MessageDelayerSettings {
                    maximum_release_delay_in_rounds: NonZeroU64::try_from(3u64).unwrap(),
                },
            },
            minimum_messages_coefficient: backend_core.minimum_messages_coefficient,
            normalization_constant: backend_core.normalization_constant,
        },
    };

    let deployment = NodeDeploymentSettings::Custom(CustomDeployment {
        blend: deployment_settings,
        network: NetworkDeploymentSettings {
            identify_protocol_name: nomos_libp2p::protocol_name::StreamProtocol::new(
                "/integration/nomos/identify/1.0.0",
            ),
            kademlia_protocol_name: nomos_libp2p::protocol_name::StreamProtocol::new(
                "/integration/nomos/kad/1.0.0",
            ),
        },
    });

    (user, deployment)
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/nodes/mod.rs
────────────────────────────────────────────────
pub mod executor;
pub mod validator;




════════════════════════════════════════════════
FILE: testing-framework/configs/src/nodes/validator.rs
────────────────────────────────────────────────
use std::{
    collections::HashSet,
    num::{NonZeroU64, NonZeroUsize},
    path::PathBuf,
    time::Duration,
};

use chain_leader::LeaderSettings;
use chain_network::{ChainNetworkSettings, OrphanConfig, SyncConfig};
use chain_service::{CryptarchiaSettings, StartingState};
use cryptarchia_engine::time::SlotConfig;
use key_management_system::keys::{Key, ZkKey};
use nomos_blend_service::{
    core::settings::{CoverTrafficSettings, MessageDelayerSettings, SchedulerSettings, ZkSettings},
    settings::TimingSettings,
};
use nomos_da_network_core::{
    protocols::sampling::SubnetsConfig, swarm::DAConnectionPolicySettings,
};
use nomos_da_network_service::{
    NetworkConfig as DaNetworkConfig, api::http::ApiAdapterSettings,
    backends::libp2p::common::DaNetworkBackendSettings,
};
use nomos_da_sampling::{
    DaSamplingServiceSettings, backend::kzgrs::KzgrsSamplingBackendSettings,
    verifier::kzgrs::KzgrsDaVerifierSettings as SamplingVerifierSettings,
};
use nomos_da_verifier::{
    DaVerifierServiceSettings,
    backend::{kzgrs::KzgrsDaVerifierSettings, trigger::MempoolPublishTriggerConfig},
    storage::adapters::rocksdb::RocksAdapterSettings as VerifierStorageAdapterSettings,
};
use nomos_node::{
    Config as ValidatorConfig, RocksBackendSettings,
    api::backend::AxumBackendSettings as NodeAxumBackendSettings,
    config::{
        blend::{
            deployment::{self as blend_deployment},
            serde as blend_serde,
        },
        deployment::{CustomDeployment, Settings as NodeDeploymentSettings},
        mempool::MempoolConfig,
        network::deployment::Settings as NetworkDeploymentSettings,
    },
};
use nomos_sdp::SdpSettings;
use nomos_time::{
    TimeServiceSettings,
    backends::{NtpTimeBackendSettings, ntp::async_client::NTPClientSettings},
};
use nomos_utils::math::NonNegativeF64;
use nomos_wallet::WalletServiceSettings;

use crate::{
    adjust_timeout,
    common::kms::key_id_for_preload_backend,
    topology::configs::{
        GeneralConfig, blend::GeneralBlendConfig as TopologyBlendConfig, wallet::WalletAccount,
    },
};

#[must_use]
#[expect(
    clippy::too_many_lines,
    reason = "Validator config wiring aggregates many service settings"
)]
pub fn create_validator_config(config: GeneralConfig) -> ValidatorConfig {
    let da_policy_settings = config.da_config.policy_settings;
    let (blend_user_config, deployment_settings) = build_blend_service_config(&config.blend_config);
    ValidatorConfig {
        network: config.network_config,
        blend: blend_user_config,
        deployment: deployment_settings,
        cryptarchia: CryptarchiaSettings {
            config: config.consensus_config.ledger_config.clone(),
            starting_state: StartingState::Genesis {
                genesis_tx: config.consensus_config.genesis_tx,
            },
            // Disable on-disk recovery in compose tests to avoid serde errors on
            // non-string keys and keep services alive.
            recovery_file: PathBuf::new(),
            bootstrap: chain_service::BootstrapConfig {
                prolonged_bootstrap_period: config.bootstrapping_config.prolonged_bootstrap_period,
                force_bootstrap: false,
                offline_grace_period: chain_service::OfflineGracePeriodConfig {
                    grace_period: Duration::from_secs(20 * 60),
                    state_recording_interval: Duration::from_secs(60),
                },
            },
        },
        chain_network: ChainNetworkSettings {
            config: config.consensus_config.ledger_config.clone(),
            network_adapter_settings:
                chain_network::network::adapters::libp2p::LibP2pAdapterSettings {
                    topic: String::from(nomos_node::CONSENSUS_TOPIC),
                },
            bootstrap: chain_network::BootstrapConfig {
                ibd: chain_network::IbdConfig {
                    peers: HashSet::new(),
                    delay_before_new_download: Duration::from_secs(10),
                },
            },
            sync: SyncConfig {
                orphan: OrphanConfig {
                    max_orphan_cache_size: NonZeroUsize::new(5)
                        .expect("Max orphan cache size must be non-zero"),
                },
            },
        },
        cryptarchia_leader: LeaderSettings {
            transaction_selector_settings: (),
            config: config.consensus_config.ledger_config.clone(),
            leader_config: config.consensus_config.leader_config.clone(),
            blend_broadcast_settings:
                nomos_blend_service::core::network::libp2p::Libp2pBroadcastSettings {
                    topic: String::from(nomos_node::CONSENSUS_TOPIC),
                },
        },
        da_network: DaNetworkConfig {
            backend: DaNetworkBackendSettings {
                node_key: config.da_config.node_key,
                listening_address: config.da_config.listening_address,
                policy_settings: DAConnectionPolicySettings {
                    min_dispersal_peers: 0,
                    min_replication_peers: da_policy_settings.min_replication_peers,
                    max_dispersal_failures: da_policy_settings.max_dispersal_failures,
                    max_sampling_failures: da_policy_settings.max_sampling_failures,
                    max_replication_failures: da_policy_settings.max_replication_failures,
                    malicious_threshold: da_policy_settings.malicious_threshold,
                },
                monitor_settings: config.da_config.monitor_settings,
                balancer_interval: config.da_config.balancer_interval,
                redial_cooldown: config.da_config.redial_cooldown,
                replication_settings: config.da_config.replication_settings,
                subnets_settings: SubnetsConfig {
                    num_of_subnets: config.da_config.num_samples as usize,
                    shares_retry_limit: config.da_config.retry_shares_limit,
                    commitments_retry_limit: config.da_config.retry_commitments_limit,
                },
            },
            membership: config.da_config.membership.clone(),
            api_adapter_settings: ApiAdapterSettings {
                api_port: config.api_config.address.port(),
                is_secure: false,
            },
            subnet_refresh_interval: config.da_config.subnets_refresh_interval,
            subnet_threshold: config.da_config.num_samples as usize,
            min_session_members: config.da_config.num_samples as usize,
        },
        da_verifier: DaVerifierServiceSettings {
            share_verifier_settings: KzgrsDaVerifierSettings {
                global_params_path: config.da_config.global_params_path.clone(),
                domain_size: config.da_config.num_subnets as usize,
            },
            tx_verifier_settings: (),
            network_adapter_settings: (),
            storage_adapter_settings: VerifierStorageAdapterSettings {
                blob_storage_directory: "./".into(),
            },
            mempool_trigger_settings: MempoolPublishTriggerConfig {
                publish_threshold: NonNegativeF64::try_from(0.8).unwrap(),
                share_duration: Duration::from_secs(5),
                prune_duration: Duration::from_secs(30),
                prune_interval: Duration::from_secs(5),
            },
        },
        tracing: config.tracing_config.tracing_settings,
        http: nomos_api::ApiServiceSettings {
            backend_settings: NodeAxumBackendSettings {
                address: config.api_config.address,
                rate_limit_per_second: 10000,
                rate_limit_burst: 10000,
                max_concurrent_requests: 1000,
                ..Default::default()
            },
        },
        da_sampling: DaSamplingServiceSettings {
            sampling_settings: KzgrsSamplingBackendSettings {
                num_samples: config.da_config.num_samples,
                num_subnets: config.da_config.num_subnets,
                old_blobs_check_interval: config.da_config.old_blobs_check_interval,
                blobs_validity_duration: config.da_config.blobs_validity_duration,
            },
            share_verifier_settings: SamplingVerifierSettings {
                global_params_path: config.da_config.global_params_path,
                domain_size: config.da_config.num_subnets as usize,
            },
            commitments_wait_duration: Duration::from_secs(1),
            sdp_blob_trigger_sampling_delay: adjust_timeout(Duration::from_secs(5)),
        },
        storage: RocksBackendSettings {
            db_path: "./db".into(),
            read_only: false,
            column_family: Some("blocks".into()),
        },
        time: TimeServiceSettings {
            backend_settings: NtpTimeBackendSettings {
                ntp_server: config.time_config.ntp_server,
                ntp_client_settings: NTPClientSettings {
                    timeout: config.time_config.timeout,
                    listening_interface: config.time_config.interface,
                },
                update_interval: config.time_config.update_interval,
                slot_config: SlotConfig {
                    slot_duration: config.time_config.slot_duration,
                    chain_start_time: config.time_config.chain_start_time,
                },
                epoch_config: config.consensus_config.ledger_config.epoch_config,
                base_period_length: config.consensus_config.ledger_config.base_period_length(),
            },
        },
        mempool: MempoolConfig {
            pool_recovery_path: "./recovery/mempool.json".into(),
        },
        sdp: SdpSettings { declaration: None },
        wallet: WalletServiceSettings {
            known_keys: {
                let mut keys = HashSet::from_iter([config.consensus_config.leader_config.pk]);
                keys.extend(
                    config
                        .consensus_config
                        .wallet_accounts
                        .iter()
                        .map(WalletAccount::public_key),
                );
                keys
            },
        },
        key_management: config.kms_config,
        testing_http: nomos_api::ApiServiceSettings {
            backend_settings: NodeAxumBackendSettings {
                address: config.api_config.testing_http_address,
                rate_limit_per_second: 10000,
                rate_limit_burst: 10000,
                max_concurrent_requests: 1000,
                ..Default::default()
            },
        },
    }
}

fn build_blend_service_config(
    config: &TopologyBlendConfig,
) -> (blend_serde::Config, NodeDeploymentSettings) {
    let zk_key_id =
        key_id_for_preload_backend(&Key::from(ZkKey::new(config.secret_zk_key.clone())));

    let backend_core = &config.backend_core;
    let backend_edge = &config.backend_edge;

    let user = blend_serde::Config {
        common: blend_serde::common::Config {
            non_ephemeral_signing_key: config.private_key.clone(),
            recovery_path_prefix: PathBuf::from("./recovery/blend"),
        },
        core: blend_serde::core::Config {
            backend: blend_serde::core::BackendConfig {
                listening_address: backend_core.listening_address.clone(),
                core_peering_degree: backend_core.core_peering_degree.clone(),
                edge_node_connection_timeout: backend_core.edge_node_connection_timeout,
                max_edge_node_incoming_connections: backend_core.max_edge_node_incoming_connections,
                max_dial_attempts_per_peer: backend_core.max_dial_attempts_per_peer,
            },
            zk: ZkSettings {
                secret_key_kms_id: zk_key_id,
            },
        },
        edge: blend_serde::edge::Config {
            backend: blend_serde::edge::BackendConfig {
                max_dial_attempts_per_peer_per_message: backend_edge
                    .max_dial_attempts_per_peer_per_message,
                replication_factor: backend_edge.replication_factor,
            },
        },
    };

    let deployment_settings = blend_deployment::Settings {
        common: blend_deployment::CommonSettings {
            num_blend_layers: NonZeroU64::try_from(1).unwrap(),
            minimum_network_size: NonZeroU64::try_from(1).unwrap(),
            timing: TimingSettings {
                round_duration: Duration::from_secs(1),
                rounds_per_interval: NonZeroU64::try_from(30u64).unwrap(),
                rounds_per_session: NonZeroU64::try_from(648_000u64).unwrap(),
                rounds_per_observation_window: NonZeroU64::try_from(30u64).unwrap(),
                rounds_per_session_transition_period: NonZeroU64::try_from(30u64).unwrap(),
                epoch_transition_period_in_slots: NonZeroU64::try_from(2_600).unwrap(),
            },
            protocol_name: backend_core.protocol_name.clone(),
        },
        core: blend_deployment::CoreSettings {
            scheduler: SchedulerSettings {
                cover: CoverTrafficSettings {
                    intervals_for_safety_buffer: 100,
                    message_frequency_per_round: NonNegativeF64::try_from(1f64).unwrap(),
                },
                delayer: MessageDelayerSettings {
                    maximum_release_delay_in_rounds: NonZeroU64::try_from(3u64).unwrap(),
                },
            },
            minimum_messages_coefficient: backend_core.minimum_messages_coefficient,
            normalization_constant: backend_core.normalization_constant,
        },
    };

    let deployment = NodeDeploymentSettings::Custom(CustomDeployment {
        blend: deployment_settings,
        network: NetworkDeploymentSettings {
            identify_protocol_name: nomos_libp2p::protocol_name::StreamProtocol::new(
                "/integration/nomos/identify/1.0.0",
            ),
            kademlia_protocol_name: nomos_libp2p::protocol_name::StreamProtocol::new(
                "/integration/nomos/kad/1.0.0",
            ),
        },
    });

    (user, deployment)
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/topology/configs/api.rs
────────────────────────────────────────────────
use std::net::SocketAddr;

use nomos_utils::net::get_available_tcp_port;

#[derive(Clone)]
pub struct GeneralApiConfig {
    pub address: SocketAddr,
    pub testing_http_address: SocketAddr,
}

#[must_use]
pub fn create_api_configs(ids: &[[u8; 32]]) -> Vec<GeneralApiConfig> {
    ids.iter()
        .map(|_| GeneralApiConfig {
            address: format!("127.0.0.1:{}", get_available_tcp_port().unwrap())
                .parse()
                .unwrap(),
            testing_http_address: format!("127.0.0.1:{}", get_available_tcp_port().unwrap())
                .parse()
                .unwrap(),
        })
        .collect()
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/topology/configs/blend.rs
────────────────────────────────────────────────
use core::time::Duration;
use std::{num::NonZeroU64, str::FromStr as _};

use ed25519_dalek::SigningKey;
use nomos_blend_message::crypto::keys::Ed25519PrivateKey;
use nomos_blend_service::{
    core::backends::libp2p::Libp2pBlendBackendSettings as Libp2pCoreBlendBackendSettings,
    edge::backends::libp2p::Libp2pBlendBackendSettings as Libp2pEdgeBlendBackendSettings,
};
use nomos_libp2p::{Multiaddr, protocol_name::StreamProtocol};
use num_bigint::BigUint;
use zksign::SecretKey;

#[derive(Clone)]
pub struct GeneralBlendConfig {
    pub backend_core: Libp2pCoreBlendBackendSettings,
    pub backend_edge: Libp2pEdgeBlendBackendSettings,
    pub private_key: Ed25519PrivateKey,
    pub secret_zk_key: SecretKey,
    pub signer: SigningKey,
}

/// Builds blend configs for each node.
///
/// # Panics
///
/// Panics if the provided port strings cannot be parsed into valid `Multiaddr`s
/// or if any of the numeric blend parameters are zero, which would make the
/// libp2p configuration invalid.
#[must_use]
pub fn create_blend_configs(ids: &[[u8; 32]], ports: &[u16]) -> Vec<GeneralBlendConfig> {
    ids.iter()
        .zip(ports)
        .map(|(id, port)| {
            let signer = SigningKey::from_bytes(id);

            let private_key = Ed25519PrivateKey::from(*id);
            // We need unique ZK secret keys, so we just derive them deterministically from
            // the generated Ed25519 public keys, which are guaranteed to be unique because
            // they are in turned derived from node ID.
            let secret_zk_key =
                SecretKey::from(BigUint::from_bytes_le(private_key.public_key().as_bytes()));
            GeneralBlendConfig {
                backend_core: Libp2pCoreBlendBackendSettings {
                    listening_address: Multiaddr::from_str(&format!(
                        "/ip4/127.0.0.1/udp/{port}/quic-v1",
                    ))
                    .unwrap(),
                    core_peering_degree: 1..=3,
                    minimum_messages_coefficient: NonZeroU64::try_from(1)
                        .expect("Minimum messages coefficient cannot be zero."),
                    normalization_constant: 1.03f64
                        .try_into()
                        .expect("Normalization constant cannot be negative."),
                    edge_node_connection_timeout: Duration::from_secs(1),
                    max_edge_node_incoming_connections: 300,
                    max_dial_attempts_per_peer: NonZeroU64::try_from(3)
                        .expect("Max dial attempts per peer cannot be zero."),
                    protocol_name: StreamProtocol::new("/blend/integration-tests"),
                },
                backend_edge: Libp2pEdgeBlendBackendSettings {
                    max_dial_attempts_per_peer_per_message: 1.try_into().unwrap(),
                    protocol_name: StreamProtocol::new("/blend/integration-tests"),
                    replication_factor: 1.try_into().unwrap(),
                },
                private_key,
                secret_zk_key,
                signer,
            }
        })
        .collect()
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/topology/configs/bootstrap.rs
────────────────────────────────────────────────
use std::time::Duration;

#[derive(Clone)]
pub struct GeneralBootstrapConfig {
    pub prolonged_bootstrap_period: Duration,
}

pub const SHORT_PROLONGED_BOOTSTRAP_PERIOD: Duration = Duration::from_secs(1);

#[must_use]
pub fn create_bootstrap_configs(
    ids: &[[u8; 32]],
    prolonged_bootstrap_period: Duration,
) -> Vec<GeneralBootstrapConfig> {
    ids.iter()
        .map(|_| GeneralBootstrapConfig {
            prolonged_bootstrap_period,
        })
        .collect()
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/topology/configs/consensus.rs
────────────────────────────────────────────────
use std::{num::NonZero, sync::Arc};

use chain_leader::LeaderConfig;
use cryptarchia_engine::EpochConfig;
use ed25519_dalek::ed25519::signature::SignerMut as _;
use groth16::CompressedGroth16Proof;
use nomos_core::{
    mantle::{
        MantleTx, Note, OpProof, Utxo,
        genesis_tx::GenesisTx,
        ledger::Tx as LedgerTx,
        ops::{
            Op,
            channel::{ChannelId, Ed25519PublicKey, MsgId, inscribe::InscriptionOp},
        },
    },
    sdp::{DeclarationMessage, Locator, ProviderId, ServiceParameters, ServiceType},
};
use nomos_node::{SignedMantleTx, Transaction as _};
use num_bigint::BigUint;
use zksign::{PublicKey, SecretKey};

use super::wallet::{WalletAccount, WalletConfig};

#[derive(Clone)]
pub struct ConsensusParams {
    pub n_participants: usize,
    pub security_param: NonZero<u32>,
    pub active_slot_coeff: f64,
}

impl ConsensusParams {
    #[must_use]
    pub const fn default_for_participants(n_participants: usize) -> Self {
        Self {
            n_participants,
            // by setting the slot coeff to 1, we also increase the probability of multiple blocks
            // (forks) being produced in the same slot (epoch). Setting the security
            // parameter to some value > 1 ensures nodes have some time to sync before
            // deciding on the longest chain.
            security_param: NonZero::new(10).unwrap(),
            // a block should be produced (on average) every slot
            active_slot_coeff: 0.9,
        }
    }
}

#[derive(Clone)]
pub struct ProviderInfo {
    pub service_type: ServiceType,
    pub provider_sk: ed25519_dalek::SigningKey,
    pub zk_sk: SecretKey,
    pub locator: Locator,
    pub note: ServiceNote,
}

impl ProviderInfo {
    #[must_use]
    pub fn provider_id(&self) -> ProviderId {
        ProviderId(self.provider_sk.verifying_key())
    }

    #[must_use]
    pub fn zk_id(&self) -> PublicKey {
        self.zk_sk.to_public_key()
    }
}

/// General consensus configuration for a chosen participant, that later could
/// be converted into a specific service or services configuration.
#[derive(Clone)]
pub struct GeneralConsensusConfig {
    pub leader_config: LeaderConfig,
    pub ledger_config: nomos_ledger::Config,
    pub genesis_tx: GenesisTx,
    pub utxos: Vec<Utxo>,
    pub blend_notes: Vec<ServiceNote>,
    pub da_notes: Vec<ServiceNote>,
    pub wallet_accounts: Vec<WalletAccount>,
}

#[derive(Clone)]
pub struct ServiceNote {
    pub pk: PublicKey,
    pub sk: SecretKey,
    pub note: Note,
    pub output_index: usize,
}

fn create_genesis_tx(utxos: &[Utxo]) -> GenesisTx {
    // Create a genesis inscription op (similar to config.yaml)
    let inscription = InscriptionOp {
        channel_id: ChannelId::from([0; 32]),
        inscription: vec![103, 101, 110, 101, 115, 105, 115], // "genesis" in bytes
        parent: MsgId::root(),
        signer: Ed25519PublicKey::from_bytes(&[0; 32]).unwrap(),
    };

    // Create ledger transaction with the utxos as outputs
    let outputs: Vec<Note> = utxos.iter().map(|u| u.note).collect();
    let ledger_tx = LedgerTx::new(vec![], outputs);

    // Create the mantle transaction
    let mantle_tx = MantleTx {
        ops: vec![Op::ChannelInscribe(inscription)],
        ledger_tx,
        execution_gas_price: 0,
        storage_gas_price: 0,
    };
    let signed_mantle_tx = SignedMantleTx {
        mantle_tx,
        ops_proofs: vec![OpProof::NoProof],
        ledger_tx_proof: zksign::Signature::new(CompressedGroth16Proof::from_bytes(&[0u8; 128])),
    };

    // Wrap in GenesisTx
    GenesisTx::from_tx(signed_mantle_tx).expect("Invalid genesis transaction")
}

#[must_use]
pub fn create_consensus_configs(
    ids: &[[u8; 32]],
    consensus_params: &ConsensusParams,
    wallet: &WalletConfig,
) -> Vec<GeneralConsensusConfig> {
    let mut leader_keys = Vec::new();
    let mut blend_notes = Vec::new();
    let mut da_notes = Vec::new();

    let utxos = create_utxos_for_leader_and_services(
        ids,
        &mut leader_keys,
        &mut blend_notes,
        &mut da_notes,
    );
    let utxos = append_wallet_utxos(utxos, wallet);
    let genesis_tx = create_genesis_tx(&utxos);
    let ledger_config = nomos_ledger::Config {
        epoch_config: EpochConfig {
            epoch_stake_distribution_stabilization: NonZero::new(3).unwrap(),
            epoch_period_nonce_buffer: NonZero::new(3).unwrap(),
            epoch_period_nonce_stabilization: NonZero::new(4).unwrap(),
        },
        consensus_config: cryptarchia_engine::Config {
            security_param: consensus_params.security_param,
            active_slot_coeff: consensus_params.active_slot_coeff,
        },
        sdp_config: nomos_ledger::mantle::sdp::Config {
            service_params: Arc::new(
                [
                    (
                        ServiceType::BlendNetwork,
                        ServiceParameters {
                            lock_period: 10,
                            inactivity_period: 20,
                            retention_period: 100,
                            timestamp: 0,
                            session_duration: 1000,
                        },
                    ),
                    (
                        ServiceType::DataAvailability,
                        ServiceParameters {
                            lock_period: 10,
                            inactivity_period: 20,
                            retention_period: 100,
                            timestamp: 0,
                            session_duration: 1000,
                        },
                    ),
                ]
                .into(),
            ),
            min_stake: nomos_core::sdp::MinStake {
                threshold: 1,
                timestamp: 0,
            },
        },
    };

    leader_keys
        .into_iter()
        .map(|(pk, sk)| GeneralConsensusConfig {
            leader_config: LeaderConfig { pk, sk },
            ledger_config: ledger_config.clone(),
            genesis_tx: genesis_tx.clone(),
            utxos: utxos.clone(),
            da_notes: da_notes.clone(),
            blend_notes: blend_notes.clone(),
            wallet_accounts: wallet.accounts.clone(),
        })
        .collect()
}

fn create_utxos_for_leader_and_services(
    ids: &[[u8; 32]],
    leader_keys: &mut Vec<(PublicKey, SecretKey)>,
    blend_notes: &mut Vec<ServiceNote>,
    da_notes: &mut Vec<ServiceNote>,
) -> Vec<Utxo> {
    let derive_key_material = |prefix: &[u8], id_bytes: &[u8]| -> [u8; 16] {
        let mut sk_data = [0; 16];
        let prefix_len = prefix.len();

        sk_data[..prefix_len].copy_from_slice(prefix);
        let remaining_len = 16 - prefix_len;
        sk_data[prefix_len..].copy_from_slice(&id_bytes[..remaining_len]);

        sk_data
    };

    let mut utxos = Vec::new();

    // Assume output index which will be set by the ledger tx.
    let mut output_index = 0;

    // Create notes for leader, Blend and DA declarations.
    for &id in ids {
        let sk_leader_data = derive_key_material(b"ld", &id);
        let sk_leader = SecretKey::from(BigUint::from_bytes_le(&sk_leader_data));
        let pk_leader = sk_leader.to_public_key();
        leader_keys.push((pk_leader, sk_leader));
        utxos.push(Utxo {
            note: Note::new(1_000, pk_leader),
            tx_hash: BigUint::from(0u8).into(),
            output_index: 0,
        });
        output_index += 1;

        let sk_da_data = derive_key_material(b"da", &id);
        let sk_da = SecretKey::from(BigUint::from_bytes_le(&sk_da_data));
        let pk_da = sk_da.to_public_key();
        let note_da = Note::new(1, pk_da);
        da_notes.push(ServiceNote {
            pk: pk_da,
            sk: sk_da,
            note: note_da,
            output_index,
        });
        utxos.push(Utxo {
            note: note_da,
            tx_hash: BigUint::from(0u8).into(),
            output_index: 0,
        });
        output_index += 1;

        let sk_blend_data = derive_key_material(b"bn", &id);
        let sk_blend = SecretKey::from(BigUint::from_bytes_le(&sk_blend_data));
        let pk_blend = sk_blend.to_public_key();
        let note_blend = Note::new(1, pk_blend);
        blend_notes.push(ServiceNote {
            pk: pk_blend,
            sk: sk_blend,
            note: note_blend,
            output_index,
        });
        utxos.push(Utxo {
            note: note_blend,
            tx_hash: BigUint::from(0u8).into(),
            output_index: 0,
        });
        output_index += 1;
    }

    utxos
}

fn append_wallet_utxos(mut utxos: Vec<Utxo>, wallet: &WalletConfig) -> Vec<Utxo> {
    for account in &wallet.accounts {
        utxos.push(Utxo {
            note: Note::new(account.value, account.public_key()),
            tx_hash: BigUint::from(0u8).into(),
            output_index: 0,
        });
    }

    utxos
}

#[must_use]
pub fn create_genesis_tx_with_declarations(
    ledger_tx: LedgerTx,
    providers: Vec<ProviderInfo>,
) -> GenesisTx {
    let inscription = InscriptionOp {
        channel_id: ChannelId::from([0; 32]),
        inscription: vec![103, 101, 110, 101, 115, 105, 115], // "genesis" in bytes
        parent: MsgId::root(),
        signer: Ed25519PublicKey::from_bytes(&[0; 32]).unwrap(),
    };

    let ledger_tx_hash = ledger_tx.hash();

    let mut ops = vec![Op::ChannelInscribe(inscription)];

    for provider in &providers {
        let utxo = Utxo {
            tx_hash: ledger_tx_hash,
            output_index: provider.note.output_index,
            note: provider.note.note,
        };
        let declaration = DeclarationMessage {
            service_type: provider.service_type,
            locators: vec![provider.locator.clone()],
            provider_id: provider.provider_id(),
            zk_id: provider.zk_id(),
            locked_note_id: utxo.id(),
        };
        ops.push(Op::SDPDeclare(declaration));
    }

    let mantle_tx = MantleTx {
        ops,
        ledger_tx,
        execution_gas_price: 0,
        storage_gas_price: 0,
    };

    let mantle_tx_hash = mantle_tx.hash();
    let mut ops_proofs = vec![OpProof::NoProof];

    for mut provider in providers {
        let zk_sig =
            SecretKey::multi_sign(&[provider.note.sk, provider.zk_sk], mantle_tx_hash.as_ref())
                .unwrap();
        let ed25519_sig = provider
            .provider_sk
            .sign(mantle_tx_hash.as_signing_bytes().as_ref());

        ops_proofs.push(OpProof::ZkAndEd25519Sigs {
            zk_sig,
            ed25519_sig,
        });
    }

    let signed_mantle_tx = SignedMantleTx {
        mantle_tx,
        ops_proofs,
        ledger_tx_proof: zksign::Signature::new(CompressedGroth16Proof::from_bytes(&[0u8; 128])),
    };

    GenesisTx::from_tx(signed_mantle_tx).expect("Invalid genesis transaction")
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/topology/configs/da.rs
────────────────────────────────────────────────
use std::{
    collections::{HashMap, HashSet},
    path::PathBuf,
    str::FromStr as _,
    sync::LazyLock,
    time::Duration,
};

use ed25519_dalek::SigningKey;
use nomos_core::sdp::SessionNumber;
use nomos_da_network_core::swarm::{
    DAConnectionMonitorSettings, DAConnectionPolicySettings, ReplicationConfig,
};
use nomos_libp2p::{Multiaddr, PeerId, ed25519};
use nomos_node::NomosDaMembership;
use num_bigint::BigUint;
use subnetworks_assignations::{MembershipCreator as _, MembershipHandler as _};
use zksign::SecretKey;

use crate::secret_key_to_peer_id;

pub static GLOBAL_PARAMS_PATH: LazyLock<String> = LazyLock::new(|| {
    let manifest_dir = PathBuf::from(env!("CARGO_MANIFEST_DIR"));
    let relative_path = PathBuf::from("../../tests/kzgrs/kzgrs_test_params");
    manifest_dir
        .join(relative_path)
        .canonicalize()
        .expect("Failed to resolve absolute path")
        .to_string_lossy()
        .to_string()
});

#[derive(Clone)]
pub struct DaParams {
    pub subnetwork_size: usize,
    pub dispersal_factor: usize,
    pub num_samples: u16,
    pub num_subnets: u16,
    pub old_blobs_check_interval: Duration,
    pub blobs_validity_duration: Duration,
    pub global_params_path: String,
    pub policy_settings: DAConnectionPolicySettings,
    pub monitor_settings: DAConnectionMonitorSettings,
    pub balancer_interval: Duration,
    pub redial_cooldown: Duration,
    pub replication_settings: ReplicationConfig,
    pub subnets_refresh_interval: Duration,
    pub retry_shares_limit: usize,
    pub retry_commitments_limit: usize,
}

impl Default for DaParams {
    fn default() -> Self {
        Self {
            subnetwork_size: 2,
            dispersal_factor: 1,
            num_samples: 1,
            num_subnets: 2,
            old_blobs_check_interval: Duration::from_secs(5),
            blobs_validity_duration: Duration::from_secs(60),
            global_params_path: GLOBAL_PARAMS_PATH.to_string(),
            policy_settings: DAConnectionPolicySettings {
                min_dispersal_peers: 1,
                min_replication_peers: 1,
                max_dispersal_failures: 0,
                max_sampling_failures: 0,
                max_replication_failures: 0,
                malicious_threshold: 0,
            },
            monitor_settings: DAConnectionMonitorSettings {
                failure_time_window: Duration::from_secs(5),
                ..Default::default()
            },
            balancer_interval: Duration::from_secs(1),
            redial_cooldown: Duration::ZERO,
            replication_settings: ReplicationConfig {
                seen_message_cache_size: 1000,
                seen_message_ttl: Duration::from_secs(3600),
            },
            subnets_refresh_interval: Duration::from_secs(30),
            retry_shares_limit: 1,
            retry_commitments_limit: 1,
        }
    }
}

#[derive(Debug, Clone)]
pub struct GeneralDaConfig {
    pub node_key: ed25519::SecretKey,
    pub signer: SigningKey,
    pub peer_id: PeerId,
    pub membership: NomosDaMembership,
    pub listening_address: Multiaddr,
    pub blob_storage_directory: PathBuf,
    pub global_params_path: String,
    pub verifier_sk: String,
    pub verifier_index: HashSet<u16>,
    pub num_samples: u16,
    pub num_subnets: u16,
    pub old_blobs_check_interval: Duration,
    pub blobs_validity_duration: Duration,
    pub policy_settings: DAConnectionPolicySettings,
    pub monitor_settings: DAConnectionMonitorSettings,
    pub balancer_interval: Duration,
    pub redial_cooldown: Duration,
    pub replication_settings: ReplicationConfig,
    pub subnets_refresh_interval: Duration,
    pub retry_shares_limit: usize,
    pub retry_commitments_limit: usize,
    pub secret_zk_key: SecretKey,
}

#[must_use]
pub fn create_da_configs(
    ids: &[[u8; 32]],
    da_params: &DaParams,
    ports: &[u16],
) -> Vec<GeneralDaConfig> {
    let mut node_keys = vec![];
    let mut peer_ids = vec![];
    let mut listening_addresses = vec![];

    for (i, id) in ids.iter().enumerate() {
        let mut node_key_bytes = *id;
        let node_key = ed25519::SecretKey::try_from_bytes(&mut node_key_bytes)
            .expect("Failed to generate secret key from bytes");
        node_keys.push(node_key.clone());

        let peer_id = secret_key_to_peer_id(node_key);
        peer_ids.push(peer_id);

        let listening_address =
            Multiaddr::from_str(&format!("/ip4/127.0.0.1/udp/{}/quic-v1", ports[i],))
                .expect("Failed to create multiaddr");
        listening_addresses.push(listening_address);
    }

    let membership = {
        let template = NomosDaMembership::new(
            SessionNumber::default(),
            da_params.subnetwork_size,
            da_params.dispersal_factor,
        );
        let mut assignations: HashMap<u16, HashSet<PeerId>> = HashMap::new();
        if peer_ids.is_empty() {
            for id in 0..da_params.subnetwork_size {
                assignations.insert(u16::try_from(id).unwrap_or_default(), HashSet::new());
            }
        } else {
            let mut sorted_peers = peer_ids.clone();
            sorted_peers.sort_unstable();
            let dispersal = da_params.dispersal_factor.max(1);
            let mut peer_cycle = sorted_peers.iter().cycle();
            for id in 0..da_params.subnetwork_size {
                let mut members = HashSet::new();
                for _ in 0..dispersal {
                    // cycle() only yields None when the iterator is empty, which we guard against.
                    if let Some(peer) = peer_cycle.next() {
                        members.insert(*peer);
                    }
                }
                assignations.insert(u16::try_from(id).unwrap_or_default(), members);
            }
        }

        template.init(SessionNumber::default(), assignations)
    };

    ids.iter()
        .zip(node_keys)
        .enumerate()
        .map(|(i, (id, node_key))| {
            let blob_storage_directory = PathBuf::from(format!("/tmp/blob_storage_{i}"));
            let verifier_sk = blst::min_sig::SecretKey::key_gen(id, &[]).unwrap();
            let verifier_sk_bytes = verifier_sk.to_bytes();
            let peer_id = peer_ids[i];
            let signer = SigningKey::from_bytes(id);
            let subnetwork_ids = membership.membership(&peer_id);

            // We need unique ZK secret keys, so we just derive them deterministically from
            // the generated Ed25519 public keys, which are guaranteed to be unique because
            // they are in turned derived from node ID.
            let secret_zk_key =
                SecretKey::from(BigUint::from_bytes_le(signer.verifying_key().as_bytes()));

            GeneralDaConfig {
                node_key,
                signer,
                peer_id,
                secret_zk_key,
                membership: membership.clone(),
                listening_address: listening_addresses[i].clone(),
                blob_storage_directory,
                global_params_path: da_params.global_params_path.clone(),
                verifier_sk: hex::encode(verifier_sk_bytes),
                verifier_index: subnetwork_ids,
                num_samples: da_params.num_samples,
                num_subnets: da_params.num_subnets,
                old_blobs_check_interval: da_params.old_blobs_check_interval,
                blobs_validity_duration: da_params.blobs_validity_duration,
                policy_settings: da_params.policy_settings.clone(),
                monitor_settings: da_params.monitor_settings.clone(),
                balancer_interval: da_params.balancer_interval,
                redial_cooldown: da_params.redial_cooldown,
                replication_settings: da_params.replication_settings,
                subnets_refresh_interval: da_params.subnets_refresh_interval,
                retry_shares_limit: da_params.retry_shares_limit,
                retry_commitments_limit: da_params.retry_commitments_limit,
            }
        })
        .collect()
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/topology/configs/deployment.rs
────────────────────────────────────────────────
use core::{num::NonZeroU64, time::Duration};

use nomos_blend_service::{
    core::settings::{CoverTrafficSettings, MessageDelayerSettings, SchedulerSettings},
    settings::TimingSettings,
};
use nomos_libp2p::protocol_name::StreamProtocol;
use nomos_node::config::{
    blend::deployment::{
        CommonSettings as BlendCommonSettings, CoreSettings as BlendCoreSettings,
        Settings as BlendDeploymentSettings,
    },
    deployment::{CustomDeployment, Settings as DeploymentSettings},
    network::deployment::Settings as NetworkDeploymentSettings,
};
use nomos_utils::math::NonNegativeF64;

#[must_use]
pub fn default_e2e_deployment_settings() -> DeploymentSettings {
    DeploymentSettings::Custom(CustomDeployment {
        blend: BlendDeploymentSettings {
            common: BlendCommonSettings {
                minimum_network_size: NonZeroU64::try_from(30u64)
                    .expect("Minimum network size cannot be zero."),
                num_blend_layers: NonZeroU64::try_from(3)
                    .expect("Number of blend layers cannot be zero."),
                timing: TimingSettings {
                    round_duration: Duration::from_secs(1),
                    rounds_per_interval: NonZeroU64::try_from(30u64)
                        .expect("Rounds per interval cannot be zero."),
                    // (21,600 blocks * 30s per block) / 1s per round = 648,000 rounds
                    rounds_per_session: NonZeroU64::try_from(648_000u64)
                        .expect("Rounds per session cannot be zero."),
                    rounds_per_observation_window: NonZeroU64::try_from(30u64)
                        .expect("Rounds per observation window cannot be zero."),
                    rounds_per_session_transition_period: NonZeroU64::try_from(30u64)
                        .expect("Rounds per session transition period cannot be zero."),
                    epoch_transition_period_in_slots: NonZeroU64::try_from(2_600)
                        .expect("Epoch transition period in slots cannot be zero."),
                },
                protocol_name: StreamProtocol::new("/blend/integration-tests"),
            },
            core: BlendCoreSettings {
                minimum_messages_coefficient: NonZeroU64::try_from(1)
                    .expect("Minimum messages coefficient cannot be zero."),
                normalization_constant: 1.03f64
                    .try_into()
                    .expect("Normalization constant cannot be negative."),
                scheduler: SchedulerSettings {
                    cover: CoverTrafficSettings {
                        intervals_for_safety_buffer: 100,
                        message_frequency_per_round: NonNegativeF64::try_from(1f64)
                            .expect("Message frequency per round cannot be negative."),
                    },
                    delayer: MessageDelayerSettings {
                        maximum_release_delay_in_rounds: NonZeroU64::try_from(3u64)
                            .expect("Maximum release delay between rounds cannot be zero."),
                    },
                },
            },
        },
        network: NetworkDeploymentSettings {
            identify_protocol_name: StreamProtocol::new("/integration/nomos/identify/1.0.0"),
            kademlia_protocol_name: StreamProtocol::new("/integration/nomos/kad/1.0.0"),
        },
    })
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/topology/configs/mod.rs
────────────────────────────────────────────────
pub mod api;
pub mod blend;
pub mod bootstrap;
pub mod consensus;
pub mod da;
pub mod network;
pub mod time;
pub mod tracing;
pub mod wallet;

use blend::GeneralBlendConfig;
use consensus::{GeneralConsensusConfig, ProviderInfo, create_genesis_tx_with_declarations};
use da::GeneralDaConfig;
use key_management_system::{
    backend::preload::PreloadKMSBackendSettings,
    keys::{Ed25519Key, Key, ZkKey},
};
use network::GeneralNetworkConfig;
use nomos_core::{
    mantle::GenesisTx as _,
    sdp::{Locator, ServiceType},
};
use nomos_utils::net::get_available_udp_port;
use rand::{Rng as _, thread_rng};
use tracing::GeneralTracingConfig;
use wallet::WalletConfig;

use crate::{
    common::kms::key_id_for_preload_backend,
    topology::configs::{
        api::GeneralApiConfig,
        bootstrap::{GeneralBootstrapConfig, SHORT_PROLONGED_BOOTSTRAP_PERIOD},
        consensus::ConsensusParams,
        da::DaParams,
        network::NetworkParams,
        time::GeneralTimeConfig,
    },
};

#[derive(Clone)]
pub struct GeneralConfig {
    pub api_config: GeneralApiConfig,
    pub consensus_config: GeneralConsensusConfig,
    pub bootstrapping_config: GeneralBootstrapConfig,
    pub da_config: GeneralDaConfig,
    pub network_config: GeneralNetworkConfig,
    pub blend_config: GeneralBlendConfig,
    pub tracing_config: GeneralTracingConfig,
    pub time_config: GeneralTimeConfig,
    pub kms_config: PreloadKMSBackendSettings,
}

#[must_use]
pub fn create_general_configs(n_nodes: usize) -> Vec<GeneralConfig> {
    create_general_configs_with_network(n_nodes, &NetworkParams::default())
}

#[must_use]
pub fn create_general_configs_with_network(
    n_nodes: usize,
    network_params: &NetworkParams,
) -> Vec<GeneralConfig> {
    create_general_configs_with_blend_core_subset(n_nodes, n_nodes, network_params)
}

#[must_use]
pub fn create_general_configs_with_blend_core_subset(
    n_nodes: usize,
    // TODO: Instead of this, define a config struct for each node.
    // That would be also useful for non-even token distributions: https://github.com/logos-co/nomos/issues/1888
    n_blend_core_nodes: usize,
    network_params: &NetworkParams,
) -> Vec<GeneralConfig> {
    assert!(
        n_blend_core_nodes <= n_nodes,
        "n_blend_core_nodes({n_blend_core_nodes}) must be less than or equal to n_nodes({n_nodes})",
    );

    // Blend relies on each node declaring a different ZK public key, so we need
    // different IDs to generate different keys.
    let mut ids: Vec<_> = (0..n_nodes).map(|i| [i as u8; 32]).collect();
    let mut da_ports = vec![];
    let mut blend_ports = vec![];

    for id in &mut ids {
        thread_rng().fill(id);
        da_ports.push(get_available_udp_port().unwrap());
        blend_ports.push(get_available_udp_port().unwrap());
    }

    let consensus_params = ConsensusParams::default_for_participants(n_nodes);
    let mut consensus_configs =
        consensus::create_consensus_configs(&ids, &consensus_params, &WalletConfig::default());
    let bootstrap_config =
        bootstrap::create_bootstrap_configs(&ids, SHORT_PROLONGED_BOOTSTRAP_PERIOD);
    let network_configs = network::create_network_configs(&ids, network_params);
    let da_configs = da::create_da_configs(&ids, &DaParams::default(), &da_ports);
    let api_configs = api::create_api_configs(&ids);
    let blend_configs = blend::create_blend_configs(&ids, &blend_ports);
    let tracing_configs = tracing::create_tracing_configs(&ids);
    let time_config = time::default_time_config();

    let providers: Vec<_> = blend_configs
        .iter()
        .enumerate()
        .take(n_blend_core_nodes)
        .map(|(i, blend_conf)| ProviderInfo {
            service_type: ServiceType::BlendNetwork,
            provider_sk: blend_conf.signer.clone(),
            zk_sk: blend_conf.secret_zk_key.clone(),
            locator: Locator(blend_conf.backend_core.listening_address.clone()),
            note: consensus_configs[0].blend_notes[i].clone(),
        })
        .collect();
    let ledger_tx = consensus_configs[0]
        .genesis_tx
        .mantle_tx()
        .ledger_tx
        .clone();
    let genesis_tx = create_genesis_tx_with_declarations(ledger_tx, providers);
    for c in &mut consensus_configs {
        c.genesis_tx = genesis_tx.clone();
    }

    // Set Blend and DA keys in KMS of each node config.
    let kms_configs: Vec<_> = blend_configs
        .iter()
        .map(|blend_conf| {
            let ed_key = Ed25519Key::new(blend_conf.signer.clone());
            let zk_key = ZkKey::new(blend_conf.secret_zk_key.clone());
            PreloadKMSBackendSettings {
                keys: [
                    (
                        key_id_for_preload_backend(&Key::from(ed_key.clone())),
                        Key::from(ed_key),
                    ),
                    (
                        key_id_for_preload_backend(&Key::from(zk_key.clone())),
                        Key::from(zk_key),
                    ),
                ]
                .into(),
            }
        })
        .collect();

    let mut general_configs = vec![];

    for i in 0..n_nodes {
        general_configs.push(GeneralConfig {
            api_config: api_configs[i].clone(),
            consensus_config: consensus_configs[i].clone(),
            bootstrapping_config: bootstrap_config[i].clone(),
            da_config: da_configs[i].clone(),
            network_config: network_configs[i].clone(),
            blend_config: blend_configs[i].clone(),
            tracing_config: tracing_configs[i].clone(),
            time_config: time_config.clone(),
            kms_config: kms_configs[i].clone(),
        });
    }

    general_configs
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/topology/configs/network.rs
────────────────────────────────────────────────
use std::time::Duration;

use nomos_libp2p::{
    IdentifySettings, KademliaSettings, Multiaddr, NatSettings, ed25519, gossipsub,
};
use nomos_node::config::network::serde::{BackendSettings, Config, SwarmConfig};
use nomos_utils::net::get_available_udp_port;

use crate::node_address_from_port;

#[derive(Default, Clone)]
pub enum Libp2pNetworkLayout {
    #[default]
    Star,
    Chain,
    Full,
}

#[derive(Default, Clone)]
pub struct NetworkParams {
    pub libp2p_network_layout: Libp2pNetworkLayout,
}

pub type GeneralNetworkConfig = Config;

fn default_swarm_config() -> SwarmConfig {
    SwarmConfig {
        host: std::net::Ipv4Addr::UNSPECIFIED,
        port: 60000,
        node_key: ed25519::SecretKey::generate(),
        gossipsub_config: gossipsub::Config::default(),
        kademlia_config: KademliaSettings::default(),
        identify_config: IdentifySettings::default(),
        chain_sync_config: cryptarchia_sync::Config::default(),
        nat_config: NatSettings::default(),
    }
}

#[must_use]
pub fn create_network_configs(
    ids: &[[u8; 32]],
    network_params: &NetworkParams,
) -> Vec<GeneralNetworkConfig> {
    let swarm_configs: Vec<SwarmConfig> = ids
        .iter()
        .map(|id| {
            let mut node_key_bytes = *id;
            let node_key = ed25519::SecretKey::try_from_bytes(&mut node_key_bytes)
                .expect("Failed to generate secret key from bytes");

            SwarmConfig {
                node_key,
                port: get_available_udp_port().unwrap(),
                chain_sync_config: cryptarchia_sync::Config {
                    peer_response_timeout: Duration::from_secs(60),
                },
                ..default_swarm_config()
            }
        })
        .collect();

    let all_initial_peers = initial_peers_by_network_layout(&swarm_configs, network_params);

    swarm_configs
        .iter()
        .zip(all_initial_peers)
        .map(|(swarm_config, initial_peers)| GeneralNetworkConfig {
            backend: BackendSettings {
                initial_peers,
                inner: swarm_config.to_owned(),
            },
        })
        .collect()
}

fn initial_peers_by_network_layout(
    swarm_configs: &[SwarmConfig],
    network_params: &NetworkParams,
) -> Vec<Vec<Multiaddr>> {
    let mut all_initial_peers = vec![];

    match network_params.libp2p_network_layout {
        Libp2pNetworkLayout::Star => {
            // First node is the hub - has no initial peers
            all_initial_peers.push(vec![]);
            let first_addr = node_address_from_port(swarm_configs[0].port);

            // All other nodes connect to the first node
            for _ in 1..swarm_configs.len() {
                all_initial_peers.push(vec![first_addr.clone()]);
            }
        }
        Libp2pNetworkLayout::Chain => {
            // First node has no initial peers
            all_initial_peers.push(vec![]);

            // Each subsequent node connects to the previous one
            for i in 1..swarm_configs.len() {
                let prev_addr = node_address_from_port(swarm_configs[i - 1].port);
                all_initial_peers.push(vec![prev_addr]);
            }
        }
        Libp2pNetworkLayout::Full => {
            // Each node connects to all previous nodes, unidirectional connections
            for i in 0..swarm_configs.len() {
                let mut peers = vec![];
                for swarm_config in swarm_configs.iter().take(i) {
                    peers.push(node_address_from_port(swarm_config.port));
                }
                all_initial_peers.push(peers);
            }
        }
    }

    all_initial_peers
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/topology/configs/time.rs
────────────────────────────────────────────────
use std::{
    net::{IpAddr, Ipv4Addr},
    str::FromStr as _,
    time::Duration,
};

use time::OffsetDateTime;

const DEFAULT_SLOT_TIME: u64 = 2;
const CONSENSUS_SLOT_TIME_VAR: &str = "CONSENSUS_SLOT_TIME";

#[derive(Clone, Debug)]
pub struct GeneralTimeConfig {
    pub slot_duration: Duration,
    pub chain_start_time: OffsetDateTime,
    pub ntp_server: String,
    pub timeout: Duration,
    pub interface: IpAddr,
    pub update_interval: Duration,
}

#[must_use]
pub fn default_time_config() -> GeneralTimeConfig {
    let slot_duration = std::env::var(CONSENSUS_SLOT_TIME_VAR)
        .map(|s| <u64>::from_str(&s).unwrap())
        .unwrap_or(DEFAULT_SLOT_TIME);
    GeneralTimeConfig {
        slot_duration: Duration::from_secs(slot_duration),
        chain_start_time: OffsetDateTime::now_utc(),
        ntp_server: String::from("pool.ntp.org"),
        timeout: Duration::from_secs(5),
        interface: IpAddr::V4(Ipv4Addr::UNSPECIFIED),
        update_interval: Duration::from_secs(16),
    }
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/topology/configs/tracing.rs
────────────────────────────────────────────────
use nomos_tracing::{
    logging::loki::LokiConfig, metrics::otlp::OtlpMetricsConfig, tracing::otlp::OtlpTracingConfig,
};
use nomos_tracing_service::{
    ConsoleLayer, FilterLayer, LoggerLayer, MetricsLayer, TracingLayer, TracingSettings,
};
use tracing::Level;

use crate::IS_DEBUG_TRACING;

#[derive(Clone, Default)]
pub struct GeneralTracingConfig {
    pub tracing_settings: TracingSettings,
}

impl GeneralTracingConfig {
    fn local_debug_tracing(id: usize) -> Self {
        let host_identifier = format!("node-{id}");
        Self {
            tracing_settings: TracingSettings {
                logger: LoggerLayer::Loki(LokiConfig {
                    endpoint: "http://localhost:3100".try_into().unwrap(),
                    host_identifier: host_identifier.clone(),
                }),
                tracing: TracingLayer::Otlp(OtlpTracingConfig {
                    endpoint: "http://localhost:4317".try_into().unwrap(),
                    sample_ratio: 0.5,
                    service_name: host_identifier.clone(),
                }),
                filter: FilterLayer::EnvFilter(nomos_tracing::filter::envfilter::EnvFilterConfig {
                    // Allow events only from modules that matches the regex, if it matches - use
                    // provided tracing level. Libp2p related crates are very log intensive in debug
                    // mode.
                    filters: std::iter::once(&("nomos", "debug"))
                        .map(|(k, v)| ((*k).to_owned(), (*v).to_owned()))
                        .collect(),
                }),
                metrics: MetricsLayer::Otlp(OtlpMetricsConfig {
                    endpoint: "http://127.0.0.1:9090/api/v1/otlp/v1/metrics"
                        .try_into()
                        .unwrap(),
                    host_identifier,
                }),
                console: ConsoleLayer::None,
                level: Level::DEBUG,
            },
        }
    }
}

#[must_use]
pub fn create_tracing_configs(ids: &[[u8; 32]]) -> Vec<GeneralTracingConfig> {
    if *IS_DEBUG_TRACING {
        create_debug_configs(ids)
    } else {
        create_default_configs(ids)
    }
}

fn create_debug_configs(ids: &[[u8; 32]]) -> Vec<GeneralTracingConfig> {
    ids.iter()
        .enumerate()
        .map(|(i, _)| GeneralTracingConfig::local_debug_tracing(i))
        .collect()
}

fn create_default_configs(ids: &[[u8; 32]]) -> Vec<GeneralTracingConfig> {
    ids.iter()
        .map(|_| GeneralTracingConfig::default())
        .collect()
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/topology/configs/wallet.rs
────────────────────────────────────────────────
use std::num::NonZeroUsize;

use num_bigint::BigUint;
use zksign::{PublicKey, SecretKey};

/// Collection of wallet accounts that should be funded at genesis.
#[derive(Clone, Default, Debug, serde::Serialize, serde::Deserialize)]
pub struct WalletConfig {
    pub accounts: Vec<WalletAccount>,
}

impl WalletConfig {
    #[must_use]
    pub const fn new(accounts: Vec<WalletAccount>) -> Self {
        Self { accounts }
    }

    #[must_use]
    pub fn uniform(total_funds: u64, users: NonZeroUsize) -> Self {
        let user_count = users.get() as u64;
        assert!(user_count > 0, "wallet user count must be non-zero");
        assert!(
            total_funds >= user_count,
            "wallet funds must allocate at least 1 token per user"
        );

        let base_allocation = total_funds / user_count;
        let mut remainder = total_funds % user_count;

        let accounts = (0..users.get())
            .map(|idx| {
                let mut amount = base_allocation;
                if remainder > 0 {
                    amount += 1;
                    remainder -= 1;
                }

                WalletAccount::deterministic(idx as u64, amount)
            })
            .collect();

        Self { accounts }
    }
}

/// Wallet account that holds funds in the genesis state.
#[derive(Clone, Debug, serde::Serialize, serde::Deserialize)]
pub struct WalletAccount {
    pub label: String,
    pub secret_key: SecretKey,
    pub value: u64,
}

impl WalletAccount {
    #[must_use]
    pub fn new(label: impl Into<String>, secret_key: SecretKey, value: u64) -> Self {
        assert!(value > 0, "wallet account value must be positive");
        Self {
            label: label.into(),
            secret_key,
            value,
        }
    }

    #[must_use]
    pub fn deterministic(index: u64, value: u64) -> Self {
        let mut seed = [0u8; 32];
        seed[..2].copy_from_slice(b"wl");
        seed[2..10].copy_from_slice(&index.to_le_bytes());

        let secret_key = SecretKey::from(BigUint::from_bytes_le(&seed));
        Self::new(format!("wallet-user-{index}"), secret_key, value)
    }

    #[must_use]
    pub fn public_key(&self) -> PublicKey {
        self.secret_key.to_public_key()
    }
}




════════════════════════════════════════════════
FILE: testing-framework/configs/src/topology/mod.rs
────────────────────────────────────────────────
pub mod configs;




════════════════════════════════════════════════
FILE: testing-framework/core/src/lib.rs
────────────────────────────────────────────────
pub mod nodes;
pub mod scenario;
pub mod topology;

use std::{env, ops::Mul as _, sync::LazyLock, time::Duration};

pub use integration_configs::{
    IS_DEBUG_TRACING, node_address_from_port, secret_key_to_peer_id, secret_key_to_provider_id,
    topology::configs::da::GLOBAL_PARAMS_PATH,
};

static IS_SLOW_TEST_ENV: LazyLock<bool> =
    LazyLock::new(|| env::var("SLOW_TEST_ENV").is_ok_and(|s| s == "true"));

/// In slow test environments like Codecov, use 2x timeout.
#[must_use]
pub fn adjust_timeout(d: Duration) -> Duration {
    if *IS_SLOW_TEST_ENV { d.mul(2) } else { d }
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/nodes/api_client.rs
────────────────────────────────────────────────
use std::net::SocketAddr;

use chain_service::CryptarchiaInfo;
use common_http_client::CommonHttpClient;
use nomos_core::{block::Block, da::BlobId, mantle::SignedMantleTx, sdp::SessionNumber};
use nomos_da_network_core::swarm::{BalancerStats, MonitorStats};
use nomos_da_network_service::MembershipResponse;
use nomos_http_api_common::paths::{
    CRYPTARCHIA_INFO, DA_BALANCER_STATS, DA_BLACKLISTED_PEERS, DA_BLOCK_PEER, DA_GET_MEMBERSHIP,
    DA_HISTORIC_SAMPLING, DA_MONITOR_STATS, DA_UNBLOCK_PEER, MEMPOOL_ADD_TX, NETWORK_INFO,
    STORAGE_BLOCK,
};
use nomos_network::backends::libp2p::Libp2pInfo;
use nomos_node::{HeaderId, api::testing::handlers::HistoricSamplingRequest};
use reqwest::{Client, RequestBuilder, Response, Url};
use serde::{Serialize, de::DeserializeOwned};
use serde_json::Value;

pub const DA_GET_TESTING_ENDPOINT_ERROR: &str = "Failed to connect to testing endpoint. The binary was likely built without the 'testing' \
     feature. Try: cargo build --workspace --all-features";

#[derive(Clone)]
pub struct ApiClient {
    pub(crate) base_url: Url,
    pub(crate) testing_url: Option<Url>,
    client: Client,
    pub(crate) http_client: CommonHttpClient,
}

impl ApiClient {
    #[must_use]
    pub fn new(base_addr: SocketAddr, testing_addr: Option<SocketAddr>) -> Self {
        let base_url =
            Url::parse(&format!("http://{base_addr}")).expect("Valid base address for node");
        let testing_url = testing_addr
            .map(|addr| Url::parse(&format!("http://{addr}")).expect("Valid testing address"));
        Self::from_urls(base_url, testing_url)
    }

    #[must_use]
    pub fn from_urls(base_url: Url, testing_url: Option<Url>) -> Self {
        let client = Client::new();
        Self {
            base_url,
            testing_url,
            http_client: CommonHttpClient::new_with_client(client.clone(), None),
            client,
        }
    }

    #[must_use]
    pub fn testing_url(&self) -> Option<Url> {
        self.testing_url.clone()
    }

    pub fn get_builder(&self, path: &str) -> RequestBuilder {
        self.client.get(self.join_base(path))
    }

    pub async fn get_response(&self, path: &str) -> reqwest::Result<Response> {
        self.client.get(self.join_base(path)).send().await
    }

    pub async fn get_json<T>(&self, path: &str) -> reqwest::Result<T>
    where
        T: DeserializeOwned,
    {
        self.get_response(path)
            .await?
            .error_for_status()?
            .json()
            .await
    }

    pub async fn post_json_decode<T, R>(&self, path: &str, body: &T) -> reqwest::Result<R>
    where
        T: Serialize + Sync + ?Sized,
        R: DeserializeOwned,
    {
        self.post_json_response(path, body)
            .await?
            .error_for_status()?
            .json()
            .await
    }

    pub async fn post_json_response<T>(&self, path: &str, body: &T) -> reqwest::Result<Response>
    where
        T: Serialize + Sync + ?Sized,
    {
        self.client
            .post(self.join_base(path))
            .json(body)
            .send()
            .await
    }

    pub async fn post_json_unit<T>(&self, path: &str, body: &T) -> reqwest::Result<()>
    where
        T: Serialize + Sync + ?Sized,
    {
        self.post_json_response(path, body)
            .await?
            .error_for_status()?;
        Ok(())
    }

    pub async fn get_testing_json<T>(&self, path: &str) -> reqwest::Result<T>
    where
        T: DeserializeOwned,
    {
        self.get_testing_response(path)
            .await?
            .error_for_status()?
            .json()
            .await
    }

    pub async fn post_testing_json_decode<T, R>(&self, path: &str, body: &T) -> reqwest::Result<R>
    where
        T: Serialize + Sync + ?Sized,
        R: DeserializeOwned,
    {
        self.post_testing_json_response(path, body)
            .await?
            .error_for_status()?
            .json()
            .await
    }

    pub async fn post_testing_json_unit<T>(&self, path: &str, body: &T) -> reqwest::Result<()>
    where
        T: Serialize + Sync + ?Sized,
    {
        self.post_testing_json_response(path, body)
            .await?
            .error_for_status()?;
        Ok(())
    }

    pub async fn post_testing_json_response<T>(
        &self,
        path: &str,
        body: &T,
    ) -> reqwest::Result<Response>
    where
        T: Serialize + Sync + ?Sized,
    {
        let testing_url = self
            .testing_url
            .as_ref()
            .expect(DA_GET_TESTING_ENDPOINT_ERROR);
        self.client
            .post(Self::join_url(testing_url, path))
            .json(body)
            .send()
            .await
    }

    pub async fn get_testing_response(&self, path: &str) -> reqwest::Result<Response> {
        let testing_url = self
            .testing_url
            .as_ref()
            .expect(DA_GET_TESTING_ENDPOINT_ERROR);
        self.client
            .get(Self::join_url(testing_url, path))
            .send()
            .await
    }

    pub async fn block_peer(&self, peer_id: &str) -> reqwest::Result<bool> {
        self.post_json_decode(DA_BLOCK_PEER, &peer_id).await
    }

    pub async fn unblock_peer(&self, peer_id: &str) -> reqwest::Result<bool> {
        self.post_json_decode(DA_UNBLOCK_PEER, &peer_id).await
    }

    pub async fn blacklisted_peers(&self) -> reqwest::Result<Vec<String>> {
        self.get_json(DA_BLACKLISTED_PEERS).await
    }

    pub async fn balancer_stats(&self) -> reqwest::Result<BalancerStats> {
        self.get_json(DA_BALANCER_STATS).await
    }

    pub async fn monitor_stats(&self) -> reqwest::Result<MonitorStats> {
        self.get_json(DA_MONITOR_STATS).await
    }

    pub async fn consensus_info(&self) -> reqwest::Result<CryptarchiaInfo> {
        self.get_json(CRYPTARCHIA_INFO).await
    }

    pub async fn network_info(&self) -> reqwest::Result<Libp2pInfo> {
        self.get_json(NETWORK_INFO).await
    }

    pub async fn storage_block(
        &self,
        id: &HeaderId,
    ) -> reqwest::Result<Option<Block<SignedMantleTx>>> {
        self.post_json_decode(STORAGE_BLOCK, id).await
    }

    pub async fn da_get_membership(
        &self,
        session_id: &SessionNumber,
    ) -> reqwest::Result<MembershipResponse> {
        self.post_testing_json_decode(DA_GET_MEMBERSHIP, session_id)
            .await
    }

    pub async fn da_historic_sampling(
        &self,
        request: &HistoricSamplingRequest<BlobId>,
    ) -> reqwest::Result<bool> {
        self.post_testing_json_decode(DA_HISTORIC_SAMPLING, request)
            .await
    }

    pub async fn submit_transaction(&self, tx: &SignedMantleTx) -> reqwest::Result<()> {
        self.post_json_unit(MEMPOOL_ADD_TX, tx).await
    }

    pub async fn get_headers_raw(&self, builder: RequestBuilder) -> reqwest::Result<Response> {
        builder.send().await
    }

    pub async fn mempool_metrics(&self, pool: &str) -> reqwest::Result<Value> {
        self.get_json(&format!("/{pool}/metrics")).await
    }

    #[must_use]
    pub const fn base_url(&self) -> &Url {
        &self.base_url
    }

    #[must_use]
    pub const fn http_client(&self) -> &CommonHttpClient {
        &self.http_client
    }

    fn join_base(&self, path: &str) -> Url {
        Self::join_url(&self.base_url, path)
    }

    fn join_url(base: &Url, path: &str) -> Url {
        let trimmed = path.trim_start_matches('/');
        base.join(trimmed).expect("valid relative path")
    }
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/nodes/executor.rs
────────────────────────────────────────────────
use std::{
    collections::HashSet,
    path::PathBuf,
    process::{Child, Command, Stdio},
    time::Duration,
};

use broadcast_service::BlockInfo;
use chain_service::CryptarchiaInfo;
use futures::Stream;
pub use integration_configs::nodes::executor::create_executor_config;
use kzgrs_backend::common::share::{DaLightShare, DaShare, DaSharesCommitments};
use nomos_core::{
    block::Block, da::BlobId, header::HeaderId, mantle::SignedMantleTx, sdp::SessionNumber,
};
use nomos_da_network_core::swarm::{BalancerStats, MonitorStats};
use nomos_da_network_service::MembershipResponse;
use nomos_executor::config::Config;
use nomos_http_api_common::paths::{DA_GET_SHARES_COMMITMENTS, MANTLE_METRICS, MEMPOOL_ADD_TX};
use nomos_network::backends::libp2p::Libp2pInfo;
use nomos_node::api::testing::handlers::HistoricSamplingRequest;
use nomos_tracing::logging::local::FileConfig;
use nomos_tracing_service::LoggerLayer;
use reqwest::Url;
use serde_yaml::{Mapping, Number as YamlNumber, Value};

use super::{ApiClient, create_tempdir, persist_tempdir, should_persist_tempdir};
use crate::{IS_DEBUG_TRACING, adjust_timeout, nodes::LOGS_PREFIX};

const BIN_PATH: &str = "target/debug/nomos-executor";

fn binary_path() -> PathBuf {
    PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .join("../../")
        .join(BIN_PATH)
}

pub struct Executor {
    tempdir: tempfile::TempDir,
    child: Child,
    config: Config,
    api: ApiClient,
}

fn inject_ibd_into_cryptarchia(yaml_value: &mut Value) {
    let Some(root) = yaml_value.as_mapping_mut() else {
        return;
    };
    let Some(cryptarchia) = root
        .get_mut(&Value::String("cryptarchia".into()))
        .and_then(Value::as_mapping_mut)
    else {
        return;
    };
    if !cryptarchia.contains_key(&Value::String("network_adapter_settings".into())) {
        let mut network = Mapping::new();
        network.insert(
            Value::String("topic".into()),
            Value::String(nomos_node::CONSENSUS_TOPIC.into()),
        );
        cryptarchia.insert(
            Value::String("network_adapter_settings".into()),
            Value::Mapping(network),
        );
    }
    if !cryptarchia.contains_key(&Value::String("sync".into())) {
        let mut orphan = Mapping::new();
        orphan.insert(
            Value::String("max_orphan_cache_size".into()),
            Value::Number(YamlNumber::from(5)),
        );
        let mut sync = Mapping::new();
        sync.insert(Value::String("orphan".into()), Value::Mapping(orphan));
        cryptarchia.insert(Value::String("sync".into()), Value::Mapping(sync));
    }
    let Some(bootstrap) = cryptarchia
        .get_mut(&Value::String("bootstrap".into()))
        .and_then(Value::as_mapping_mut)
    else {
        return;
    };

    let ibd_key = Value::String("ibd".into());
    if bootstrap.contains_key(&ibd_key) {
        return;
    }

    let mut ibd = Mapping::new();
    ibd.insert(Value::String("peers".into()), Value::Sequence(vec![]));

    bootstrap.insert(ibd_key, Value::Mapping(ibd));
}

impl Drop for Executor {
    fn drop(&mut self) {
        if should_persist_tempdir()
            && let Err(e) = persist_tempdir(&mut self.tempdir, "nomos-executor")
        {
            println!("failed to persist tempdir: {e}");
        }

        if let Err(e) = self.child.kill() {
            println!("failed to kill the child process: {e}");
        }
    }
}

impl Executor {
    pub async fn spawn(mut config: Config) -> Self {
        let dir = create_tempdir().unwrap();
        let config_path = dir.path().join("executor.yaml");
        let file = std::fs::File::create(&config_path).unwrap();

        if !*IS_DEBUG_TRACING {
            // setup logging so that we can intercept it later in testing
            config.tracing.logger = LoggerLayer::File(FileConfig {
                directory: dir.path().to_owned(),
                prefix: Some(LOGS_PREFIX.into()),
            });
        }

        config.storage.db_path = dir.path().join("db");
        dir.path().clone_into(
            &mut config
                .da_verifier
                .storage_adapter_settings
                .blob_storage_directory,
        );

        let addr = config.http.backend_settings.address;
        let testing_addr = config.testing_http.backend_settings.address;

        let mut yaml_value = serde_yaml::to_value(&config).unwrap();
        inject_ibd_into_cryptarchia(&mut yaml_value);
        serde_yaml::to_writer(file, &yaml_value).unwrap();
        let child = Command::new(binary_path())
            .arg(&config_path)
            .current_dir(dir.path())
            .stdout(Stdio::inherit())
            .spawn()
            .unwrap();
        let node = Self {
            child,
            tempdir: dir,
            config,
            api: ApiClient::new(addr, Some(testing_addr)),
        };
        tokio::time::timeout(adjust_timeout(Duration::from_secs(10)), async {
            node.wait_online().await;
        })
        .await
        .unwrap();

        node
    }

    pub async fn block_peer(&self, peer_id: String) -> bool {
        self.api.block_peer(&peer_id).await.unwrap()
    }

    pub async fn unblock_peer(&self, peer_id: String) -> bool {
        self.api.unblock_peer(&peer_id).await.unwrap()
    }

    pub async fn blacklisted_peers(&self) -> Vec<String> {
        self.api.blacklisted_peers().await.unwrap()
    }

    async fn wait_online(&self) {
        loop {
            let res = self.api.get_response(MANTLE_METRICS).await;
            if res.is_ok() && res.unwrap().status().is_success() {
                break;
            }
            tokio::time::sleep(Duration::from_millis(100)).await;
        }
    }

    #[must_use]
    pub const fn config(&self) -> &Config {
        &self.config
    }

    #[must_use]
    pub fn url(&self) -> Url {
        self.api.base_url().clone()
    }

    #[must_use]
    pub fn testing_url(&self) -> Option<Url> {
        self.api.testing_url()
    }

    pub async fn balancer_stats(&self) -> BalancerStats {
        self.api.balancer_stats().await.unwrap()
    }

    pub async fn monitor_stats(&self) -> MonitorStats {
        self.api.monitor_stats().await.unwrap()
    }

    pub async fn network_info(&self) -> Libp2pInfo {
        self.api.network_info().await.unwrap()
    }

    pub async fn consensus_info(&self) -> CryptarchiaInfo {
        self.api.consensus_info().await.unwrap()
    }

    pub async fn get_block(&self, id: HeaderId) -> Option<Block<SignedMantleTx>> {
        self.api.storage_block(&id).await.unwrap()
    }

    pub async fn get_shares(
        &self,
        blob_id: BlobId,
        requested_shares: HashSet<[u8; 2]>,
        filter_shares: HashSet<[u8; 2]>,
        return_available: bool,
    ) -> Result<impl Stream<Item = DaLightShare>, common_http_client::Error> {
        self.api
            .http_client()
            .get_shares::<DaShare>(
                self.api.base_url().clone(),
                blob_id,
                requested_shares,
                filter_shares,
                return_available,
            )
            .await
    }

    pub async fn get_commitments(&self, blob_id: BlobId) -> Option<DaSharesCommitments> {
        self.api
            .post_json_decode(DA_GET_SHARES_COMMITMENTS, &blob_id)
            .await
            .unwrap()
    }

    pub async fn get_storage_commitments(
        &self,
        blob_id: BlobId,
    ) -> Result<Option<DaSharesCommitments>, common_http_client::Error> {
        self.api
            .http_client()
            .get_storage_commitments::<DaShare>(self.api.base_url().clone(), blob_id)
            .await
    }

    pub async fn da_get_membership(
        &self,
        session_id: SessionNumber,
    ) -> Result<MembershipResponse, reqwest::Error> {
        self.api.da_get_membership(&session_id).await
    }

    pub async fn da_historic_sampling<I>(
        &self,
        block_id: HeaderId,
        blob_ids: I,
    ) -> Result<bool, reqwest::Error>
    where
        I: IntoIterator<Item = (BlobId, SessionNumber)>,
    {
        let request = HistoricSamplingRequest {
            block_id,
            blob_ids: blob_ids.into_iter().collect(),
        };

        self.api.da_historic_sampling(&request).await
    }

    pub async fn get_lib_stream(
        &self,
    ) -> Result<impl Stream<Item = BlockInfo>, common_http_client::Error> {
        self.api
            .http_client()
            .get_lib_stream(self.api.base_url().clone())
            .await
    }

    pub async fn add_tx(&self, tx: SignedMantleTx) -> Result<(), reqwest::Error> {
        self.api.post_json_unit(MEMPOOL_ADD_TX, &tx).await
    }
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/nodes/mod.rs
────────────────────────────────────────────────
mod api_client;
pub mod executor;
pub mod validator;

use std::sync::LazyLock;

pub use api_client::ApiClient;
use tempfile::TempDir;

pub(crate) const LOGS_PREFIX: &str = "__logs";
static KEEP_NODE_TEMPDIRS: LazyLock<bool> =
    LazyLock::new(|| std::env::var("NOMOS_TESTS_KEEP_LOGS").is_ok());

fn create_tempdir() -> std::io::Result<TempDir> {
    // It's easier to use the current location instead of OS-default tempfile
    // location because Github Actions can easily access files in the current
    // location using wildcard to upload them as artifacts.
    TempDir::new_in(std::env::current_dir()?)
}

fn persist_tempdir(tempdir: &mut TempDir, label: &str) -> std::io::Result<()> {
    println!(
        "{}: persisting directory at {}",
        label,
        tempdir.path().display()
    );
    // we need ownership of the dir to persist it
    let dir = std::mem::replace(tempdir, tempfile::tempdir()?);
    let _ = dir.keep();
    Ok(())
}

pub(crate) fn should_persist_tempdir() -> bool {
    std::thread::panicking() || *KEEP_NODE_TEMPDIRS
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/nodes/validator.rs
────────────────────────────────────────────────
use std::{
    collections::HashSet,
    path::PathBuf,
    process::{Child, Command, Stdio},
    time::Duration,
};

use broadcast_service::BlockInfo;
use chain_service::CryptarchiaInfo;
use futures::Stream;
pub use integration_configs::nodes::validator::create_validator_config;
use kzgrs_backend::common::share::{DaLightShare, DaShare, DaSharesCommitments};
use nomos_core::{block::Block, da::BlobId, mantle::SignedMantleTx, sdp::SessionNumber};
use nomos_da_network_core::swarm::{BalancerStats, MonitorStats};
use nomos_da_network_service::MembershipResponse;
use nomos_http_api_common::paths::{CRYPTARCHIA_HEADERS, DA_GET_SHARES_COMMITMENTS};
use nomos_network::backends::libp2p::Libp2pInfo;
use nomos_node::{Config, HeaderId, api::testing::handlers::HistoricSamplingRequest};
use nomos_tracing::logging::local::FileConfig;
use nomos_tracing_service::LoggerLayer;
use reqwest::Url;
use serde_yaml::{Mapping, Number as YamlNumber, Value};
use tokio::time::error::Elapsed;
use tx_service::MempoolMetrics;

use super::{ApiClient, create_tempdir, persist_tempdir, should_persist_tempdir};
use crate::{IS_DEBUG_TRACING, adjust_timeout, nodes::LOGS_PREFIX};

const BIN_PATH: &str = "target/debug/nomos-node";

fn binary_path() -> PathBuf {
    PathBuf::from(env!("CARGO_MANIFEST_DIR"))
        .join("../../")
        .join(BIN_PATH)
}

pub enum Pool {
    Da,
    Mantle,
}

pub struct Validator {
    tempdir: tempfile::TempDir,
    child: Child,
    config: Config,
    api: ApiClient,
}

fn inject_ibd_into_cryptarchia(yaml_value: &mut Value) {
    let Some(root) = yaml_value.as_mapping_mut() else {
        return;
    };
    let Some(cryptarchia) = root
        .get_mut(&Value::String("cryptarchia".into()))
        .and_then(Value::as_mapping_mut)
    else {
        return;
    };
    if !cryptarchia.contains_key(&Value::String("network_adapter_settings".into())) {
        let mut network = Mapping::new();
        network.insert(
            Value::String("topic".into()),
            Value::String(nomos_node::CONSENSUS_TOPIC.into()),
        );
        cryptarchia.insert(
            Value::String("network_adapter_settings".into()),
            Value::Mapping(network),
        );
    }
    if !cryptarchia.contains_key(&Value::String("sync".into())) {
        let mut orphan = Mapping::new();
        orphan.insert(
            Value::String("max_orphan_cache_size".into()),
            Value::Number(YamlNumber::from(5)),
        );
        let mut sync = Mapping::new();
        sync.insert(Value::String("orphan".into()), Value::Mapping(orphan));
        cryptarchia.insert(Value::String("sync".into()), Value::Mapping(sync));
    }
    let Some(bootstrap) = cryptarchia
        .get_mut(&Value::String("bootstrap".into()))
        .and_then(Value::as_mapping_mut)
    else {
        return;
    };

    let ibd_key = Value::String("ibd".into());
    if bootstrap.contains_key(&ibd_key) {
        return;
    }

    let mut ibd = Mapping::new();
    ibd.insert(Value::String("peers".into()), Value::Sequence(vec![]));

    bootstrap.insert(ibd_key, Value::Mapping(ibd));
}

impl Drop for Validator {
    fn drop(&mut self) {
        if should_persist_tempdir()
            && let Err(e) = persist_tempdir(&mut self.tempdir, "nomos-node")
        {
            println!("failed to persist tempdir: {e}");
        }

        if let Err(e) = self.child.kill() {
            println!("failed to kill the child process: {e}");
        }
    }
}

impl Validator {
    /// Check if the validator process is still running
    pub fn is_running(&mut self) -> bool {
        match self.child.try_wait() {
            Ok(None) => true,
            Ok(Some(_)) | Err(_) => false,
        }
    }

    /// Wait for the validator process to exit, with a timeout
    /// Returns true if the process exited within the timeout, false otherwise
    pub async fn wait_for_exit(&mut self, timeout: Duration) -> bool {
        tokio::time::timeout(timeout, async {
            loop {
                if !self.is_running() {
                    return;
                }
                tokio::time::sleep(Duration::from_millis(100)).await;
            }
        })
        .await
        .is_ok()
    }

    pub async fn spawn(mut config: Config) -> Result<Self, Elapsed> {
        let dir = create_tempdir().unwrap();
        let config_path = dir.path().join("validator.yaml");
        let file = std::fs::File::create(&config_path).unwrap();

        if !*IS_DEBUG_TRACING {
            // setup logging so that we can intercept it later in testing
            config.tracing.logger = LoggerLayer::File(FileConfig {
                directory: dir.path().to_owned(),
                prefix: Some(LOGS_PREFIX.into()),
            });
        }

        config.storage.db_path = dir.path().join("db");
        dir.path().clone_into(
            &mut config
                .da_verifier
                .storage_adapter_settings
                .blob_storage_directory,
        );

        let addr = config.http.backend_settings.address;
        let testing_addr = config.testing_http.backend_settings.address;

        let mut yaml_value = serde_yaml::to_value(&config).unwrap();
        inject_ibd_into_cryptarchia(&mut yaml_value);
        serde_yaml::to_writer(file, &yaml_value).unwrap();
        let child = Command::new(binary_path())
            .arg(&config_path)
            .current_dir(dir.path())
            .stdout(Stdio::inherit())
            .stderr(Stdio::inherit())
            .spawn()
            .unwrap();
        let node = Self {
            child,
            tempdir: dir,
            config,
            api: ApiClient::new(addr, Some(testing_addr)),
        };

        tokio::time::timeout(adjust_timeout(Duration::from_secs(10)), async {
            node.wait_online().await;
        })
        .await?;

        Ok(node)
    }

    #[must_use]
    pub fn url(&self) -> Url {
        self.api.base_url().clone()
    }

    #[must_use]
    pub fn testing_url(&self) -> Option<Url> {
        self.api.testing_url()
    }

    async fn wait_online(&self) {
        loop {
            if self.api.consensus_info().await.is_ok() {
                break;
            }
            tokio::time::sleep(Duration::from_millis(100)).await;
        }
    }

    pub async fn get_block(&self, id: HeaderId) -> Option<Block<SignedMantleTx>> {
        self.api.storage_block(&id).await.unwrap()
    }

    pub async fn get_commitments(&self, blob_id: BlobId) -> Option<DaSharesCommitments> {
        self.api
            .post_json_decode(DA_GET_SHARES_COMMITMENTS, &blob_id)
            .await
            .unwrap()
    }

    pub async fn get_mempoool_metrics(&self, pool: Pool) -> MempoolMetrics {
        let discr = match pool {
            Pool::Mantle => "mantle",
            Pool::Da => "da",
        };
        let res = self.api.mempool_metrics(discr).await.unwrap();
        MempoolMetrics {
            pending_items: res["pending_items"].as_u64().unwrap() as usize,
            last_item_timestamp: res["last_item_timestamp"].as_u64().unwrap(),
        }
    }

    pub async fn da_historic_sampling<I>(
        &self,
        block_id: HeaderId,
        blob_ids: I,
    ) -> Result<bool, reqwest::Error>
    where
        I: IntoIterator<Item = (BlobId, SessionNumber)>,
    {
        let request = HistoricSamplingRequest {
            block_id,
            blob_ids: blob_ids.into_iter().collect(),
        };

        self.api.da_historic_sampling(&request).await
    }

    // not async so that we can use this in `Drop`
    #[must_use]
    pub fn get_logs_from_file(&self) -> String {
        println!(
            "fetching logs from dir {}...",
            self.tempdir.path().display()
        );
        // std::thread::sleep(std::time::Duration::from_secs(50));
        std::fs::read_dir(self.tempdir.path())
            .unwrap()
            .filter_map(|entry| {
                let entry = entry.unwrap();
                let path = entry.path();
                (path.is_file() && path.to_str().unwrap().contains(LOGS_PREFIX)).then_some(path)
            })
            .map(|f| std::fs::read_to_string(f).unwrap())
            .collect::<String>()
    }

    #[must_use]
    pub const fn config(&self) -> &Config {
        &self.config
    }

    pub async fn get_headers(&self, from: Option<HeaderId>, to: Option<HeaderId>) -> Vec<HeaderId> {
        let mut req = self.api.get_builder(CRYPTARCHIA_HEADERS);

        if let Some(from) = from {
            req = req.query(&[("from", from)]);
        }

        if let Some(to) = to {
            req = req.query(&[("to", to)]);
        }

        let res = self.api.get_headers_raw(req).await;

        println!("res: {res:?}");

        res.unwrap().json::<Vec<HeaderId>>().await.unwrap()
    }

    pub async fn consensus_info(&self) -> CryptarchiaInfo {
        let info = self.api.consensus_info().await.unwrap();
        println!("{info:?}");
        info
    }

    pub async fn balancer_stats(&self) -> BalancerStats {
        self.api.balancer_stats().await.unwrap()
    }

    pub async fn monitor_stats(&self) -> MonitorStats {
        self.api.monitor_stats().await.unwrap()
    }

    pub async fn da_get_membership(
        &self,
        session_id: SessionNumber,
    ) -> Result<MembershipResponse, reqwest::Error> {
        self.api.da_get_membership(&session_id).await
    }

    pub async fn network_info(&self) -> Libp2pInfo {
        self.api.network_info().await.unwrap()
    }

    pub async fn get_shares(
        &self,
        blob_id: BlobId,
        requested_shares: HashSet<[u8; 2]>,
        filter_shares: HashSet<[u8; 2]>,
        return_available: bool,
    ) -> Result<impl Stream<Item = DaLightShare>, common_http_client::Error> {
        self.api
            .http_client()
            .get_shares::<DaShare>(
                self.api.base_url().clone(),
                blob_id,
                requested_shares,
                filter_shares,
                return_available,
            )
            .await
    }

    pub async fn get_storage_commitments(
        &self,
        blob_id: BlobId,
    ) -> Result<Option<DaSharesCommitments>, common_http_client::Error> {
        self.api
            .http_client()
            .get_storage_commitments::<DaShare>(self.api.base_url().clone(), blob_id)
            .await
    }

    pub async fn get_lib_stream(
        &self,
    ) -> Result<impl Stream<Item = BlockInfo>, common_http_client::Error> {
        self.api
            .http_client()
            .get_lib_stream(self.api.base_url().clone())
            .await
    }
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/capabilities.rs
────────────────────────────────────────────────
use async_trait::async_trait;

use super::DynError;

/// Marker type used by scenario builders to request node control support.
#[derive(Clone, Copy, Debug, Default)]
pub struct NodeControlCapability;

/// Trait implemented by scenario capability markers to signal whether node
/// control is required.
pub trait RequiresNodeControl {
    const REQUIRED: bool;
}

impl RequiresNodeControl for () {
    const REQUIRED: bool = false;
}

impl RequiresNodeControl for NodeControlCapability {
    const REQUIRED: bool = true;
}

/// Interface exposed by runners that can restart nodes at runtime.
#[async_trait]
pub trait NodeControlHandle: Send + Sync {
    async fn restart_validator(&self, index: usize) -> Result<(), DynError>;
    async fn restart_executor(&self, index: usize) -> Result<(), DynError>;
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/cfgsync.rs
────────────────────────────────────────────────
use std::{fs::File, num::NonZero, path::Path, time::Duration};

use anyhow::{Context as _, Result};
use nomos_da_network_core::swarm::ReplicationConfig;
use nomos_tracing_service::TracingSettings;
use nomos_utils::bounded_duration::{MinimalBoundedDuration, SECOND};
use serde::{Deserialize, Serialize};
use serde_with::serde_as;

use crate::topology::{GeneratedTopology, configs::wallet::WalletConfig};

#[serde_as]
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CfgSyncConfig {
    pub port: u16,
    pub n_hosts: usize,
    pub timeout: u64,
    pub security_param: NonZero<u32>,
    pub active_slot_coeff: f64,
    #[serde(default)]
    pub wallet: WalletConfig,
    #[serde(default)]
    pub ids: Option<Vec<[u8; 32]>>,
    #[serde(default)]
    pub da_ports: Option<Vec<u16>>,
    #[serde(default)]
    pub blend_ports: Option<Vec<u16>>,
    pub subnetwork_size: usize,
    pub dispersal_factor: usize,
    pub num_samples: u16,
    pub num_subnets: u16,
    #[serde_as(as = "MinimalBoundedDuration<0, SECOND>")]
    pub old_blobs_check_interval: Duration,
    #[serde_as(as = "MinimalBoundedDuration<0, SECOND>")]
    pub blobs_validity_duration: Duration,
    pub global_params_path: String,
    pub min_dispersal_peers: usize,
    pub min_replication_peers: usize,
    #[serde_as(as = "MinimalBoundedDuration<0, SECOND>")]
    pub monitor_failure_time_window: Duration,
    #[serde_as(as = "MinimalBoundedDuration<0, SECOND>")]
    pub balancer_interval: Duration,
    pub replication_settings: ReplicationConfig,
    pub retry_shares_limit: usize,
    pub retry_commitments_limit: usize,
    pub tracing_settings: TracingSettings,
}

pub fn load_cfgsync_template(path: &Path) -> Result<CfgSyncConfig> {
    let file = File::open(path)
        .with_context(|| format!("opening cfgsync template at {}", path.display()))?;
    serde_yaml::from_reader(file).context("parsing cfgsync template")
}

pub fn write_cfgsync_template(path: &Path, cfg: &CfgSyncConfig) -> Result<()> {
    let file = File::create(path)
        .with_context(|| format!("writing cfgsync template to {}", path.display()))?;
    let serializable = SerializableCfgSyncConfig::from(cfg);
    serde_yaml::to_writer(file, &serializable).context("serializing cfgsync template")
}

pub fn render_cfgsync_yaml(cfg: &CfgSyncConfig) -> Result<String> {
    let serializable = SerializableCfgSyncConfig::from(cfg);
    serde_yaml::to_string(&serializable).context("rendering cfgsync yaml")
}

pub fn apply_topology_overrides(
    cfg: &mut CfgSyncConfig,
    topology: &GeneratedTopology,
    use_kzg_mount: bool,
) {
    let hosts = topology.validators().len() + topology.executors().len();
    cfg.n_hosts = hosts;

    let consensus = &topology.config().consensus_params;
    cfg.security_param = consensus.security_param;
    cfg.active_slot_coeff = consensus.active_slot_coeff;

    let config = topology.config();
    cfg.wallet = config.wallet_config.clone();
    cfg.ids = Some(topology.nodes().map(|node| node.id).collect());
    cfg.da_ports = Some(topology.nodes().map(|node| node.da_port).collect());
    cfg.blend_ports = Some(topology.nodes().map(|node| node.blend_port).collect());

    let da = &config.da_params;
    cfg.subnetwork_size = da.subnetwork_size;
    cfg.dispersal_factor = da.dispersal_factor;
    cfg.num_samples = da.num_samples;
    cfg.num_subnets = da.num_subnets;
    cfg.old_blobs_check_interval = da.old_blobs_check_interval;
    cfg.blobs_validity_duration = da.blobs_validity_duration;
    cfg.global_params_path = if use_kzg_mount {
        "/kzgrs_test_params".into()
    } else {
        da.global_params_path.clone()
    };
    cfg.min_dispersal_peers = da.policy_settings.min_dispersal_peers;
    cfg.min_replication_peers = da.policy_settings.min_replication_peers;
    cfg.monitor_failure_time_window = da.monitor_settings.failure_time_window;
    cfg.balancer_interval = da.balancer_interval;
    cfg.replication_settings = da.replication_settings;
    cfg.retry_shares_limit = da.retry_shares_limit;
    cfg.retry_commitments_limit = da.retry_commitments_limit;
    cfg.tracing_settings = TracingSettings::default();
}

#[serde_as]
#[derive(Serialize)]
struct SerializableCfgSyncConfig {
    port: u16,
    n_hosts: usize,
    timeout: u64,
    security_param: NonZero<u32>,
    active_slot_coeff: f64,
    wallet: WalletConfig,
    #[serde(skip_serializing_if = "Option::is_none")]
    ids: Option<Vec<[u8; 32]>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    da_ports: Option<Vec<u16>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    blend_ports: Option<Vec<u16>>,
    subnetwork_size: usize,
    dispersal_factor: usize,
    num_samples: u16,
    num_subnets: u16,
    #[serde_as(as = "MinimalBoundedDuration<0, SECOND>")]
    old_blobs_check_interval: Duration,
    #[serde_as(as = "MinimalBoundedDuration<0, SECOND>")]
    blobs_validity_duration: Duration,
    global_params_path: String,
    min_dispersal_peers: usize,
    min_replication_peers: usize,
    #[serde_as(as = "MinimalBoundedDuration<0, SECOND>")]
    monitor_failure_time_window: Duration,
    #[serde_as(as = "MinimalBoundedDuration<0, SECOND>")]
    balancer_interval: Duration,
    replication_settings: ReplicationConfig,
    retry_shares_limit: usize,
    retry_commitments_limit: usize,
    tracing_settings: TracingSettings,
}

impl From<&CfgSyncConfig> for SerializableCfgSyncConfig {
    fn from(cfg: &CfgSyncConfig) -> Self {
        Self {
            port: cfg.port,
            n_hosts: cfg.n_hosts,
            timeout: cfg.timeout,
            security_param: cfg.security_param,
            active_slot_coeff: cfg.active_slot_coeff,
            wallet: cfg.wallet.clone(),
            ids: cfg.ids.clone(),
            da_ports: cfg.da_ports.clone(),
            blend_ports: cfg.blend_ports.clone(),
            subnetwork_size: cfg.subnetwork_size,
            dispersal_factor: cfg.dispersal_factor,
            num_samples: cfg.num_samples,
            num_subnets: cfg.num_subnets,
            old_blobs_check_interval: cfg.old_blobs_check_interval,
            blobs_validity_duration: cfg.blobs_validity_duration,
            global_params_path: cfg.global_params_path.clone(),
            min_dispersal_peers: cfg.min_dispersal_peers,
            min_replication_peers: cfg.min_replication_peers,
            monitor_failure_time_window: cfg.monitor_failure_time_window,
            balancer_interval: cfg.balancer_interval,
            replication_settings: cfg.replication_settings,
            retry_shares_limit: cfg.retry_shares_limit,
            retry_commitments_limit: cfg.retry_commitments_limit,
            tracing_settings: cfg.tracing_settings.clone(),
        }
    }
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/definition.rs
────────────────────────────────────────────────
use std::{num::NonZeroUsize, sync::Arc, time::Duration};

use super::{
    NodeControlCapability, expectation::Expectation, runtime::context::RunMetrics,
    workload::Workload,
};
use crate::topology::{
    GeneratedTopology, TopologyBuilder, TopologyConfig, configs::wallet::WalletConfig,
};

const DEFAULT_FUNDS_PER_WALLET: u64 = 100;

/// Immutable scenario definition shared between the runner, workloads, and
/// expectations.
pub struct Scenario<Caps = ()> {
    topology: GeneratedTopology,
    workloads: Vec<Arc<dyn Workload>>,
    expectations: Vec<Box<dyn Expectation>>,
    duration: Duration,
    capabilities: Caps,
}

impl<Caps> Scenario<Caps> {
    fn new(
        topology: GeneratedTopology,
        workloads: Vec<Arc<dyn Workload>>,
        expectations: Vec<Box<dyn Expectation>>,
        duration: Duration,
        capabilities: Caps,
    ) -> Self {
        Self {
            topology,
            workloads,
            expectations,
            duration,
            capabilities,
        }
    }

    #[must_use]
    pub const fn topology(&self) -> &GeneratedTopology {
        &self.topology
    }

    #[must_use]
    pub fn workloads(&self) -> &[Arc<dyn Workload>] {
        &self.workloads
    }

    #[must_use]
    pub fn expectations(&self) -> &[Box<dyn Expectation>] {
        &self.expectations
    }

    #[must_use]
    pub fn expectations_mut(&mut self) -> &mut [Box<dyn Expectation>] {
        &mut self.expectations
    }

    #[must_use]
    pub const fn duration(&self) -> Duration {
        self.duration
    }

    #[must_use]
    pub const fn capabilities(&self) -> &Caps {
        &self.capabilities
    }
}

/// Builder used by callers to describe the desired scenario.
pub struct Builder<Caps = ()> {
    topology: TopologyBuilder,
    workloads: Vec<Arc<dyn Workload>>,
    expectations: Vec<Box<dyn Expectation>>,
    duration: Duration,
    capabilities: Caps,
}

pub type ScenarioBuilder = Builder<()>;

impl<Caps: Default> Builder<Caps> {
    #[must_use]
    pub fn new(topology: TopologyBuilder) -> Self {
        Self {
            topology,
            workloads: Vec::new(),
            expectations: Vec::new(),
            duration: Duration::ZERO,
            capabilities: Caps::default(),
        }
    }

    #[must_use]
    pub fn with_node_counts(validators: usize, executors: usize) -> Self {
        Self::new(TopologyBuilder::new(TopologyConfig::with_node_numbers(
            validators, executors,
        )))
    }
}

impl<Caps> Builder<Caps> {
    #[must_use]
    pub fn with_capabilities<NewCaps>(self, capabilities: NewCaps) -> Builder<NewCaps> {
        let Self {
            topology,
            workloads,
            expectations,
            duration,
            ..
        } = self;

        Builder {
            topology,
            workloads,
            expectations,
            duration,
            capabilities,
        }
    }

    #[must_use]
    pub const fn capabilities(&self) -> &Caps {
        &self.capabilities
    }

    #[must_use]
    pub const fn capabilities_mut(&mut self) -> &mut Caps {
        &mut self.capabilities
    }

    #[must_use]
    pub fn with_workload<W>(mut self, workload: W) -> Self
    where
        W: Workload + 'static,
    {
        self.expectations.extend(workload.expectations());
        self.workloads.push(Arc::new(workload));
        self
    }

    #[must_use]
    pub fn with_expectation<E>(mut self, expectation: E) -> Self
    where
        E: Expectation + 'static,
    {
        self.expectations.push(Box::new(expectation));
        self
    }

    #[must_use]
    pub const fn with_run_duration(mut self, duration: Duration) -> Self {
        self.duration = duration;
        self
    }

    #[must_use]
    pub fn map_topology(mut self, f: impl FnOnce(TopologyBuilder) -> TopologyBuilder) -> Self {
        self.topology = f(self.topology);
        self
    }

    #[must_use]
    pub fn with_wallet_config(mut self, wallet: WalletConfig) -> Self {
        self.topology = self.topology.with_wallet_config(wallet);
        self
    }

    #[must_use]
    pub fn wallets(self, users: usize) -> Self {
        let user_count = NonZeroUsize::new(users).expect("wallet user count must be non-zero");
        let total_funds = DEFAULT_FUNDS_PER_WALLET
            .checked_mul(users as u64)
            .expect("wallet count exceeds capacity");
        let wallet = WalletConfig::uniform(total_funds, user_count);
        self.with_wallet_config(wallet)
    }

    #[must_use]
    pub fn build(self) -> Scenario<Caps> {
        let Self {
            topology,
            mut workloads,
            mut expectations,
            duration,
            capabilities,
            ..
        } = self;

        let generated = topology.build();
        let duration = enforce_min_duration(&generated, duration);
        let run_metrics = RunMetrics::from_topology(&generated, duration);
        initialize_components(&generated, &run_metrics, &mut workloads, &mut expectations);

        Scenario::new(generated, workloads, expectations, duration, capabilities)
    }
}

impl Builder<()> {
    #[must_use]
    pub fn enable_node_control(self) -> Builder<NodeControlCapability> {
        self.with_capabilities(NodeControlCapability)
    }
}

fn initialize_components(
    descriptors: &GeneratedTopology,
    run_metrics: &RunMetrics,
    workloads: &mut [Arc<dyn Workload>],
    expectations: &mut [Box<dyn Expectation>],
) {
    initialize_workloads(descriptors, run_metrics, workloads);
    initialize_expectations(descriptors, run_metrics, expectations);
}

fn initialize_workloads(
    descriptors: &GeneratedTopology,
    run_metrics: &RunMetrics,
    workloads: &mut [Arc<dyn Workload>],
) {
    for workload in workloads {
        let inner =
            Arc::get_mut(workload).expect("workload unexpectedly cloned before initialization");
        if let Err(err) = inner.init(descriptors, run_metrics) {
            panic!("workload '{}' failed to initialize: {err}", inner.name());
        }
    }
}

fn initialize_expectations(
    descriptors: &GeneratedTopology,
    run_metrics: &RunMetrics,
    expectations: &mut [Box<dyn Expectation>],
) {
    for expectation in expectations {
        if let Err(err) = expectation.init(descriptors, run_metrics) {
            panic!(
                "expectation '{}' failed to initialize: {err}",
                expectation.name()
            );
        }
    }
}

fn enforce_min_duration(descriptors: &GeneratedTopology, requested: Duration) -> Duration {
    const MIN_BLOCKS: u32 = 2;
    const FALLBACK_SECS: u64 = 10;

    let min_duration = descriptors.slot_duration().map_or_else(
        || Duration::from_secs(FALLBACK_SECS),
        |slot| slot * MIN_BLOCKS,
    );

    requested.max(min_duration)
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/expectation.rs
────────────────────────────────────────────────
use async_trait::async_trait;

use super::{DynError, RunContext, runtime::context::RunMetrics};
use crate::topology::GeneratedTopology;

#[async_trait]
pub trait Expectation: Send + Sync {
    fn name(&self) -> &str;

    fn init(
        &mut self,
        _descriptors: &GeneratedTopology,
        _run_metrics: &RunMetrics,
    ) -> Result<(), DynError> {
        Ok(())
    }

    async fn start_capture(&mut self, _ctx: &RunContext) -> Result<(), DynError> {
        Ok(())
    }

    async fn evaluate(&mut self, ctx: &RunContext) -> Result<(), DynError>;
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/expectations/mod.rs
────────────────────────────────────────────────
use std::{fmt::Write as _, time::Duration};

use futures::FutureExt as _;

use super::{
    BoxFuture, CONSENSUS_PROCESSED_BLOCKS, Expectation, ExpectationError, MetricsError, RunContext,
};

/// Enforces that every validator advances to the minimum block height implied
/// by the scenario duration, slot timing, and active slot coefficient (or a
/// caller-provided override).
///
/// Polls each validator's HTTP consensus info to catch stalls even when
/// Prometheus is unavailable.
#[derive(Clone, Copy, Debug)]
pub struct ConsensusLiveness {
    minimum_override: Option<u64>,
    tolerance: f64,
}

pub struct PrometheusBlockProduction {
    minimum: u64,
}

impl PrometheusBlockProduction {
    #[must_use]
    pub const fn new(minimum: u64) -> Self {
        Self { minimum }
    }

    #[must_use]
    pub const fn minimum(&self) -> u64 {
        self.minimum
    }
}

impl Expectation for PrometheusBlockProduction {
    fn name(&self) -> &'static str {
        "prometheus_block_production"
    }

    fn evaluate<'a>(&'a self, ctx: &'a RunContext) -> BoxFuture<'a, Result<(), ExpectationError>> {
        async move {
            let total = ctx
                .metrics()
                .consensus_processed_blocks()
                .map_err(|err| into_expectation_error(&err))?;

            if total >= self.minimum() as f64 {
                tracing::info!(
                    query = CONSENSUS_PROCESSED_BLOCKS,
                    observed_total = total,
                    minimum = self.minimum(),
                    "block production expectation satisfied via prometheus"
                );
                Ok(())
            } else {
                Err(ExpectationError::new(format!(
                    "prometheus query `{}` sum {total} below block target {}",
                    CONSENSUS_PROCESSED_BLOCKS,
                    self.minimum()
                )))
            }
        }
        .boxed()
    }
}

fn into_expectation_error(err: &MetricsError) -> ExpectationError {
    ExpectationError::new(err.to_string())
}

impl ConsensusLiveness {
    #[must_use]
    pub const fn with_minimum(minimum_blocks: u64) -> Self {
        Self {
            minimum_override: Some(minimum_blocks),
            tolerance: 1.0,
        }
    }

    #[must_use]
    pub const fn with_tolerance(tolerance: f64) -> Self {
        Self {
            minimum_override: None,
            tolerance,
        }
    }
}

impl Default for ConsensusLiveness {
    fn default() -> Self {
        Self::with_tolerance(0.8)
    }
}

impl Expectation for ConsensusLiveness {
    fn name(&self) -> &'static str {
        "consensus_liveness"
    }

    fn evaluate<'a>(&'a self, ctx: &'a RunContext) -> BoxFuture<'a, Result<(), ExpectationError>> {
        async move {
            if ctx.validators().is_empty() {
                return Err(ExpectationError::new(
                    "consensus liveness requires at least one validator",
                ));
            }

            let target = consensus_target_blocks(ctx, self.minimum_override, self.tolerance);
            let mut issues = Vec::new();
            let mut heights = Vec::with_capacity(ctx.validators().len());

            for handle in ctx.validators() {
                let index = handle.descriptor().index;
                match handle.client().consensus_info().await {
                    Ok(info) => {
                        heights.push(info.height);
                        if info.height < target {
                            issues.push(format!(
                                "validator-{index} height {} below target {}",
                                info.height, target
                            ));
                        }
                    }
                    Err(err) => {
                        issues.push(format!("validator-{index} consensus_info failed: {err}"));
                    }
                }
            }

            if issues.is_empty() {
                tracing::info!(
                    target,
                    heights = ?heights,
                    "consensus liveness expectation satisfied"
                );
                Ok(())
            } else {
                let mut message = String::new();
                let _ = writeln!(
                    &mut message,
                    "consensus liveness violated (target={target}):"
                );
                for issue in issues {
                    let _ = writeln!(&mut message, "- {issue}");
                }
                Err(ExpectationError::new(message.trim_end()))
            }
        }
        .boxed()
    }
}

fn consensus_target_blocks(ctx: &RunContext, override_minimum: Option<u64>, tolerance: f64) -> u64 {
    if let Some(minimum) = override_minimum {
        return minimum;
    }

    if tolerance <= 0.0 {
        return 0;
    }

    let slot_duration = ctx
        .descriptors()
        .validators()
        .first()
        .map_or(Duration::from_secs(2), |node| {
            node.general.time_config.slot_duration
        });

    if slot_duration.is_zero() {
        return 0;
    }

    let active_slot_coeff = ctx
        .descriptors()
        .config()
        .consensus_params
        .active_slot_coeff;

    if active_slot_coeff <= 0.0 {
        return 0;
    }

    let run_duration = ctx.run_duration();
    if run_duration.is_zero() {
        return 0;
    }

    let slot_duration_secs = slot_duration.as_secs_f64();
    if slot_duration_secs == 0.0 {
        return 0;
    }

    let slot_count = run_duration.as_secs_f64() / slot_duration_secs;
    if slot_count < 1.0 {
        return 0;
    }

    let expected_blocks = slot_count * active_slot_coeff;
    let adjusted = (expected_blocks * tolerance).floor();
    adjusted.max(1.0) as u64
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/http_probe.rs
────────────────────────────────────────────────
use std::{fmt, time::Duration};

use futures::future::try_join_all;
use nomos_http_api_common::paths;
use reqwest::Client as ReqwestClient;
use thiserror::Error;
use tokio::time::{sleep, timeout};

#[derive(Clone, Copy, Debug, Eq, PartialEq)]
pub enum NodeRole {
    Validator,
    Executor,
}

impl NodeRole {
    #[must_use]
    pub const fn label(self) -> &'static str {
        match self {
            Self::Validator => "validator",
            Self::Executor => "executor",
        }
    }
}

impl fmt::Display for NodeRole {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        f.write_str(self.label())
    }
}

#[derive(Clone, Copy, Debug, Error)]
#[error("timeout waiting for {role} HTTP endpoint on port {port} after {timeout:?}")]
pub struct HttpReadinessError {
    role: NodeRole,
    port: u16,
    timeout: Duration,
}

impl HttpReadinessError {
    #[must_use]
    pub const fn new(role: NodeRole, port: u16, timeout: Duration) -> Self {
        Self {
            role,
            port,
            timeout,
        }
    }

    #[must_use]
    pub const fn role(&self) -> NodeRole {
        self.role
    }

    #[must_use]
    pub const fn port(&self) -> u16 {
        self.port
    }

    #[must_use]
    pub const fn timeout(&self) -> Duration {
        self.timeout
    }
}

pub async fn wait_for_http_ports(
    ports: &[u16],
    role: NodeRole,
    timeout_duration: Duration,
    poll_interval: Duration,
) -> Result<(), HttpReadinessError> {
    wait_for_http_ports_with_host(ports, role, "127.0.0.1", timeout_duration, poll_interval).await
}

pub async fn wait_for_http_ports_with_host(
    ports: &[u16],
    role: NodeRole,
    host: &str,
    timeout_duration: Duration,
    poll_interval: Duration,
) -> Result<(), HttpReadinessError> {
    if ports.is_empty() {
        return Ok(());
    }

    let client = ReqwestClient::new();
    let probes = ports.iter().copied().map(|port| {
        wait_for_single_port(
            client.clone(),
            port,
            role,
            host,
            timeout_duration,
            poll_interval,
        )
    });

    try_join_all(probes).await.map(|_| ())
}

async fn wait_for_single_port(
    client: ReqwestClient,
    port: u16,
    role: NodeRole,
    host: &str,
    timeout_duration: Duration,
    poll_interval: Duration,
) -> Result<(), HttpReadinessError> {
    let url = format!("http://{host}:{port}{}", paths::CRYPTARCHIA_INFO);
    let probe = async {
        loop {
            let is_ready = client
                .get(&url)
                .send()
                .await
                .map(|response| response.status().is_success())
                .unwrap_or(false);

            if is_ready {
                return;
            }

            sleep(poll_interval).await;
        }
    };

    timeout(timeout_duration, probe)
        .await
        .map_err(|_| HttpReadinessError::new(role, port, timeout_duration))
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/mod.rs
────────────────────────────────────────────────
//! Scenario orchestration primitives shared by integration tests and runners.

mod capabilities;
pub mod cfgsync;
mod definition;
mod expectation;
pub mod http_probe;
mod runtime;
mod workload;

pub type DynError = Box<dyn std::error::Error + Send + Sync + 'static>;

pub use capabilities::{NodeControlCapability, NodeControlHandle, RequiresNodeControl};
pub use definition::{Builder, Scenario, ScenarioBuilder};
pub use expectation::Expectation;
pub use runtime::{
    BlockFeed, BlockFeedTask, BlockRecord, BlockStats, CleanupGuard, Deployer, NodeClients,
    RunContext, RunHandle, RunMetrics, Runner, ScenarioError,
    metrics::{
        CONSENSUS_PROCESSED_BLOCKS, CONSENSUS_TRANSACTIONS_TOTAL, Metrics, MetricsError,
        PrometheusEndpoint, PrometheusInstantSample,
    },
    spawn_block_feed,
};
pub use workload::Workload;




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/runtime/block_feed.rs
────────────────────────────────────────────────
use std::{
    collections::HashSet,
    sync::{
        Arc,
        atomic::{AtomicU64, Ordering},
    },
    time::Duration,
};

use anyhow::{Context as _, Result};
use nomos_core::{block::Block, mantle::SignedMantleTx};
use nomos_node::HeaderId;
use tokio::{sync::broadcast, task::JoinHandle, time::sleep};
use tracing::{debug, error};

use super::context::CleanupGuard;
use crate::nodes::ApiClient;

const POLL_INTERVAL: Duration = Duration::from_secs(1);

#[derive(Clone)]
pub struct BlockFeed {
    inner: Arc<BlockFeedInner>,
}

struct BlockFeedInner {
    sender: broadcast::Sender<Arc<BlockRecord>>,
    stats: Arc<BlockStats>,
}

#[derive(Clone)]
pub struct BlockRecord {
    pub header: HeaderId,
    pub block: Arc<Block<SignedMantleTx>>,
}

pub struct BlockFeedTask {
    handle: JoinHandle<()>,
}

impl BlockFeed {
    #[must_use]
    pub fn subscribe(&self) -> broadcast::Receiver<Arc<BlockRecord>> {
        self.inner.sender.subscribe()
    }

    #[must_use]
    pub fn stats(&self) -> Arc<BlockStats> {
        Arc::clone(&self.inner.stats)
    }

    fn ingest(&self, header: HeaderId, block: Block<SignedMantleTx>) {
        self.inner.stats.record_block(&block);
        let record = Arc::new(BlockRecord {
            header,
            block: Arc::new(block),
        });

        let _ = self.inner.sender.send(record);
    }
}

impl BlockFeedTask {
    #[must_use]
    pub const fn new(handle: JoinHandle<()>) -> Self {
        Self { handle }
    }
}

pub async fn spawn_block_feed(client: ApiClient) -> Result<(BlockFeed, BlockFeedTask)> {
    let (sender, _) = broadcast::channel(1024);
    let feed = BlockFeed {
        inner: Arc::new(BlockFeedInner {
            sender,
            stats: Arc::new(BlockStats::default()),
        }),
    };

    let mut scanner = BlockScanner::new(client, feed.clone());
    scanner.catch_up().await?;

    let handle = tokio::spawn(async move { scanner.run().await });

    Ok((feed, BlockFeedTask::new(handle)))
}

struct BlockScanner {
    client: ApiClient,
    feed: BlockFeed,
    seen: HashSet<HeaderId>,
}

impl BlockScanner {
    fn new(client: ApiClient, feed: BlockFeed) -> Self {
        Self {
            client,
            feed,
            seen: HashSet::new(),
        }
    }

    async fn run(&mut self) {
        loop {
            if let Err(err) = self.catch_up().await {
                error!(%err, "block feed catch up failed");
            }
            sleep(POLL_INTERVAL).await;
        }
    }

    async fn catch_up(&mut self) -> Result<()> {
        let info = self.client.consensus_info().await?;
        let tip = info.tip;
        let mut remaining_height = info.height;
        let mut stack = Vec::new();
        let mut cursor = tip;

        loop {
            if self.seen.contains(&cursor) {
                break;
            }

            if remaining_height == 0 {
                self.seen.insert(cursor);
                break;
            }

            let block = self
                .client
                .storage_block(&cursor)
                .await?
                .context("missing block while catching up")?;

            let parent = block.header().parent();
            stack.push((cursor, block));

            if self.seen.contains(&parent) || parent == cursor {
                break;
            }

            cursor = parent;
            remaining_height = remaining_height.saturating_sub(1);
        }

        let mut processed = 0usize;
        while let Some((header, block)) = stack.pop() {
            self.feed.ingest(header, block);
            self.seen.insert(header);
            processed += 1;
        }

        debug!(processed, "block feed processed catch up batch");
        Ok(())
    }
}

impl CleanupGuard for BlockFeedTask {
    fn cleanup(self: Box<Self>) {
        self.handle.abort();
    }
}

#[derive(Default)]
pub struct BlockStats {
    total_transactions: AtomicU64,
}

impl BlockStats {
    fn record_block(&self, block: &Block<SignedMantleTx>) {
        self.total_transactions
            .fetch_add(block.transactions().len() as u64, Ordering::Relaxed);
    }

    #[must_use]
    pub fn total_transactions(&self) -> u64 {
        self.total_transactions.load(Ordering::Relaxed)
    }
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/runtime/context.rs
────────────────────────────────────────────────
use std::{sync::Arc, time::Duration};

use super::{block_feed::BlockFeed, metrics::Metrics, node_clients::ClusterClient};
use crate::{
    nodes::ApiClient,
    scenario::{NodeClients, NodeControlHandle},
    topology::{GeneratedTopology, Topology, configs::wallet::WalletAccount},
};

pub struct RunContext {
    descriptors: GeneratedTopology,
    cluster: Option<Topology>,
    node_clients: NodeClients,
    metrics: RunMetrics,
    telemetry: Metrics,
    block_feed: BlockFeed,
    node_control: Option<Arc<dyn NodeControlHandle>>,
}

impl RunContext {
    /// Builds a run context, clamping the requested duration so we always run
    /// for at least a couple of slot lengths (or a fallback window if slots are
    /// unknown).
    #[must_use]
    pub fn new(
        descriptors: GeneratedTopology,
        cluster: Option<Topology>,
        node_clients: NodeClients,
        run_duration: Duration,
        telemetry: Metrics,
        block_feed: BlockFeed,
        node_control: Option<Arc<dyn NodeControlHandle>>,
    ) -> Self {
        let metrics = RunMetrics::new(&descriptors, run_duration);

        Self {
            descriptors,
            cluster,
            node_clients,
            metrics,
            telemetry,
            block_feed,
            node_control,
        }
    }

    #[must_use]
    pub const fn descriptors(&self) -> &GeneratedTopology {
        &self.descriptors
    }

    #[must_use]
    pub const fn topology(&self) -> Option<&Topology> {
        self.cluster.as_ref()
    }

    #[must_use]
    pub const fn node_clients(&self) -> &NodeClients {
        &self.node_clients
    }

    #[must_use]
    pub fn random_node_client(&self) -> Option<&ApiClient> {
        self.node_clients.any_client()
    }

    #[must_use]
    pub fn block_feed(&self) -> BlockFeed {
        self.block_feed.clone()
    }

    #[must_use]
    pub fn wallet_accounts(&self) -> &[WalletAccount] {
        self.descriptors.wallet_accounts()
    }

    #[must_use]
    pub const fn telemetry(&self) -> &Metrics {
        &self.telemetry
    }

    #[must_use]
    pub const fn run_duration(&self) -> Duration {
        self.metrics.run_duration()
    }

    #[must_use]
    pub const fn expected_blocks(&self) -> u64 {
        self.metrics.expected_consensus_blocks()
    }

    #[must_use]
    pub const fn run_metrics(&self) -> RunMetrics {
        self.metrics
    }

    #[must_use]
    pub fn node_control(&self) -> Option<Arc<dyn NodeControlHandle>> {
        self.node_control.clone()
    }

    #[must_use]
    pub const fn cluster_client(&self) -> ClusterClient<'_> {
        self.node_clients.cluster_client()
    }
}

/// Handle returned by the runner to control the lifecycle of the run.
pub struct RunHandle {
    run_context: Arc<RunContext>,
    cleanup_guard: Option<Box<dyn CleanupGuard>>,
}

impl Drop for RunHandle {
    fn drop(&mut self) {
        if let Some(guard) = self.cleanup_guard.take() {
            guard.cleanup();
        }
    }
}

impl RunHandle {
    #[must_use]
    pub fn new(context: RunContext, cleanup_guard: Option<Box<dyn CleanupGuard>>) -> Self {
        Self {
            run_context: Arc::new(context),
            cleanup_guard,
        }
    }

    #[must_use]
    pub(crate) fn from_shared(
        context: Arc<RunContext>,
        cleanup_guard: Option<Box<dyn CleanupGuard>>,
    ) -> Self {
        Self {
            run_context: context,
            cleanup_guard,
        }
    }

    #[must_use]
    pub fn context(&self) -> &RunContext {
        &self.run_context
    }
}

#[derive(Clone, Copy)]
pub struct RunMetrics {
    run_duration: Duration,
    expected_blocks: u64,
    block_interval_hint: Option<Duration>,
}

impl RunMetrics {
    #[must_use]
    pub fn new(descriptors: &GeneratedTopology, run_duration: Duration) -> Self {
        Self::from_topology(descriptors, run_duration)
    }

    #[must_use]
    pub fn from_topology(descriptors: &GeneratedTopology, run_duration: Duration) -> Self {
        let slot_duration = descriptors.slot_duration();

        let active_slot_coeff = descriptors.config().consensus_params.active_slot_coeff;
        let expected_blocks =
            calculate_expected_blocks(run_duration, slot_duration, active_slot_coeff);

        let block_interval_hint =
            slot_duration.map(|duration| duration.mul_f64(active_slot_coeff.clamp(0.0, 1.0)));

        Self {
            run_duration,
            expected_blocks,
            block_interval_hint,
        }
    }

    #[must_use]
    pub const fn run_duration(&self) -> Duration {
        self.run_duration
    }

    #[must_use]
    pub const fn expected_consensus_blocks(&self) -> u64 {
        self.expected_blocks
    }

    #[must_use]
    pub const fn block_interval_hint(&self) -> Option<Duration> {
        self.block_interval_hint
    }
}

pub trait CleanupGuard: Send {
    fn cleanup(self: Box<Self>);
}

/// Computes the minimum duration we’ll allow for a scenario run so that the
/// scheduler can observe a few block opportunities even if the caller
/// requested an extremely short window.
fn calculate_expected_blocks(
    run_duration: Duration,
    slot_duration: Option<Duration>,
    active_slot_coeff: f64,
) -> u64 {
    let Some(slot_duration) = slot_duration else {
        return 0;
    };
    let slot_secs = slot_duration.as_secs_f64();
    let run_secs = run_duration.as_secs_f64();
    let expected = run_secs / slot_secs * active_slot_coeff;

    expected.ceil().clamp(0.0, u64::MAX as f64) as u64
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/runtime/deployer.rs
────────────────────────────────────────────────
use async_trait::async_trait;

use super::runner::Runner;
use crate::scenario::{DynError, Scenario};

/// Error returned when executing workloads or expectations.
#[derive(Debug, thiserror::Error)]
pub enum ScenarioError {
    #[error("workload failure: {0}")]
    Workload(#[source] DynError),
    #[error("expectation capture failed: {0}")]
    ExpectationCapture(#[source] DynError),
    #[error("expectations failed:\n{0}")]
    Expectations(#[source] DynError),
}

#[async_trait]
pub trait Deployer<Caps = ()>: Send + Sync {
    type Error;

    async fn deploy(&self, scenario: &Scenario<Caps>) -> Result<Runner, Self::Error>;
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/runtime/metrics.rs
────────────────────────────────────────────────
use std::{collections::HashMap, sync::Arc};

use prometheus_http_query::{Client as PrometheusClient, response::Data as PrometheusData};
use reqwest::Url;
use tracing::warn;

pub const CONSENSUS_PROCESSED_BLOCKS: &str = "consensus_processed_blocks";
pub const CONSENSUS_TRANSACTIONS_TOTAL: &str = "consensus_transactions_total";
const CONSENSUS_TRANSACTIONS_VALIDATOR_QUERY: &str =
    r#"sum(consensus_transactions_total{job=~"validator-.*"})"#;

#[derive(Clone, Default)]
pub struct Metrics {
    prometheus: Option<Arc<PrometheusEndpoint>>,
}

impl Metrics {
    #[must_use]
    pub const fn empty() -> Self {
        Self { prometheus: None }
    }

    pub fn from_prometheus(url: Url) -> Result<Self, MetricsError> {
        let handle = Arc::new(PrometheusEndpoint::new(url)?);
        Ok(Self::empty().with_prometheus_endpoint(handle))
    }

    pub fn from_prometheus_str(raw_url: &str) -> Result<Self, MetricsError> {
        Url::parse(raw_url)
            .map_err(|err| MetricsError::new(format!("invalid prometheus url: {err}")))
            .and_then(Self::from_prometheus)
    }

    #[must_use]
    pub fn with_prometheus_endpoint(mut self, handle: Arc<PrometheusEndpoint>) -> Self {
        self.prometheus = Some(handle);
        self
    }

    #[must_use]
    pub fn prometheus(&self) -> Option<Arc<PrometheusEndpoint>> {
        self.prometheus.as_ref().map(Arc::clone)
    }

    #[must_use]
    pub const fn is_configured(&self) -> bool {
        self.prometheus.is_some()
    }

    pub fn instant_values(&self, query: &str) -> Result<Vec<f64>, MetricsError> {
        let handle = self
            .prometheus()
            .ok_or_else(|| MetricsError::new("prometheus endpoint unavailable"))?;
        handle.instant_values(query)
    }

    pub fn counter_value(&self, query: &str) -> Result<f64, MetricsError> {
        let handle = self
            .prometheus()
            .ok_or_else(|| MetricsError::new("prometheus endpoint unavailable"))?;
        handle.counter_value(query)
    }

    pub fn consensus_processed_blocks(&self) -> Result<f64, MetricsError> {
        self.counter_value(CONSENSUS_PROCESSED_BLOCKS)
    }

    pub fn consensus_transactions_total(&self) -> Result<f64, MetricsError> {
        let handle = self
            .prometheus()
            .ok_or_else(|| MetricsError::new("prometheus endpoint unavailable"))?;

        match handle.instant_samples(CONSENSUS_TRANSACTIONS_VALIDATOR_QUERY) {
            Ok(samples) if !samples.is_empty() => {
                return Ok(samples.into_iter().map(|sample| sample.value).sum());
            }
            Ok(_) => {
                warn!(
                    query = CONSENSUS_TRANSACTIONS_VALIDATOR_QUERY,
                    "validator-specific consensus transaction metric returned no samples; falling back to aggregate counter"
                );
            }
            Err(err) => {
                warn!(
                    query = CONSENSUS_TRANSACTIONS_VALIDATOR_QUERY,
                    error = %err,
                    "failed to query validator-specific consensus transaction metric; falling back to aggregate counter"
                );
            }
        }

        handle.counter_value(CONSENSUS_TRANSACTIONS_TOTAL)
    }
}

#[derive(Debug, thiserror::Error)]
pub enum MetricsError {
    #[error("{0}")]
    Store(String),
}

impl MetricsError {
    #[must_use]
    pub fn new(message: impl Into<String>) -> Self {
        Self::Store(message.into())
    }
}

pub struct PrometheusEndpoint {
    base_url: Url,
    client: PrometheusClient,
}

#[derive(Clone, Debug)]
pub struct PrometheusInstantSample {
    pub labels: HashMap<String, String>,
    pub timestamp: f64,
    pub value: f64,
}

impl PrometheusEndpoint {
    pub fn new(base_url: Url) -> Result<Self, MetricsError> {
        let client = PrometheusClient::try_from(base_url.as_str().to_owned()).map_err(|err| {
            MetricsError::new(format!("failed to create prometheus client: {err}"))
        })?;

        Ok(Self { base_url, client })
    }

    #[must_use]
    pub const fn base_url(&self) -> &Url {
        &self.base_url
    }

    #[must_use]
    pub fn port(&self) -> Option<u16> {
        self.base_url.port_or_known_default()
    }

    pub fn instant_samples(
        &self,
        query: &str,
    ) -> Result<Vec<PrometheusInstantSample>, MetricsError> {
        let query = query.to_owned();
        let client = self.client.clone();

        let response = std::thread::spawn(move || -> Result<_, MetricsError> {
            let runtime = tokio::runtime::Runtime::new()
                .map_err(|err| MetricsError::new(format!("failed to create runtime: {err}")))?;
            runtime
                .block_on(async { client.query(&query).get().await })
                .map_err(|err| MetricsError::new(format!("prometheus query failed: {err}")))
        })
        .join()
        .map_err(|_| MetricsError::new("prometheus query thread panicked"))??;

        let mut samples = Vec::new();
        match response.data() {
            PrometheusData::Vector(vectors) => {
                for vector in vectors {
                    samples.push(PrometheusInstantSample {
                        labels: vector.metric().clone(),
                        timestamp: vector.sample().timestamp(),
                        value: vector.sample().value(),
                    });
                }
            }
            PrometheusData::Matrix(ranges) => {
                for range in ranges {
                    let labels = range.metric().clone();
                    for sample in range.samples() {
                        samples.push(PrometheusInstantSample {
                            labels: labels.clone(),
                            timestamp: sample.timestamp(),
                            value: sample.value(),
                        });
                    }
                }
            }
            PrometheusData::Scalar(sample) => {
                samples.push(PrometheusInstantSample {
                    labels: HashMap::new(),
                    timestamp: sample.timestamp(),
                    value: sample.value(),
                });
            }
        }

        Ok(samples)
    }

    pub fn instant_values(&self, query: &str) -> Result<Vec<f64>, MetricsError> {
        self.instant_samples(query)
            .map(|samples| samples.into_iter().map(|sample| sample.value).collect())
    }

    pub fn counter_value(&self, query: &str) -> Result<f64, MetricsError> {
        self.instant_values(query)
            .map(|values| values.into_iter().sum())
    }
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/runtime/mod.rs
────────────────────────────────────────────────
mod block_feed;
pub mod context;
mod deployer;
pub mod metrics;
mod node_clients;
mod runner;

pub use block_feed::{BlockFeed, BlockFeedTask, BlockRecord, BlockStats, spawn_block_feed};
pub use context::{CleanupGuard, RunContext, RunHandle, RunMetrics};
pub use deployer::{Deployer, ScenarioError};
pub use node_clients::NodeClients;
pub use runner::Runner;




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/runtime/node_clients.rs
────────────────────────────────────────────────
use std::pin::Pin;

use rand::{Rng as _, seq::SliceRandom as _, thread_rng};

use crate::{
    nodes::ApiClient,
    scenario::DynError,
    topology::{GeneratedTopology, Topology},
};

#[derive(Clone, Default)]
pub struct NodeClients {
    validators: Vec<ApiClient>,
    executors: Vec<ApiClient>,
}

impl NodeClients {
    #[must_use]
    pub const fn new(validators: Vec<ApiClient>, executors: Vec<ApiClient>) -> Self {
        Self {
            validators,
            executors,
        }
    }

    #[must_use]
    pub fn from_topology(_descriptors: &GeneratedTopology, topology: &Topology) -> Self {
        let validator_clients = topology.validators().iter().map(|node| {
            let testing = node.testing_url();
            ApiClient::from_urls(node.url(), testing)
        });

        let executor_clients = topology.executors().iter().map(|node| {
            let testing = node.testing_url();
            ApiClient::from_urls(node.url(), testing)
        });

        Self::new(validator_clients.collect(), executor_clients.collect())
    }

    #[must_use]
    pub fn validator_clients(&self) -> &[ApiClient] {
        &self.validators
    }

    #[must_use]
    pub fn executor_clients(&self) -> &[ApiClient] {
        &self.executors
    }

    #[must_use]
    pub fn random_validator(&self) -> Option<&ApiClient> {
        if self.validators.is_empty() {
            return None;
        }
        let mut rng = thread_rng();
        let idx = rng.gen_range(0..self.validators.len());
        self.validators.get(idx)
    }

    #[must_use]
    pub fn random_executor(&self) -> Option<&ApiClient> {
        if self.executors.is_empty() {
            return None;
        }
        let mut rng = thread_rng();
        let idx = rng.gen_range(0..self.executors.len());
        self.executors.get(idx)
    }

    pub fn all_clients(&self) -> impl Iterator<Item = &ApiClient> {
        self.validators.iter().chain(self.executors.iter())
    }

    #[must_use]
    pub fn any_client(&self) -> Option<&ApiClient> {
        let validator_count = self.validators.len();
        let executor_count = self.executors.len();
        let total = validator_count + executor_count;
        if total == 0 {
            return None;
        }
        let mut rng = thread_rng();
        let choice = rng.gen_range(0..total);
        if choice < validator_count {
            self.validators.get(choice)
        } else {
            self.executors.get(choice - validator_count)
        }
    }

    #[must_use]
    pub const fn cluster_client(&self) -> ClusterClient<'_> {
        ClusterClient::new(self)
    }
}

pub struct ClusterClient<'a> {
    node_clients: &'a NodeClients,
}

impl<'a> ClusterClient<'a> {
    #[must_use]
    pub const fn new(node_clients: &'a NodeClients) -> Self {
        Self { node_clients }
    }

    pub async fn try_all_clients<T, E>(
        &self,
        mut f: impl for<'b> FnMut(
            &'b ApiClient,
        ) -> Pin<Box<dyn Future<Output = Result<T, E>> + Send + 'b>>
        + Send,
    ) -> Result<T, DynError>
    where
        E: Into<DynError>,
    {
        let mut clients: Vec<&ApiClient> = self.node_clients.all_clients().collect();
        if clients.is_empty() {
            return Err("cluster client has no api clients".into());
        }

        clients.shuffle(&mut thread_rng());

        let mut last_err = None;
        for client in clients {
            match f(client).await {
                Ok(value) => return Ok(value),
                Err(err) => last_err = Some(err.into()),
            }
        }

        Err(last_err.unwrap_or_else(|| "cluster client exhausted all nodes".into()))
    }
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/runtime/runner.rs
────────────────────────────────────────────────
use std::{any::Any, panic::AssertUnwindSafe, sync::Arc, time::Duration};

use futures::FutureExt as _;
use tokio::{
    task::JoinSet,
    time::{sleep, timeout},
};

use super::deployer::ScenarioError;
use crate::scenario::{
    DynError, Expectation, Scenario,
    runtime::context::{CleanupGuard, RunContext, RunHandle},
};

type WorkloadOutcome = Result<(), DynError>;

/// Represents a fully prepared environment capable of executing a scenario.
pub struct Runner {
    context: Arc<RunContext>,
    cleanup_guard: Option<Box<dyn CleanupGuard>>,
}

impl Runner {
    #[must_use]
    pub fn new(context: RunContext, cleanup_guard: Option<Box<dyn CleanupGuard>>) -> Self {
        Self {
            context: Arc::new(context),
            cleanup_guard,
        }
    }

    #[must_use]
    pub fn context(&self) -> Arc<RunContext> {
        Arc::clone(&self.context)
    }

    pub(crate) fn cleanup(&mut self) {
        if let Some(guard) = self.cleanup_guard.take() {
            guard.cleanup();
        }
    }

    pub(crate) fn into_run_handle(mut self) -> RunHandle {
        RunHandle::from_shared(Arc::clone(&self.context), self.cleanup_guard.take())
    }

    /// Executes the scenario by driving workloads first and then evaluating all
    /// expectations. On any failure it cleans up resources and propagates the
    /// error to the caller.
    pub async fn run<Caps>(
        mut self,
        scenario: &mut Scenario<Caps>,
    ) -> Result<RunHandle, ScenarioError>
    where
        Caps: Send + Sync,
    {
        let context = self.context();
        if let Err(error) =
            Self::prepare_expectations(scenario.expectations_mut(), context.as_ref()).await
        {
            self.cleanup();
            return Err(error);
        }

        if let Err(error) = Self::run_workloads(&context, scenario).await {
            self.cleanup();
            return Err(error);
        }

        Self::cooldown(&context).await;

        if let Err(error) =
            Self::run_expectations(scenario.expectations_mut(), context.as_ref()).await
        {
            self.cleanup();
            return Err(error);
        }

        Ok(self.into_run_handle())
    }

    async fn prepare_expectations(
        expectations: &mut [Box<dyn Expectation>],
        context: &RunContext,
    ) -> Result<(), ScenarioError> {
        for expectation in expectations {
            if let Err(source) = expectation.start_capture(context).await {
                return Err(ScenarioError::ExpectationCapture(source));
            }
        }
        Ok(())
    }

    /// Spawns every workload, waits until the configured duration elapses (or a
    /// workload fails), and then aborts the remaining tasks.
    async fn run_workloads<Caps>(
        context: &Arc<RunContext>,
        scenario: &Scenario<Caps>,
    ) -> Result<(), ScenarioError>
    where
        Caps: Send + Sync,
    {
        let mut workloads = Self::spawn_workloads(scenario, context);
        let _ = Self::drive_until_timer(&mut workloads, scenario.duration()).await?;
        Self::drain_workloads(&mut workloads).await
    }

    /// Evaluates every registered expectation, aggregating failures so callers
    /// can see all missing conditions in a single report.
    async fn run_expectations(
        expectations: &mut [Box<dyn Expectation>],
        context: &RunContext,
    ) -> Result<(), ScenarioError> {
        let mut failures: Vec<(String, DynError)> = Vec::new();
        for expectation in expectations {
            if let Err(source) = expectation.evaluate(context).await {
                failures.push((expectation.name().to_owned(), source));
            }
        }

        if failures.is_empty() {
            return Ok(());
        }

        let summary = failures
            .into_iter()
            .map(|(name, source)| format!("{name}: {source}"))
            .collect::<Vec<_>>()
            .join("\n");

        Err(ScenarioError::Expectations(summary.into()))
    }

    async fn cooldown(context: &Arc<RunContext>) {
        let metrics = context.run_metrics();
        let needs_stabilization = context.node_control().is_some();

        if let Some(interval) = metrics.block_interval_hint() {
            if interval.is_zero() {
                return;
            }
            let mut wait = interval.mul_f64(5.0);
            if needs_stabilization {
                let minimum = Duration::from_secs(30);
                if wait < minimum {
                    wait = minimum;
                }
            }
            if !wait.is_zero() {
                sleep(wait).await;
            }
        } else if needs_stabilization {
            sleep(Duration::from_secs(30)).await;
        }
    }

    /// Spawns each workload inside its own task and returns the join set for
    /// cooperative management.
    fn spawn_workloads<Caps>(
        scenario: &Scenario<Caps>,
        context: &Arc<RunContext>,
    ) -> JoinSet<WorkloadOutcome>
    where
        Caps: Send + Sync,
    {
        let mut workloads = JoinSet::new();
        for workload in scenario.workloads() {
            let workload = Arc::clone(workload);
            let ctx = Arc::clone(context);

            workloads.spawn(async move {
                let outcome = AssertUnwindSafe(async { workload.start(ctx.as_ref()).await })
                    .catch_unwind()
                    .await;

                outcome.unwrap_or_else(|panic| {
                    Err(format!("workload panicked: {}", panic_message(panic)).into())
                })
            });
        }

        workloads
    }

    /// Polls workload tasks until the timeout fires or one reports an error.
    async fn drive_until_timer(
        workloads: &mut JoinSet<WorkloadOutcome>,
        duration: Duration,
    ) -> Result<bool, ScenarioError> {
        let run_future = async {
            while let Some(result) = workloads.join_next().await {
                Self::map_join_result(result)?;
            }
            Ok(())
        };

        timeout(duration, run_future)
            .await
            .map_or(Ok(true), |result| {
                result?;
                Ok(false)
            })
    }

    /// Aborts and drains any remaining workload tasks so we do not leak work
    /// across scenario runs.
    async fn drain_workloads(
        workloads: &mut JoinSet<WorkloadOutcome>,
    ) -> Result<(), ScenarioError> {
        workloads.abort_all();

        while let Some(result) = workloads.join_next().await {
            Self::map_join_result(result)?;
        }

        Ok(())
    }

    /// Converts the outcome of a workload task into the canonical scenario
    /// error, tolerating cancellation when the runner aborts unfinished tasks.
    fn map_join_result(
        result: Result<WorkloadOutcome, tokio::task::JoinError>,
    ) -> Result<(), ScenarioError> {
        match result {
            Ok(outcome) => outcome.map_err(ScenarioError::Workload),
            Err(join_err) if join_err.is_cancelled() => Ok(()),
            Err(join_err) => Err(ScenarioError::Workload(
                format!("workload task failed: {join_err}").into(),
            )),
        }
    }
}

/// Attempts to turn a panic payload into a readable string for diagnostics.
fn panic_message(panic: Box<dyn Any + Send>) -> String {
    panic.downcast::<String>().map_or_else(
        |panic| {
            panic.downcast::<&'static str>().map_or_else(
                |_| "unknown panic".to_owned(),
                |message| (*message).to_owned(),
            )
        },
        |message| *message,
    )
}

impl Drop for Runner {
    fn drop(&mut self) {
        self.cleanup();
    }
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/scenario/workload.rs
────────────────────────────────────────────────
use async_trait::async_trait;

use super::{DynError, Expectation, RunContext, runtime::context::RunMetrics};
use crate::topology::GeneratedTopology;

#[async_trait]
pub trait Workload: Send + Sync {
    fn name(&self) -> &str;

    fn expectations(&self) -> Vec<Box<dyn Expectation>> {
        Vec::new()
    }

    fn init(
        &mut self,
        _descriptors: &GeneratedTopology,
        _run_metrics: &RunMetrics,
    ) -> Result<(), DynError> {
        Ok(())
    }

    async fn start(&self, ctx: &RunContext) -> Result<(), DynError>;
}




════════════════════════════════════════════════
FILE: testing-framework/core/src/topology/mod.rs
────────────────────────────────────────────────
pub mod configs {
    pub use integration_configs::topology::configs::*;
}

use std::{
    collections::{HashMap, HashSet},
    iter,
    time::Duration,
};

use configs::{
    GeneralConfig,
    consensus::{ProviderInfo, create_genesis_tx_with_declarations},
    da::{DaParams, create_da_configs},
    network::{Libp2pNetworkLayout, NetworkParams, create_network_configs},
    tracing::create_tracing_configs,
    wallet::{WalletAccount, WalletConfig},
};
use futures::future::join_all;
use groth16::fr_to_bytes;
use key_management_system::{
    backend::preload::PreloadKMSBackendSettings,
    keys::{Ed25519Key, Key, ZkKey},
};
use nomos_core::{
    mantle::GenesisTx as _,
    sdp::{Locator, ServiceType, SessionNumber},
};
use nomos_da_network_core::swarm::{BalancerStats, DAConnectionPolicySettings};
use nomos_da_network_service::MembershipResponse;
use nomos_http_api_common::paths;
use nomos_network::backends::libp2p::Libp2pInfo;
use nomos_utils::net::get_available_udp_port;
use rand::{Rng as _, thread_rng};
use reqwest::{Client, Url};
use thiserror::Error;
use tokio::time::{sleep, timeout};
use tracing::warn;

use crate::{
    adjust_timeout,
    nodes::{
        executor::{Executor, create_executor_config},
        validator::{Validator, create_validator_config},
    },
    topology::configs::{
        api::create_api_configs,
        blend::{GeneralBlendConfig, create_blend_configs},
        bootstrap::{SHORT_PROLONGED_BOOTSTRAP_PERIOD, create_bootstrap_configs},
        consensus::{ConsensusParams, create_consensus_configs},
        da::GeneralDaConfig,
        time::default_time_config,
    },
};

#[derive(Clone)]
pub struct TopologyConfig {
    pub n_validators: usize,
    pub n_executors: usize,
    pub consensus_params: ConsensusParams,
    pub da_params: DaParams,
    pub network_params: NetworkParams,
    pub wallet_config: WalletConfig,
}

impl TopologyConfig {
    #[must_use]
    pub fn two_validators() -> Self {
        Self {
            n_validators: 2,
            n_executors: 0,
            consensus_params: ConsensusParams::default_for_participants(2),
            da_params: DaParams::default(),
            network_params: NetworkParams::default(),
            wallet_config: WalletConfig::default(),
        }
    }

    #[must_use]
    pub fn validator_and_executor() -> Self {
        Self {
            n_validators: 1,
            n_executors: 1,
            consensus_params: ConsensusParams::default_for_participants(2),
            da_params: DaParams {
                dispersal_factor: 2,
                subnetwork_size: 2,
                num_subnets: 2,
                policy_settings: DAConnectionPolicySettings {
                    min_dispersal_peers: 1,
                    min_replication_peers: 1,
                    max_dispersal_failures: 0,
                    max_sampling_failures: 0,
                    max_replication_failures: 0,
                    malicious_threshold: 0,
                },
                balancer_interval: Duration::from_secs(1),
                ..Default::default()
            },
            network_params: NetworkParams::default(),
            wallet_config: WalletConfig::default(),
        }
    }

    #[must_use]
    pub fn with_node_numbers(validators: usize, executors: usize) -> Self {
        let participants = validators + executors;
        assert!(participants > 0, "topology must include at least one node");

        let mut da_params = DaParams::default();
        let da_nodes = participants;
        if da_nodes <= 1 {
            da_params.subnetwork_size = 1;
            da_params.num_subnets = 1;
            da_params.dispersal_factor = 1;
            da_params.policy_settings.min_dispersal_peers = 0;
            da_params.policy_settings.min_replication_peers = 0;
        } else {
            let dispersal = da_nodes.min(da_params.dispersal_factor.max(2));
            da_params.dispersal_factor = dispersal;
            da_params.subnetwork_size = da_params.subnetwork_size.max(dispersal);
            da_params.num_subnets = da_params.subnetwork_size as u16;
            let min_peers = dispersal.saturating_sub(1).max(1);
            da_params.policy_settings.min_dispersal_peers = min_peers;
            da_params.policy_settings.min_replication_peers = min_peers;
            da_params.balancer_interval = Duration::from_secs(1);
        }

        Self {
            n_validators: validators,
            n_executors: executors,
            consensus_params: ConsensusParams::default_for_participants(participants),
            da_params,
            network_params: NetworkParams::default(),
            wallet_config: WalletConfig::default(),
        }
    }

    #[must_use]
    pub fn validators_and_executor(
        num_validators: usize,
        num_subnets: usize,
        dispersal_factor: usize,
    ) -> Self {
        Self {
            n_validators: num_validators,
            n_executors: 1,
            consensus_params: ConsensusParams::default_for_participants(num_validators + 1),
            da_params: DaParams {
                dispersal_factor,
                subnetwork_size: num_subnets,
                num_subnets: num_subnets as u16,
                policy_settings: DAConnectionPolicySettings {
                    min_dispersal_peers: num_subnets,
                    min_replication_peers: dispersal_factor - 1,
                    max_dispersal_failures: 0,
                    max_sampling_failures: 0,
                    max_replication_failures: 0,
                    malicious_threshold: 0,
                },
                balancer_interval: Duration::from_secs(5),
                ..Default::default()
            },
            network_params: NetworkParams::default(),
            wallet_config: WalletConfig::default(),
        }
    }

    #[must_use]
    pub const fn wallet(&self) -> &WalletConfig {
        &self.wallet_config
    }
}

#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub enum NodeRole {
    Validator,
    Executor,
}

#[derive(Clone)]
pub struct GeneratedNodeConfig {
    pub role: NodeRole,
    pub index: usize,
    pub id: [u8; 32],
    pub general: GeneralConfig,
    pub da_port: u16,
    pub blend_port: u16,
}

impl GeneratedNodeConfig {
    #[must_use]
    pub const fn role(&self) -> NodeRole {
        self.role
    }

    #[must_use]
    pub const fn index(&self) -> usize {
        self.index
    }

    #[must_use]
    pub const fn network_port(&self) -> u16 {
        self.general.network_config.backend.inner.port
    }

    #[must_use]
    pub const fn api_port(&self) -> u16 {
        self.general.api_config.address.port()
    }

    #[must_use]
    pub const fn testing_http_port(&self) -> u16 {
        self.general.api_config.testing_http_address.port()
    }
}

#[derive(Clone)]
pub struct GeneratedTopology {
    config: TopologyConfig,
    validators: Vec<GeneratedNodeConfig>,
    executors: Vec<GeneratedNodeConfig>,
}

impl GeneratedTopology {
    #[must_use]
    pub const fn config(&self) -> &TopologyConfig {
        &self.config
    }

    #[must_use]
    pub fn validators(&self) -> &[GeneratedNodeConfig] {
        &self.validators
    }

    #[must_use]
    pub fn executors(&self) -> &[GeneratedNodeConfig] {
        &self.executors
    }

    pub fn nodes(&self) -> impl Iterator<Item = &GeneratedNodeConfig> {
        self.validators.iter().chain(self.executors.iter())
    }

    #[must_use]
    pub fn slot_duration(&self) -> Option<Duration> {
        self.validators
            .first()
            .map(|node| node.general.time_config.slot_duration)
    }

    #[must_use]
    pub fn wallet_accounts(&self) -> &[WalletAccount] {
        &self.config.wallet_config.accounts
    }

    pub async fn spawn_local(&self) -> Topology {
        let configs = self
            .nodes()
            .map(|node| node.general.clone())
            .collect::<Vec<_>>();

        let (validators, executors) = Topology::spawn_validators_executors(
            configs,
            self.config.n_validators,
            self.config.n_executors,
        )
        .await;

        Topology {
            validators,
            executors,
        }
    }

    pub async fn wait_remote_readiness(
        &self,
        validator_endpoints: &[Url],
        executor_endpoints: &[Url],
        validator_membership_endpoints: Option<&[Url]>,
        executor_membership_endpoints: Option<&[Url]>,
    ) -> Result<(), ReadinessError> {
        let total_nodes = self.validators.len() + self.executors.len();
        if total_nodes == 0 {
            return Ok(());
        }

        assert_eq!(
            self.validators.len(),
            validator_endpoints.len(),
            "validator endpoints must match topology"
        );
        assert_eq!(
            self.executors.len(),
            executor_endpoints.len(),
            "executor endpoints must match topology"
        );

        let mut endpoints = Vec::with_capacity(total_nodes);
        endpoints.extend_from_slice(validator_endpoints);
        endpoints.extend_from_slice(executor_endpoints);

        let labels = self.labels();
        let client = Client::new();
        let make_testing_base_url = |port: u16| -> Url {
            Url::parse(&format!("http://127.0.0.1:{port}/"))
                .expect("failed to construct local testing base url")
        };

        if endpoints.len() > 1 {
            let listen_ports = self.listen_ports();
            let initial_peer_ports = self.initial_peer_ports();
            let expected_peer_counts =
                find_expected_peer_counts(&listen_ports, &initial_peer_ports);
            let network_check = HttpNetworkReadiness {
                client: &client,
                endpoints: &endpoints,
                expected_peer_counts: &expected_peer_counts,
                labels: &labels,
            };

            network_check.wait().await?;
        }

        let mut membership_endpoints = Vec::with_capacity(total_nodes);
        if let Some(urls) = validator_membership_endpoints {
            assert_eq!(
                self.validators.len(),
                urls.len(),
                "validator membership endpoints must match topology"
            );
            membership_endpoints.extend_from_slice(urls);
        } else {
            membership_endpoints.extend(
                self.validators
                    .iter()
                    .map(|node| make_testing_base_url(node.testing_http_port())),
            );
        }

        if let Some(urls) = executor_membership_endpoints {
            assert_eq!(
                self.executors.len(),
                urls.len(),
                "executor membership endpoints must match topology"
            );
            membership_endpoints.extend_from_slice(urls);
        } else {
            membership_endpoints.extend(
                self.executors
                    .iter()
                    .map(|node| make_testing_base_url(node.testing_http_port())),
            );
        }

        let membership_check = HttpMembershipReadiness {
            client: &client,
            endpoints: &membership_endpoints,
            session: SessionNumber::from(0u64),
            labels: &labels,
            expect_non_empty: true,
        };

        membership_check.wait().await
    }

    fn listen_ports(&self) -> Vec<u16> {
        self.validators
            .iter()
            .map(|node| node.general.network_config.backend.inner.port)
            .chain(
                self.executors
                    .iter()
                    .map(|node| node.general.network_config.backend.inner.port),
            )
            .collect()
    }

    fn initial_peer_ports(&self) -> Vec<HashSet<u16>> {
        self.validators
            .iter()
            .map(|node| {
                node.general
                    .network_config
                    .backend
                    .initial_peers
                    .iter()
                    .filter_map(multiaddr_port)
                    .collect::<HashSet<u16>>()
            })
            .chain(self.executors.iter().map(|node| {
                node.general
                    .network_config
                    .backend
                    .initial_peers
                    .iter()
                    .filter_map(multiaddr_port)
                    .collect::<HashSet<u16>>()
            }))
            .collect()
    }

    fn labels(&self) -> Vec<String> {
        self.validators
            .iter()
            .enumerate()
            .map(|(idx, node)| {
                format!(
                    "validator#{idx}@{}",
                    node.general.network_config.backend.inner.port
                )
            })
            .chain(self.executors.iter().enumerate().map(|(idx, node)| {
                format!(
                    "executor#{idx}@{}",
                    node.general.network_config.backend.inner.port
                )
            }))
            .collect()
    }
}

#[derive(Clone)]
pub struct TopologyBuilder {
    config: TopologyConfig,
    ids: Option<Vec<[u8; 32]>>,
    da_ports: Option<Vec<u16>>,
    blend_ports: Option<Vec<u16>>,
}

impl TopologyBuilder {
    #[must_use]
    pub const fn new(config: TopologyConfig) -> Self {
        Self {
            config,
            ids: None,
            da_ports: None,
            blend_ports: None,
        }
    }

    #[must_use]
    pub fn with_ids(mut self, ids: Vec<[u8; 32]>) -> Self {
        self.ids = Some(ids);
        self
    }

    #[must_use]
    pub fn with_da_ports(mut self, ports: Vec<u16>) -> Self {
        self.da_ports = Some(ports);
        self
    }

    #[must_use]
    pub fn with_blend_ports(mut self, ports: Vec<u16>) -> Self {
        self.blend_ports = Some(ports);
        self
    }

    #[must_use]
    pub const fn with_validator_count(mut self, validators: usize) -> Self {
        self.config.n_validators = validators;
        self
    }

    #[must_use]
    pub const fn with_executor_count(mut self, executors: usize) -> Self {
        self.config.n_executors = executors;
        self
    }

    #[must_use]
    pub const fn with_node_counts(mut self, validators: usize, executors: usize) -> Self {
        self.config.n_validators = validators;
        self.config.n_executors = executors;
        self
    }

    #[must_use]
    pub const fn with_network_layout(mut self, layout: Libp2pNetworkLayout) -> Self {
        self.config.network_params.libp2p_network_layout = layout;
        self
    }

    #[must_use]
    pub fn with_wallet_config(mut self, wallet: WalletConfig) -> Self {
        self.config.wallet_config = wallet;
        self
    }

    #[must_use]
    pub fn build(self) -> GeneratedTopology {
        let Self {
            config,
            ids,
            da_ports,
            blend_ports,
        } = self;

        let n_participants = config.n_validators + config.n_executors;
        assert!(n_participants > 0, "topology must have at least one node");

        let ids = resolve_ids(ids, n_participants);
        let da_ports = resolve_ports(da_ports, n_participants, "DA");
        let blend_ports = resolve_ports(blend_ports, n_participants, "Blend");

        let mut consensus_configs =
            create_consensus_configs(&ids, &config.consensus_params, &config.wallet_config);
        let bootstrapping_config = create_bootstrap_configs(&ids, SHORT_PROLONGED_BOOTSTRAP_PERIOD);
        let da_configs = create_da_configs(&ids, &config.da_params, &da_ports);
        let network_configs = create_network_configs(&ids, &config.network_params);
        let blend_configs = create_blend_configs(&ids, &blend_ports);
        let api_configs = create_api_configs(&ids);
        let tracing_configs = create_tracing_configs(&ids);
        let time_config = default_time_config();

        let mut providers: Vec<_> = da_configs
            .iter()
            .enumerate()
            .map(|(i, da_conf)| ProviderInfo {
                service_type: ServiceType::DataAvailability,
                provider_sk: da_conf.signer.clone(),
                zk_sk: da_conf.secret_zk_key.clone(),
                locator: Locator(da_conf.listening_address.clone()),
                note: consensus_configs[0].da_notes[i].clone(),
            })
            .collect();
        providers.extend(
            blend_configs
                .iter()
                .enumerate()
                .map(|(i, blend_conf)| ProviderInfo {
                    service_type: ServiceType::BlendNetwork,
                    provider_sk: blend_conf.signer.clone(),
                    zk_sk: blend_conf.secret_zk_key.clone(),
                    locator: Locator(blend_conf.backend_core.listening_address.clone()),
                    note: consensus_configs[0].blend_notes[i].clone(),
                }),
        );

        let ledger_tx = consensus_configs[0]
            .genesis_tx
            .mantle_tx()
            .ledger_tx
            .clone();
        let genesis_tx = create_genesis_tx_with_declarations(ledger_tx, providers);
        for c in &mut consensus_configs {
            c.genesis_tx = genesis_tx.clone();
        }

        let kms_configs =
            create_kms_configs(&blend_configs, &da_configs, &config.wallet_config.accounts);

        let mut validators = Vec::with_capacity(config.n_validators);
        let mut executors = Vec::with_capacity(config.n_executors);

        for i in 0..n_participants {
            let general = GeneralConfig {
                consensus_config: consensus_configs[i].clone(),
                bootstrapping_config: bootstrapping_config[i].clone(),
                da_config: da_configs[i].clone(),
                network_config: network_configs[i].clone(),
                blend_config: blend_configs[i].clone(),
                api_config: api_configs[i].clone(),
                tracing_config: tracing_configs[i].clone(),
                time_config: time_config.clone(),
                kms_config: kms_configs[i].clone(),
            };

            let role = if i < config.n_validators {
                NodeRole::Validator
            } else {
                NodeRole::Executor
            };
            let index = match role {
                NodeRole::Validator => i,
                NodeRole::Executor => i - config.n_validators,
            };

            let descriptor = GeneratedNodeConfig {
                role,
                index,
                id: ids[i],
                general,
                da_port: da_ports[i],
                blend_port: blend_ports[i],
            };

            match role {
                NodeRole::Validator => validators.push(descriptor),
                NodeRole::Executor => executors.push(descriptor),
            }
        }

        GeneratedTopology {
            config,
            validators,
            executors,
        }
    }
}

pub struct Topology {
    validators: Vec<Validator>,
    executors: Vec<Executor>,
}

impl Topology {
    pub async fn spawn(config: TopologyConfig) -> Self {
        let n_participants = config.n_validators + config.n_executors;

        // we use the same random bytes for:
        // * da id
        // * coin sk
        // * coin nonce
        // * libp2p node key
        let mut ids = vec![[0; 32]; n_participants];
        let mut da_ports = vec![];
        let mut blend_ports = vec![];
        for id in &mut ids {
            thread_rng().fill(id);
            da_ports.push(get_available_udp_port().unwrap());
            blend_ports.push(get_available_udp_port().unwrap());
        }

        let mut consensus_configs =
            create_consensus_configs(&ids, &config.consensus_params, &config.wallet_config);
        let bootstrapping_config = create_bootstrap_configs(&ids, SHORT_PROLONGED_BOOTSTRAP_PERIOD);
        let da_configs = create_da_configs(&ids, &config.da_params, &da_ports);
        let network_configs = create_network_configs(&ids, &config.network_params);
        let blend_configs = create_blend_configs(&ids, &blend_ports);
        let api_configs = create_api_configs(&ids);
        let tracing_configs = create_tracing_configs(&ids);
        let time_config = default_time_config();

        // Setup genesis TX with Blend and DA service declarationse
        let mut providers: Vec<_> = da_configs
            .iter()
            .enumerate()
            .map(|(i, da_conf)| ProviderInfo {
                service_type: ServiceType::DataAvailability,
                provider_sk: da_conf.signer.clone(),
                zk_sk: da_conf.secret_zk_key.clone(),
                locator: Locator(da_conf.listening_address.clone()),
                note: consensus_configs[0].da_notes[i].clone(),
            })
            .collect();
        providers.extend(
            blend_configs
                .iter()
                .enumerate()
                .map(|(i, blend_conf)| ProviderInfo {
                    service_type: ServiceType::BlendNetwork,
                    provider_sk: blend_conf.signer.clone(),
                    zk_sk: blend_conf.secret_zk_key.clone(),
                    locator: Locator(blend_conf.backend_core.listening_address.clone()),
                    note: consensus_configs[0].blend_notes[i].clone(),
                }),
        );

        // Update genesis TX to contain Blend and DA providers.
        let ledger_tx = consensus_configs[0]
            .genesis_tx
            .mantle_tx()
            .ledger_tx
            .clone();
        let genesis_tx = create_genesis_tx_with_declarations(ledger_tx, providers);
        for c in &mut consensus_configs {
            c.genesis_tx = genesis_tx.clone();
        }

        // Set Blend and DA keys in KMS of each node config.
        let kms_configs =
            create_kms_configs(&blend_configs, &da_configs, &config.wallet_config.accounts);

        let mut node_configs = vec![];

        for i in 0..n_participants {
            node_configs.push(GeneralConfig {
                consensus_config: consensus_configs[i].clone(),
                bootstrapping_config: bootstrapping_config[i].clone(),
                da_config: da_configs[i].clone(),
                network_config: network_configs[i].clone(),
                blend_config: blend_configs[i].clone(),
                api_config: api_configs[i].clone(),
                tracing_config: tracing_configs[i].clone(),
                time_config: time_config.clone(),
                kms_config: kms_configs[i].clone(),
            });
        }

        let (validators, executors) =
            Self::spawn_validators_executors(node_configs, config.n_validators, config.n_executors)
                .await;

        Self {
            validators,
            executors,
        }
    }

    pub async fn spawn_with_empty_membership(
        config: TopologyConfig,
        ids: &[[u8; 32]],
        da_ports: &[u16],
        blend_ports: &[u16],
    ) -> Self {
        let n_participants = config.n_validators + config.n_executors;

        let consensus_configs =
            create_consensus_configs(ids, &config.consensus_params, &config.wallet_config);
        let bootstrapping_config = create_bootstrap_configs(ids, SHORT_PROLONGED_BOOTSTRAP_PERIOD);
        let da_configs = create_da_configs(ids, &config.da_params, da_ports);
        let network_configs = create_network_configs(ids, &config.network_params);
        let blend_configs = create_blend_configs(ids, blend_ports);
        let api_configs = create_api_configs(ids);
        // Create membership configs without DA nodes.
        let tracing_configs = create_tracing_configs(ids);
        let time_config = default_time_config();

        let kms_config = PreloadKMSBackendSettings {
            keys: HashMap::new(),
        };

        let mut node_configs = vec![];

        for i in 0..n_participants {
            node_configs.push(GeneralConfig {
                consensus_config: consensus_configs[i].clone(),
                bootstrapping_config: bootstrapping_config[i].clone(),
                da_config: da_configs[i].clone(),
                network_config: network_configs[i].clone(),
                blend_config: blend_configs[i].clone(),
                api_config: api_configs[i].clone(),
                tracing_config: tracing_configs[i].clone(),
                time_config: time_config.clone(),
                kms_config: kms_config.clone(),
            });
        }
        let (validators, executors) =
            Self::spawn_validators_executors(node_configs, config.n_validators, config.n_executors)
                .await;

        Self {
            validators,
            executors,
        }
    }

    async fn spawn_validators_executors(
        config: Vec<GeneralConfig>,
        n_validators: usize,
        n_executors: usize,
    ) -> (Vec<Validator>, Vec<Executor>) {
        let mut validators = Vec::new();
        for i in 0..n_validators {
            let config = create_validator_config(config[i].clone());
            validators.push(Validator::spawn(config).await.unwrap());
        }

        let mut executors = Vec::new();
        for i in n_validators..(n_validators + n_executors) {
            let config = create_executor_config(config[i].clone());
            executors.push(Executor::spawn(config).await);
        }

        (validators, executors)
    }

    #[must_use]
    pub fn validators(&self) -> &[Validator] {
        &self.validators
    }

    #[must_use]
    pub fn executors(&self) -> &[Executor] {
        &self.executors
    }

    pub async fn wait_network_ready(&self) -> Result<(), ReadinessError> {
        let listen_ports = self.node_listen_ports();
        if listen_ports.len() <= 1 {
            return Ok(());
        }

        let initial_peer_ports = self.node_initial_peer_ports();
        let expected_peer_counts = find_expected_peer_counts(&listen_ports, &initial_peer_ports);
        let labels = self.node_labels();

        let check = NetworkReadiness {
            topology: self,
            expected_peer_counts: &expected_peer_counts,
            labels: &labels,
        };

        check.wait().await?;
        Ok(())
    }

    pub async fn wait_da_balancer_ready(&self) -> Result<(), ReadinessError> {
        if self.validators.is_empty() && self.executors.is_empty() {
            return Ok(());
        }

        let labels = self.node_labels();
        let check = DaBalancerReadiness {
            topology: self,
            labels: &labels,
        };

        check.wait().await?;
        Ok(())
    }

    pub async fn wait_membership_ready(&self) -> Result<(), ReadinessError> {
        self.wait_membership_ready_for_session(SessionNumber::from(0u64))
            .await
    }

    pub async fn wait_membership_ready_for_session(
        &self,
        session: SessionNumber,
    ) -> Result<(), ReadinessError> {
        self.wait_membership_assignations(session, true).await
    }

    pub async fn wait_membership_empty_for_session(
        &self,
        session: SessionNumber,
    ) -> Result<(), ReadinessError> {
        self.wait_membership_assignations(session, false).await
    }

    async fn wait_membership_assignations(
        &self,
        session: SessionNumber,
        expect_non_empty: bool,
    ) -> Result<(), ReadinessError> {
        let total_nodes = self.validators.len() + self.executors.len();

        if total_nodes == 0 {
            return Ok(());
        }

        let labels = self.node_labels();
        let check = MembershipReadiness {
            topology: self,
            session,
            labels: &labels,
            expect_non_empty,
        };

        check.wait().await?;
        Ok(())
    }

    fn node_listen_ports(&self) -> Vec<u16> {
        self.validators
            .iter()
            .map(|node| node.config().network.backend.inner.port)
            .chain(
                self.executors
                    .iter()
                    .map(|node| node.config().network.backend.inner.port),
            )
            .collect()
    }

    fn node_initial_peer_ports(&self) -> Vec<HashSet<u16>> {
        self.validators
            .iter()
            .map(|node| {
                node.config()
                    .network
                    .backend
                    .initial_peers
                    .iter()
                    .filter_map(multiaddr_port)
                    .collect::<HashSet<u16>>()
            })
            .chain(self.executors.iter().map(|node| {
                node.config()
                    .network
                    .backend
                    .initial_peers
                    .iter()
                    .filter_map(multiaddr_port)
                    .collect::<HashSet<u16>>()
            }))
            .collect()
    }

    fn node_labels(&self) -> Vec<String> {
        self.validators
            .iter()
            .enumerate()
            .map(|(idx, node)| {
                format!(
                    "validator#{idx}@{}",
                    node.config().network.backend.inner.port
                )
            })
            .chain(self.executors.iter().enumerate().map(|(idx, node)| {
                format!(
                    "executor#{idx}@{}",
                    node.config().network.backend.inner.port
                )
            }))
            .collect()
    }
}

#[derive(Debug, Error)]
pub enum ReadinessError {
    #[error("{message}")]
    Timeout { message: String },
}

#[async_trait::async_trait]
trait ReadinessCheck<'a> {
    type Data: Send;

    async fn collect(&'a self) -> Self::Data;

    fn is_ready(&self, data: &Self::Data) -> bool;

    fn timeout_message(&self, data: Self::Data) -> String;

    fn poll_interval(&self) -> Duration {
        Duration::from_millis(200)
    }

    async fn wait(&'a self) -> Result<(), ReadinessError> {
        let timeout_duration = adjust_timeout(Duration::from_secs(60));
        let poll_interval = self.poll_interval();
        let mut data = self.collect().await;

        let wait_result = timeout(timeout_duration, async {
            loop {
                if self.is_ready(&data) {
                    return;
                }

                sleep(poll_interval).await;

                data = self.collect().await;
            }
        })
        .await;

        if wait_result.is_err() {
            let message = self.timeout_message(data);
            return Err(ReadinessError::Timeout { message });
        }

        Ok(())
    }
}

struct NetworkReadiness<'a> {
    topology: &'a Topology,
    expected_peer_counts: &'a [usize],
    labels: &'a [String],
}

#[async_trait::async_trait]
impl<'a> ReadinessCheck<'a> for NetworkReadiness<'a> {
    type Data = Vec<Libp2pInfo>;

    async fn collect(&'a self) -> Self::Data {
        let (validator_infos, executor_infos) = tokio::join!(
            join_all(self.topology.validators.iter().map(Validator::network_info)),
            join_all(self.topology.executors.iter().map(Executor::network_info))
        );

        validator_infos.into_iter().chain(executor_infos).collect()
    }

    fn is_ready(&self, data: &Self::Data) -> bool {
        data.iter()
            .enumerate()
            .all(|(idx, info)| info.n_peers >= self.expected_peer_counts[idx])
    }

    fn timeout_message(&self, data: Self::Data) -> String {
        let summary = build_timeout_summary(self.labels, data, self.expected_peer_counts);
        format!("timed out waiting for network readiness: {summary}")
    }
}

struct HttpNetworkReadiness<'a> {
    client: &'a Client,
    endpoints: &'a [Url],
    expected_peer_counts: &'a [usize],
    labels: &'a [String],
}

#[async_trait::async_trait]
impl<'a> ReadinessCheck<'a> for HttpNetworkReadiness<'a> {
    type Data = Vec<Libp2pInfo>;

    async fn collect(&'a self) -> Self::Data {
        let futures = self
            .endpoints
            .iter()
            .map(|endpoint| fetch_network_info(self.client, endpoint));
        join_all(futures).await
    }

    fn is_ready(&self, data: &Self::Data) -> bool {
        data.iter()
            .enumerate()
            .all(|(idx, info)| info.n_peers >= self.expected_peer_counts[idx])
    }

    fn timeout_message(&self, data: Self::Data) -> String {
        let summary = build_timeout_summary(self.labels, data, self.expected_peer_counts);
        format!("timed out waiting for network readiness: {summary}")
    }
}

struct MembershipReadiness<'a> {
    topology: &'a Topology,
    session: SessionNumber,
    labels: &'a [String],
    expect_non_empty: bool,
}

#[async_trait::async_trait]
impl<'a> ReadinessCheck<'a> for MembershipReadiness<'a> {
    type Data = Vec<Result<MembershipResponse, reqwest::Error>>;

    async fn collect(&'a self) -> Self::Data {
        let (validator_responses, executor_responses) = tokio::join!(
            join_all(
                self.topology
                    .validators
                    .iter()
                    .map(|node| node.da_get_membership(self.session)),
            ),
            join_all(
                self.topology
                    .executors
                    .iter()
                    .map(|node| node.da_get_membership(self.session)),
            )
        );

        validator_responses
            .into_iter()
            .chain(executor_responses)
            .collect()
    }

    fn is_ready(&self, data: &Self::Data) -> bool {
        self.assignation_statuses(data)
            .into_iter()
            .all(|ready| ready)
    }

    fn timeout_message(&self, data: Self::Data) -> String {
        let statuses = self.assignation_statuses(&data);
        let description = if self.expect_non_empty {
            "non-empty assignations"
        } else {
            "empty assignations"
        };
        let summary = build_membership_summary(self.labels, &statuses, description);
        format!("timed out waiting for DA membership readiness ({description}): {summary}")
    }
}

impl MembershipReadiness<'_> {
    fn assignation_statuses(
        &self,
        responses: &[Result<MembershipResponse, reqwest::Error>],
    ) -> Vec<bool> {
        responses
            .iter()
            .map(|res| {
                res.as_ref()
                    .map(|resp| {
                        let is_non_empty = !resp.assignations.is_empty();
                        if self.expect_non_empty {
                            is_non_empty
                        } else {
                            !is_non_empty
                        }
                    })
                    .unwrap_or(false)
            })
            .collect()
    }
}

struct HttpMembershipReadiness<'a> {
    client: &'a Client,
    endpoints: &'a [Url],
    session: SessionNumber,
    labels: &'a [String],
    expect_non_empty: bool,
}

#[async_trait::async_trait]
impl<'a> ReadinessCheck<'a> for HttpMembershipReadiness<'a> {
    type Data = Vec<Result<MembershipResponse, reqwest::Error>>;

    async fn collect(&'a self) -> Self::Data {
        let futures = self
            .endpoints
            .iter()
            .map(|endpoint| fetch_membership(self.client, endpoint, self.session));
        join_all(futures).await
    }

    fn is_ready(&self, data: &Self::Data) -> bool {
        assignation_statuses(data, self.expect_non_empty)
            .into_iter()
            .all(|ready| ready)
    }

    fn timeout_message(&self, data: Self::Data) -> String {
        let statuses = assignation_statuses(&data, self.expect_non_empty);
        let description = if self.expect_non_empty {
            "non-empty assignations"
        } else {
            "empty assignations"
        };
        let summary = build_membership_summary(self.labels, &statuses, description);
        format!("timed out waiting for DA membership readiness ({description}): {summary}")
    }
}

struct DaBalancerReadiness<'a> {
    topology: &'a Topology,
    labels: &'a [String],
}

#[async_trait::async_trait]
impl<'a> ReadinessCheck<'a> for DaBalancerReadiness<'a> {
    type Data = Vec<(String, usize, BalancerStats)>;

    async fn collect(&'a self) -> Self::Data {
        let mut data = Vec::new();
        for (idx, validator) in self.topology.validators.iter().enumerate() {
            data.push((
                self.labels[idx].clone(),
                validator.config().da_network.subnet_threshold,
                validator.balancer_stats().await,
            ));
        }
        for (offset, executor) in self.topology.executors.iter().enumerate() {
            let label_index = self.topology.validators.len() + offset;
            data.push((
                self.labels[label_index].clone(),
                executor.config().da_network.subnet_threshold,
                executor.balancer_stats().await,
            ));
        }
        data
    }

    fn is_ready(&self, data: &Self::Data) -> bool {
        data.iter().all(|(_, threshold, stats)| {
            if *threshold == 0 {
                return true;
            }
            connected_subnetworks(stats) >= *threshold
        })
    }

    fn timeout_message(&self, data: Self::Data) -> String {
        let summary = data
            .into_iter()
            .map(|(label, threshold, stats)| {
                let connected = connected_subnetworks(&stats);
                format!("{label}: connected={connected}, required={threshold}")
            })
            .collect::<Vec<_>>()
            .join(", ");
        format!("timed out waiting for DA balancer readiness: {summary}")
    }

    fn poll_interval(&self) -> Duration {
        Duration::from_secs(1)
    }
}

fn connected_subnetworks(stats: &BalancerStats) -> usize {
    stats
        .values()
        .filter(|stat| stat.inbound > 0 || stat.outbound > 0)
        .count()
}

fn build_timeout_summary(
    labels: &[String],
    infos: Vec<Libp2pInfo>,
    expected_counts: &[usize],
) -> String {
    infos
        .into_iter()
        .zip(expected_counts.iter())
        .zip(labels.iter())
        .map(|((info, expected), label)| {
            format!("{}: peers={}, expected={}", label, info.n_peers, expected)
        })
        .collect::<Vec<_>>()
        .join(", ")
}

fn build_membership_summary(labels: &[String], statuses: &[bool], description: &str) -> String {
    statuses
        .iter()
        .zip(labels.iter())
        .map(|(ready, label)| {
            let status = if *ready { "ready" } else { "waiting" };
            format!("{label}: status={status}, expected {description}")
        })
        .collect::<Vec<_>>()
        .join(", ")
}

async fn fetch_network_info(client: &Client, base: &Url) -> Libp2pInfo {
    let url = join_path(base, paths::NETWORK_INFO);
    let response = match client.get(url).send().await {
        Ok(resp) => resp,
        Err(err) => {
            return log_network_warning(base, err, "failed to reach network info endpoint");
        }
    };

    let response = match response.error_for_status() {
        Ok(resp) => resp,
        Err(err) => {
            return log_network_warning(base, err, "network info endpoint returned error");
        }
    };

    match response.json::<Libp2pInfo>().await {
        Ok(info) => info,
        Err(err) => log_network_warning(base, err, "failed to decode network info response"),
    }
}

async fn fetch_membership(
    client: &Client,
    base: &Url,
    session: SessionNumber,
) -> Result<MembershipResponse, reqwest::Error> {
    let url = join_path(base, paths::DA_GET_MEMBERSHIP);
    client
        .post(url)
        .json(&session)
        .send()
        .await?
        .error_for_status()?
        .json()
        .await
}

fn log_network_warning(base: &Url, err: impl std::fmt::Display, message: &str) -> Libp2pInfo {
    warn!(target: "readiness", url = %base, error = %err, "{message}");
    empty_libp2p_info()
}

fn empty_libp2p_info() -> Libp2pInfo {
    Libp2pInfo {
        listen_addresses: Vec::with_capacity(0),
        n_peers: 0,
        n_connections: 0,
        n_pending_connections: 0,
    }
}

fn join_path(base: &Url, path: &str) -> Url {
    base.join(path.trim_start_matches('/'))
        .unwrap_or_else(|err| panic!("failed to join url {base} with path {path}: {err}"))
}

fn assignation_statuses(
    responses: &[Result<MembershipResponse, reqwest::Error>],
    expect_non_empty: bool,
) -> Vec<bool> {
    responses
        .iter()
        .map(|res| {
            res.as_ref()
                .map(|resp| {
                    let is_non_empty = !resp.assignations.is_empty();
                    if expect_non_empty {
                        is_non_empty
                    } else {
                        !is_non_empty
                    }
                })
                .unwrap_or(false)
        })
        .collect()
}

fn multiaddr_port(addr: &nomos_libp2p::Multiaddr) -> Option<u16> {
    for protocol in addr {
        match protocol {
            nomos_libp2p::Protocol::Udp(port) | nomos_libp2p::Protocol::Tcp(port) => {
                return Some(port);
            }
            _ => {}
        }
    }
    None
}

fn find_expected_peer_counts(
    listen_ports: &[u16],
    initial_peer_ports: &[HashSet<u16>],
) -> Vec<usize> {
    let mut expected: Vec<HashSet<usize>> = vec![HashSet::new(); initial_peer_ports.len()];

    for (idx, ports) in initial_peer_ports.iter().enumerate() {
        for port in ports {
            let Some(peer_idx) = listen_ports.iter().position(|p| p == port) else {
                continue;
            };
            if peer_idx == idx {
                continue;
            }

            expected[idx].insert(peer_idx);
            expected[peer_idx].insert(idx);
        }
    }

    expected.into_iter().map(|set| set.len()).collect()
}

#[must_use]
pub fn create_kms_configs(
    blend_configs: &[GeneralBlendConfig],
    da_configs: &[GeneralDaConfig],
    _wallet_accounts: &[WalletAccount],
) -> Vec<PreloadKMSBackendSettings> {
    da_configs
        .iter()
        .zip(blend_configs.iter())
        .map(|(da_conf, blend_conf)| PreloadKMSBackendSettings {
            keys: [
                (
                    hex::encode(blend_conf.signer.verifying_key().as_bytes()),
                    Key::Ed25519(Ed25519Key::new(blend_conf.signer.clone())),
                ),
                (
                    hex::encode(fr_to_bytes(
                        &blend_conf.secret_zk_key.to_public_key().into_inner(),
                    )),
                    Key::Zk(ZkKey::new(blend_conf.secret_zk_key.clone())),
                ),
                (
                    hex::encode(da_conf.signer.verifying_key().as_bytes()),
                    Key::Ed25519(Ed25519Key::new(da_conf.signer.clone())),
                ),
                (
                    hex::encode(fr_to_bytes(
                        &da_conf.secret_zk_key.to_public_key().into_inner(),
                    )),
                    Key::Zk(ZkKey::new(da_conf.secret_zk_key.clone())),
                ),
            ]
            .into(),
        })
        .collect()
}

fn resolve_ids(ids: Option<Vec<[u8; 32]>>, count: usize) -> Vec<[u8; 32]> {
    ids.map_or_else(
        || {
            let mut generated = vec![[0; 32]; count];
            for id in &mut generated {
                thread_rng().fill(id);
            }
            generated
        },
        |ids| {
            assert_eq!(
                ids.len(),
                count,
                "expected {count} ids but got {}",
                ids.len()
            );
            ids
        },
    )
}

fn resolve_ports(ports: Option<Vec<u16>>, count: usize, label: &str) -> Vec<u16> {
    let resolved = ports.unwrap_or_else(|| {
        iter::repeat_with(|| get_available_udp_port().unwrap())
            .take(count)
            .collect()
    });
    assert_eq!(
        resolved.len(),
        count,
        "expected {count} {label} ports but got {}",
        resolved.len()
    );
    resolved
}




════════════════════════════════════════════════
FILE: testing-framework/runners/compose/src/cfgsync.rs
────────────────────────────────────────────────
use std::{net::Ipv4Addr, path::Path, sync::Arc};

use anyhow::Context as _;
use axum::serve;
use cfgsync::{
    repo::ConfigRepo,
    server::{CfgSyncConfig as ServerCfgSyncConfig, cfgsync_app},
};
use testing_framework_core::{
    scenario::cfgsync::{apply_topology_overrides, load_cfgsync_template, write_cfgsync_template},
    topology::GeneratedTopology,
};
use tokio::{net::TcpListener, sync::oneshot, task::JoinHandle};

#[derive(Debug)]
pub struct CfgsyncServerHandle {
    shutdown: Option<oneshot::Sender<()>>,
    pub join: JoinHandle<()>,
}

impl CfgsyncServerHandle {
    pub fn shutdown(&mut self) {
        if let Some(tx) = self.shutdown.take() {
            let _ = tx.send(());
        }
        self.join.abort();
    }
}

pub fn update_cfgsync_config(
    path: &Path,
    topology: &GeneratedTopology,
    use_kzg_mount: bool,
) -> anyhow::Result<()> {
    let mut cfg = load_cfgsync_template(path)?;
    apply_topology_overrides(&mut cfg, topology, use_kzg_mount);
    write_cfgsync_template(path, &cfg)?;
    Ok(())
}

pub async fn start_cfgsync_server(
    cfgsync_path: &Path,
    port: u16,
) -> anyhow::Result<CfgsyncServerHandle> {
    let cfg_path = cfgsync_path.to_path_buf();
    let config = ServerCfgSyncConfig::load_from_file(&cfg_path)
        .map_err(|err| anyhow::anyhow!("loading cfgsync config: {err}"))?;
    let repo: Arc<ConfigRepo> = config.into();

    let listener = TcpListener::bind((Ipv4Addr::UNSPECIFIED, port))
        .await
        .context("binding cfgsync listener")?;

    let cfgsync_router = cfgsync_app(repo);
    let (shutdown_tx, shutdown_rx) = oneshot::channel();
    let (ready_tx, ready_rx) = oneshot::channel();

    let join = tokio::spawn(async move {
        let server =
            serve(listener, cfgsync_router.into_make_service()).with_graceful_shutdown(async {
                let _ = shutdown_rx.await;
            });
        let _ = ready_tx.send(());
        if let Err(err) = server.await {
            eprintln!("[compose-runner] cfgsync server error: {err}");
        }
    });

    ready_rx
        .await
        .context("waiting for cfgsync server to become ready")?;

    Ok(CfgsyncServerHandle {
        shutdown: Some(shutdown_tx),
        join,
    })
}




════════════════════════════════════════════════
FILE: testing-framework/runners/compose/src/cleanup.rs
────────────────────────────────────────────────
use std::{env, path::PathBuf};

use testing_framework_core::scenario::CleanupGuard;

use crate::{cfgsync::CfgsyncServerHandle, compose::compose_down, workspace::ComposeWorkspace};

pub struct RunnerCleanup {
    pub compose_file: PathBuf,
    pub project_name: String,
    pub root: PathBuf,
    workspace: Option<ComposeWorkspace>,
    cfgsync: Option<CfgsyncServerHandle>,
}

impl RunnerCleanup {
    pub fn new(
        compose_file: PathBuf,
        project_name: String,
        root: PathBuf,
        workspace: ComposeWorkspace,
        cfgsync: Option<CfgsyncServerHandle>,
    ) -> Self {
        debug_assert!(
            !compose_file.as_os_str().is_empty() && !project_name.is_empty(),
            "compose cleanup should receive valid identifiers"
        );
        Self {
            compose_file,
            project_name,
            root,
            workspace: Some(workspace),
            cfgsync,
        }
    }

    fn teardown_compose(&self) {
        if let Err(err) = compose_down(&self.compose_file, &self.project_name, &self.root) {
            eprintln!("[compose-runner] docker compose down failed: {err}");
        }
    }
}

impl CleanupGuard for RunnerCleanup {
    fn cleanup(mut self: Box<Self>) {
        let preserve = env::var("COMPOSE_RUNNER_PRESERVE").is_ok()
            || env::var("TESTNET_RUNNER_PRESERVE").is_ok();
        if preserve {
            if let Some(workspace) = self.workspace.take() {
                let keep = workspace.into_inner().keep();
                eprintln!(
                    "[compose-runner] preserving docker state at {}",
                    keep.display()
                );
            }

            eprintln!("[compose-runner] compose preserve flag set; skipping docker compose down");
            return;
        }

        self.teardown_compose();

        if let Some(mut handle) = self.cfgsync.take() {
            handle.shutdown();
        }
    }
}




════════════════════════════════════════════════
FILE: testing-framework/runners/compose/src/compose.rs
────────────────────────────────────────────────
use std::{
    env, fs, io,
    path::{Path, PathBuf},
    process,
    time::Duration,
};

use anyhow::Context as _;
use serde::Serialize;
use tera::Context as TeraContext;
use testing_framework_core::{
    adjust_timeout,
    topology::{GeneratedNodeConfig, GeneratedTopology},
};
use tokio::{process::Command, time::timeout};

const COMPOSE_UP_TIMEOUT: Duration = Duration::from_secs(120);
const TEMPLATE_RELATIVE_PATH: &str =
    "testing-framework/runners/compose/assets/docker-compose.yml.tera";

#[derive(Debug, thiserror::Error)]
pub enum ComposeCommandError {
    #[error("{command} exited with status {status}")]
    Failed {
        command: String,
        status: process::ExitStatus,
    },
    #[error("failed to spawn {command}: {source}")]
    Spawn {
        command: String,
        #[source]
        source: io::Error,
    },
    #[error("{command} timed out after {timeout:?}")]
    Timeout { command: String, timeout: Duration },
}

pub async fn compose_up(
    compose_path: &Path,
    project_name: &str,
    root: &Path,
) -> Result<(), ComposeCommandError> {
    let mut cmd = Command::new("docker");
    cmd.arg("compose")
        .arg("-f")
        .arg(compose_path)
        .arg("-p")
        .arg(project_name)
        .arg("up")
        .arg("-d")
        .current_dir(root);

    run_compose_command(cmd, adjust_timeout(COMPOSE_UP_TIMEOUT), "docker compose up").await
}

pub fn compose_down(
    compose_path: &Path,
    project_name: &str,
    root: &Path,
) -> Result<(), ComposeCommandError> {
    let description = "docker compose down".to_owned();
    let status = process::Command::new("docker")
        .arg("compose")
        .arg("-f")
        .arg(compose_path)
        .arg("-p")
        .arg(project_name)
        .arg("down")
        .arg("--volumes")
        .current_dir(root)
        .status()
        .map_err(|source| ComposeCommandError::Spawn {
            command: description.clone(),
            source,
        })?;

    if status.success() {
        Ok(())
    } else {
        Err(ComposeCommandError::Failed {
            command: description,
            status,
        })
    }
}

#[derive(Debug, thiserror::Error)]
pub enum TemplateError {
    #[error("failed to resolve repository root for compose template: {source}")]
    RepositoryRoot {
        #[source]
        source: anyhow::Error,
    },
    #[error("failed to read compose template at {path}: {source}")]
    Read {
        path: PathBuf,
        #[source]
        source: io::Error,
    },
    #[error("failed to serialise compose descriptor for templating: {source}")]
    Serialize {
        #[source]
        source: tera::Error,
    },
    #[error("failed to render compose template at {path}: {source}")]
    Render {
        path: PathBuf,
        #[source]
        source: tera::Error,
    },
    #[error("failed to write compose file at {path}: {source}")]
    Write {
        path: PathBuf,
        #[source]
        source: io::Error,
    },
}

#[derive(Debug, thiserror::Error)]
pub enum DescriptorBuildError {
    #[error("cfgsync port is not configured for compose descriptor")]
    MissingCfgsyncPort,
    #[error("prometheus port is not configured for compose descriptor")]
    MissingPrometheusPort,
}

#[derive(Clone, Debug, Serialize)]
pub struct ComposeDescriptor {
    prometheus: PrometheusTemplate,
    validators: Vec<NodeDescriptor>,
    executors: Vec<NodeDescriptor>,
}

impl ComposeDescriptor {
    #[must_use]
    pub const fn builder(topology: &GeneratedTopology) -> ComposeDescriptorBuilder<'_> {
        ComposeDescriptorBuilder::new(topology)
    }

    #[cfg(test)]
    fn validators(&self) -> &[NodeDescriptor] {
        &self.validators
    }

    #[cfg(test)]
    fn executors(&self) -> &[NodeDescriptor] {
        &self.executors
    }
}

pub struct ComposeDescriptorBuilder<'a> {
    topology: &'a GeneratedTopology,
    use_kzg_mount: bool,
    cfgsync_port: Option<u16>,
    prometheus_port: Option<u16>,
}

impl<'a> ComposeDescriptorBuilder<'a> {
    const fn new(topology: &'a GeneratedTopology) -> Self {
        Self {
            topology,
            use_kzg_mount: false,
            cfgsync_port: None,
            prometheus_port: None,
        }
    }

    #[must_use]
    pub const fn with_kzg_mount(mut self, enabled: bool) -> Self {
        self.use_kzg_mount = enabled;
        self
    }

    #[must_use]
    pub const fn with_cfgsync_port(mut self, port: u16) -> Self {
        self.cfgsync_port = Some(port);
        self
    }

    #[must_use]
    pub const fn with_prometheus_port(mut self, port: u16) -> Self {
        self.prometheus_port = Some(port);
        self
    }

    pub fn build(self) -> Result<ComposeDescriptor, DescriptorBuildError> {
        let cfgsync_port = self
            .cfgsync_port
            .ok_or(DescriptorBuildError::MissingCfgsyncPort)?;
        let prometheus_host_port = self
            .prometheus_port
            .ok_or(DescriptorBuildError::MissingPrometheusPort)?;

        let (default_image, default_platform) = resolve_image();
        let image = default_image;
        let platform = default_platform;

        let validators = build_nodes(
            self.topology.validators(),
            ComposeNodeKind::Validator,
            &image,
            platform.as_deref(),
            self.use_kzg_mount,
            cfgsync_port,
        );

        let executors = build_nodes(
            self.topology.executors(),
            ComposeNodeKind::Executor,
            &image,
            platform.as_deref(),
            self.use_kzg_mount,
            cfgsync_port,
        );

        Ok(ComposeDescriptor {
            prometheus: PrometheusTemplate::new(prometheus_host_port),
            validators,
            executors,
        })
    }
}

#[derive(Clone, Debug, Serialize)]
pub struct PrometheusTemplate {
    host_port: String,
}

impl PrometheusTemplate {
    fn new(port: u16) -> Self {
        Self {
            host_port: format!("127.0.0.1:{port}:9090"),
        }
    }
}

#[derive(Clone, Debug, Serialize, PartialEq, Eq)]
pub struct EnvEntry {
    key: String,
    value: String,
}

impl EnvEntry {
    fn new(key: impl Into<String>, value: impl Into<String>) -> Self {
        Self {
            key: key.into(),
            value: value.into(),
        }
    }

    #[cfg(test)]
    fn key(&self) -> &str {
        &self.key
    }

    #[cfg(test)]
    fn value(&self) -> &str {
        &self.value
    }
}

#[derive(Clone, Debug, Serialize)]
pub struct NodeDescriptor {
    name: String,
    image: String,
    entrypoint: String,
    volumes: Vec<String>,
    extra_hosts: Vec<String>,
    ports: Vec<String>,
    environment: Vec<EnvEntry>,
    #[serde(skip_serializing_if = "Option::is_none")]
    platform: Option<String>,
}

#[derive(Clone, Debug)]
pub struct NodeHostPorts {
    pub api: u16,
    pub testing: u16,
}

#[derive(Clone, Debug)]
pub struct HostPortMapping {
    pub validators: Vec<NodeHostPorts>,
    pub executors: Vec<NodeHostPorts>,
}

impl HostPortMapping {
    pub fn validator_api_ports(&self) -> Vec<u16> {
        self.validators.iter().map(|ports| ports.api).collect()
    }

    pub fn executor_api_ports(&self) -> Vec<u16> {
        self.executors.iter().map(|ports| ports.api).collect()
    }
}

impl NodeDescriptor {
    fn from_node(
        kind: ComposeNodeKind,
        index: usize,
        node: &GeneratedNodeConfig,
        image: &str,
        platform: Option<&str>,
        use_kzg_mount: bool,
        cfgsync_port: u16,
    ) -> Self {
        let mut environment = base_environment(cfgsync_port);
        let identifier = kind.instance_name(index);
        environment.extend([
            EnvEntry::new(
                "CFG_NETWORK_PORT",
                node.general.network_config.backend.inner.port.to_string(),
            ),
            EnvEntry::new("CFG_DA_PORT", node.da_port.to_string()),
            EnvEntry::new("CFG_BLEND_PORT", node.blend_port.to_string()),
            EnvEntry::new(
                "CFG_API_PORT",
                node.general.api_config.address.port().to_string(),
            ),
            EnvEntry::new(
                "CFG_TESTING_HTTP_PORT",
                node.general
                    .api_config
                    .testing_http_address
                    .port()
                    .to_string(),
            ),
            EnvEntry::new("CFG_HOST_IDENTIFIER", identifier),
        ]);

        let ports = vec![
            node.general.api_config.address.port().to_string(),
            node.general
                .api_config
                .testing_http_address
                .port()
                .to_string(),
        ];

        Self {
            name: kind.instance_name(index),
            image: image.to_owned(),
            entrypoint: kind.entrypoint().to_owned(),
            volumes: base_volumes(use_kzg_mount),
            extra_hosts: default_extra_hosts(),
            ports,
            environment,
            platform: platform.map(ToOwned::to_owned),
        }
    }

    #[cfg(test)]
    fn ports(&self) -> &[String] {
        &self.ports
    }

    #[cfg(test)]
    fn environment(&self) -> &[EnvEntry] {
        &self.environment
    }
}

pub fn write_compose_file(
    descriptor: &ComposeDescriptor,
    compose_path: &Path,
) -> Result<(), TemplateError> {
    TemplateSource::load()?.write(descriptor, compose_path)
}

pub async fn dump_compose_logs(compose_file: &Path, project: &str, root: &Path) {
    let mut cmd = Command::new("docker");
    cmd.arg("compose")
        .arg("-f")
        .arg(compose_file)
        .arg("-p")
        .arg(project)
        .arg("logs")
        .arg("--no-color")
        .current_dir(root);

    match cmd.output().await {
        Ok(output) => {
            if !output.stdout.is_empty() {
                eprintln!(
                    "[compose-runner] docker compose logs:\n{}",
                    String::from_utf8_lossy(&output.stdout)
                );
            }
            if !output.stderr.is_empty() {
                eprintln!(
                    "[compose-runner] docker compose errors:\n{}",
                    String::from_utf8_lossy(&output.stderr)
                );
            }
        }
        Err(err) => {
            eprintln!("[compose-runner] failed to collect docker compose logs: {err}");
        }
    }
}

struct TemplateSource {
    path: PathBuf,
    contents: String,
}

impl TemplateSource {
    fn load() -> Result<Self, TemplateError> {
        let repo_root =
            repository_root().map_err(|source| TemplateError::RepositoryRoot { source })?;
        let path = repo_root.join(TEMPLATE_RELATIVE_PATH);
        let contents = fs::read_to_string(&path).map_err(|source| TemplateError::Read {
            path: path.clone(),
            source,
        })?;

        Ok(Self { path, contents })
    }

    fn render(&self, descriptor: &ComposeDescriptor) -> Result<String, TemplateError> {
        let context = TeraContext::from_serialize(descriptor)
            .map_err(|source| TemplateError::Serialize { source })?;

        tera::Tera::one_off(&self.contents, &context, false).map_err(|source| {
            TemplateError::Render {
                path: self.path.clone(),
                source,
            }
        })
    }

    fn write(&self, descriptor: &ComposeDescriptor, output: &Path) -> Result<(), TemplateError> {
        let rendered = self.render(descriptor)?;
        fs::write(output, rendered).map_err(|source| TemplateError::Write {
            path: output.to_path_buf(),
            source,
        })
    }
}

pub fn repository_root() -> anyhow::Result<PathBuf> {
    env::var("CARGO_WORKSPACE_DIR")
        .map(PathBuf::from)
        .or_else(|_| {
            Path::new(env!("CARGO_MANIFEST_DIR"))
                .parent()
                .and_then(Path::parent)
                .and_then(Path::parent)
                .map(PathBuf::from)
                .context("resolving repository root from manifest dir")
        })
}

#[derive(Clone, Copy)]
enum ComposeNodeKind {
    Validator,
    Executor,
}

impl ComposeNodeKind {
    fn instance_name(self, index: usize) -> String {
        match self {
            Self::Validator => format!("validator-{index}"),
            Self::Executor => format!("executor-{index}"),
        }
    }

    const fn entrypoint(self) -> &'static str {
        match self {
            Self::Validator => "/etc/nomos/scripts/run_nomos_node.sh",
            Self::Executor => "/etc/nomos/scripts/run_nomos_executor.sh",
        }
    }
}

fn build_nodes(
    nodes: &[GeneratedNodeConfig],
    kind: ComposeNodeKind,
    image: &str,
    platform: Option<&str>,
    use_kzg_mount: bool,
    cfgsync_port: u16,
) -> Vec<NodeDescriptor> {
    nodes
        .iter()
        .enumerate()
        .map(|(index, node)| {
            NodeDescriptor::from_node(
                kind,
                index,
                node,
                image,
                platform,
                use_kzg_mount,
                cfgsync_port,
            )
        })
        .collect()
}

fn base_environment(cfgsync_port: u16) -> Vec<EnvEntry> {
    vec![
        EnvEntry::new("POL_PROOF_DEV_MODE", "true"),
        EnvEntry::new(
            "CFG_SERVER_ADDR",
            format!("http://host.docker.internal:{cfgsync_port}"),
        ),
        EnvEntry::new("OTEL_METRIC_EXPORT_INTERVAL", "5000"),
    ]
}

fn base_volumes(use_kzg_mount: bool) -> Vec<String> {
    let mut volumes = vec!["./testnet:/etc/nomos".into()];
    if use_kzg_mount {
        volumes.push("./kzgrs_test_params:/kzgrs_test_params:z".into());
    }
    volumes
}

fn default_extra_hosts() -> Vec<String> {
    host_gateway_entry().into_iter().collect()
}

pub fn resolve_image() -> (String, Option<String>) {
    let image =
        env::var("NOMOS_TESTNET_IMAGE").unwrap_or_else(|_| String::from("nomos-testnet:local"));
    let platform = (image == "ghcr.io/logos-co/nomos:testnet").then(|| "linux/amd64".to_owned());
    (image, platform)
}

fn host_gateway_entry() -> Option<String> {
    if let Ok(value) = env::var("COMPOSE_RUNNER_HOST_GATEWAY") {
        if value.eq_ignore_ascii_case("disable") || value.is_empty() {
            return None;
        }
        return Some(value);
    }

    if cfg!(any(target_os = "macos", target_os = "windows")) {
        return Some("host.docker.internal:host-gateway".into());
    }

    env::var("DOCKER_HOST_GATEWAY")
        .ok()
        .filter(|value| !value.is_empty())
        .map(|gateway| format!("host.docker.internal:{gateway}"))
}

async fn run_compose_command(
    mut command: Command,
    timeout_duration: Duration,
    description: &str,
) -> Result<(), ComposeCommandError> {
    match timeout(timeout_duration, command.status()).await {
        Ok(Ok(status)) if status.success() => Ok(()),
        Ok(Ok(status)) => Err(ComposeCommandError::Failed {
            command: description.to_owned(),
            status,
        }),
        Ok(Err(err)) => Err(ComposeCommandError::Spawn {
            command: description.to_owned(),
            source: err,
        }),
        Err(_) => Err(ComposeCommandError::Timeout {
            command: description.to_owned(),
            timeout: timeout_duration,
        }),
    }
}

#[cfg(test)]
mod tests {
    use testing_framework_core::topology::{TopologyBuilder, TopologyConfig};

    use super::*;

    #[test]
    fn descriptor_matches_topology_counts() {
        let topology = TopologyBuilder::new(TopologyConfig::with_node_numbers(2, 1)).build();
        let descriptor = ComposeDescriptor::builder(&topology)
            .with_cfgsync_port(4400)
            .with_prometheus_port(9090)
            .build()
            .expect("descriptor");

        assert_eq!(descriptor.validators().len(), topology.validators().len());
        assert_eq!(descriptor.executors().len(), topology.executors().len());
    }

    #[test]
    fn descriptor_includes_expected_env_and_ports() {
        let topology = TopologyBuilder::new(TopologyConfig::with_node_numbers(1, 1)).build();
        let cfgsync_port = 4555;
        let descriptor = ComposeDescriptor::builder(&topology)
            .with_cfgsync_port(cfgsync_port)
            .with_prometheus_port(9090)
            .build()
            .expect("descriptor");

        let validator = &descriptor.validators()[0];
        assert!(
            validator
                .environment()
                .iter()
                .any(|entry| entry.key() == "CFG_SERVER_ADDR"
                    && entry.value() == format!("http://host.docker.internal:{cfgsync_port}"))
        );

        let api_container = topology.validators()[0].general.api_config.address.port();
        assert!(validator.ports().contains(&api_container.to_string()));
    }
}




════════════════════════════════════════════════
FILE: testing-framework/runners/compose/src/lib.rs
────────────────────────────────────────────────
mod cfgsync;
mod cleanup;
mod compose;
mod runner;
mod wait;
mod workspace;

pub use runner::{ComposeRunner, ComposeRunnerError};
pub use workspace::ComposeWorkspace;




════════════════════════════════════════════════
FILE: testing-framework/runners/compose/src/runner.rs
────────────────────────────────────────────────
use std::{
    env,
    net::{Ipv4Addr, TcpListener as StdTcpListener},
    path::{Path, PathBuf},
    process::{Command as StdCommand, Stdio},
    sync::Arc,
    time::Duration,
};

use anyhow::{Context as _, anyhow};
use async_trait::async_trait;
use reqwest::Url;
use testing_framework_core::{
    nodes::ApiClient,
    scenario::{
        BlockFeed, BlockFeedTask, CleanupGuard, Deployer, DynError, Metrics, MetricsError,
        NodeClients, NodeControlHandle, RequiresNodeControl, RunContext, Runner, Scenario,
        http_probe::{HttpReadinessError, NodeRole as HttpNodeRole},
        spawn_block_feed,
    },
    topology::{GeneratedTopology, NodeRole as TopologyNodeRole, ReadinessError},
};
use tokio::{
    process::Command,
    time::{sleep, timeout},
};
use tracing::{error, info, warn};
use url::ParseError;
use uuid::Uuid;

use crate::{
    cfgsync::{CfgsyncServerHandle, start_cfgsync_server, update_cfgsync_config},
    cleanup::RunnerCleanup,
    compose::{
        ComposeCommandError, ComposeDescriptor, DescriptorBuildError, HostPortMapping,
        NodeHostPorts, TemplateError, compose_up, dump_compose_logs, repository_root,
        resolve_image, write_compose_file,
    },
    wait::{wait_for_executors, wait_for_validators},
    workspace::ComposeWorkspace,
};

pub struct ComposeRunner {
    readiness_checks: bool,
}

impl Default for ComposeRunner {
    fn default() -> Self {
        Self::new()
    }
}

impl ComposeRunner {
    #[must_use]
    pub const fn new() -> Self {
        Self {
            readiness_checks: true,
        }
    }

    #[must_use]
    pub const fn with_readiness(mut self, enabled: bool) -> Self {
        self.readiness_checks = enabled;
        self
    }
}

const PROMETHEUS_PORT_ENV: &str = "TEST_FRAMEWORK_PROMETHEUS_PORT";
const DEFAULT_PROMETHEUS_PORT: u16 = 9090;
const IMAGE_BUILD_TIMEOUT: Duration = Duration::from_secs(600);
const BLOCK_FEED_MAX_ATTEMPTS: usize = 5;
const BLOCK_FEED_RETRY_DELAY: Duration = Duration::from_secs(1);

#[derive(Debug, thiserror::Error)]
pub enum ComposeRunnerError {
    #[error(
        "compose runner requires at least one validator (validators={validators}, executors={executors})"
    )]
    MissingValidator { validators: usize, executors: usize },
    #[error("docker does not appear to be available on this host")]
    DockerUnavailable,
    #[error("failed to resolve host port for {service} container port {container_port}: {source}")]
    PortDiscovery {
        service: String,
        container_port: u16,
        #[source]
        source: anyhow::Error,
    },
    #[error(transparent)]
    Workspace(#[from] WorkspaceError),
    #[error(transparent)]
    Config(#[from] ConfigError),
    #[error(transparent)]
    Compose(#[from] ComposeCommandError),
    #[error(transparent)]
    Readiness(#[from] StackReadinessError),
    #[error(transparent)]
    NodeClients(#[from] NodeClientError),
    #[error(transparent)]
    Telemetry(#[from] MetricsError),
    #[error("block feed requires at least one validator client")]
    BlockFeedMissing,
    #[error("failed to start block feed: {source}")]
    BlockFeed {
        #[source]
        source: anyhow::Error,
    },
    #[error(
        "docker image '{image}' is not available; set NOMOS_TESTNET_IMAGE or build the image manually"
    )]
    MissingImage { image: String },
    #[error("failed to prepare docker image: {source}")]
    ImageBuild {
        #[source]
        source: anyhow::Error,
    },
}

#[derive(Debug, thiserror::Error)]
#[error("failed to prepare compose workspace: {source}")]
pub struct WorkspaceError {
    #[source]
    source: anyhow::Error,
}

impl WorkspaceError {
    const fn new(source: anyhow::Error) -> Self {
        Self { source }
    }
}

#[derive(Debug, thiserror::Error)]
pub enum ConfigError {
    #[error("failed to update cfgsync configuration at {path}: {source}")]
    Cfgsync {
        path: PathBuf,
        #[source]
        source: anyhow::Error,
    },
    #[error("failed to allocate cfgsync port: {source}")]
    Port {
        #[source]
        source: anyhow::Error,
    },
    #[error("failed to start cfgsync server on port {port}: {source}")]
    CfgsyncStart {
        port: u16,
        #[source]
        source: anyhow::Error,
    },
    #[error("failed to build compose descriptor: {source}")]
    Descriptor {
        #[source]
        source: DescriptorBuildError,
    },
    #[error("failed to render compose template: {source}")]
    Template {
        #[source]
        source: TemplateError,
    },
}

#[derive(Debug, thiserror::Error)]
pub enum StackReadinessError {
    #[error(transparent)]
    Http(#[from] HttpReadinessError),
    #[error("failed to build readiness URL for {role} port {port}: {source}")]
    Endpoint {
        role: HttpNodeRole,
        port: u16,
        #[source]
        source: ParseError,
    },
    #[error("remote readiness probe failed: {source}")]
    Remote {
        #[source]
        source: ReadinessError,
    },
}

#[derive(Debug, thiserror::Error)]
pub enum NodeClientError {
    #[error("failed to build {endpoint} client URL for {role} port {port}: {source}")]
    Endpoint {
        role: HttpNodeRole,
        endpoint: &'static str,
        port: u16,
        #[source]
        source: ParseError,
    },
}

#[async_trait]
impl<Caps> Deployer<Caps> for ComposeRunner
where
    Caps: RequiresNodeControl + Send + Sync,
{
    type Error = ComposeRunnerError;

    async fn deploy(&self, scenario: &Scenario<Caps>) -> Result<Runner, Self::Error> {
        ensure_docker_available()?;
        let descriptors = scenario.topology().clone();
        ensure_supported_topology(&descriptors)?;

        info!(
            validators = descriptors.validators().len(),
            executors = descriptors.executors().len(),
            "starting compose deployment"
        );

        let prometheus_port = desired_prometheus_port();
        let mut environment = prepare_environment(&descriptors, prometheus_port).await?;

        let host_ports = match discover_host_ports(&environment, &descriptors).await {
            Ok(mapping) => mapping,
            Err(err) => {
                environment
                    .fail("failed to determine container host ports")
                    .await;
                return Err(err);
            }
        };

        if self.readiness_checks {
            info!("waiting for validator HTTP endpoints");
            if let Err(err) =
                ensure_validators_ready_with_ports(&host_ports.validator_api_ports()).await
            {
                environment.fail("validator readiness failed").await;
                return Err(err.into());
            }

            info!("waiting for executor HTTP endpoints");
            if let Err(err) =
                ensure_executors_ready_with_ports(&host_ports.executor_api_ports()).await
            {
                environment.fail("executor readiness failed").await;
                return Err(err.into());
            }

            info!("waiting for remote service readiness");
            if let Err(err) = ensure_remote_readiness_with_ports(&descriptors, &host_ports).await {
                environment.fail("remote readiness probe failed").await;
                return Err(err.into());
            }
        } else {
            info!("readiness checks disabled; giving the stack a short grace period");
            sleep(Duration::from_secs(5)).await;
        }

        info!("compose stack ready; building node clients");
        let node_clients = match build_node_clients_with_ports(&descriptors, &host_ports) {
            Ok(clients) => clients,
            Err(err) => {
                environment
                    .fail("failed to construct node api clients")
                    .await;
                return Err(err.into());
            }
        };
        let telemetry = metrics_handle_from_port(prometheus_port)?;
        let node_control = Caps::REQUIRED.then(|| {
            Arc::new(ComposeNodeControl {
                compose_file: environment.compose_path().to_path_buf(),
                project_name: environment.project_name().to_owned(),
            }) as Arc<dyn NodeControlHandle>
        });
        let (block_feed, block_feed_guard) = match spawn_block_feed_with_retry(&node_clients).await
        {
            Ok(pair) => pair,
            Err(err) => {
                environment.fail("failed to initialize block feed").await;
                return Err(err);
            }
        };
        let cleanup_guard: Box<dyn CleanupGuard> = Box::new(ComposeCleanupGuard::new(
            environment.into_cleanup(),
            block_feed_guard,
        ));
        let context = RunContext::new(
            descriptors,
            None,
            node_clients,
            scenario.duration(),
            telemetry,
            block_feed,
            node_control,
        );

        Ok(Runner::new(context, Some(cleanup_guard)))
    }
}

fn desired_prometheus_port() -> u16 {
    env::var(PROMETHEUS_PORT_ENV)
        .ok()
        .and_then(|raw| raw.parse::<u16>().ok())
        .unwrap_or_else(|| allocate_prometheus_port().unwrap_or(DEFAULT_PROMETHEUS_PORT))
}

fn allocate_prometheus_port() -> Option<u16> {
    let try_bind = |port| StdTcpListener::bind((Ipv4Addr::LOCALHOST, port));
    let listener = try_bind(DEFAULT_PROMETHEUS_PORT)
        .or_else(|_| try_bind(0))
        .ok()?;
    listener.local_addr().ok().map(|addr| addr.port())
}

fn build_node_clients_with_ports(
    descriptors: &GeneratedTopology,
    mapping: &HostPortMapping,
) -> Result<NodeClients, NodeClientError> {
    let validators = descriptors
        .validators()
        .iter()
        .zip(mapping.validators.iter())
        .map(|(node, ports)| api_client_from_host_ports(to_http_role(node.role()), ports))
        .collect::<Result<Vec<_>, _>>()?;
    let executors = descriptors
        .executors()
        .iter()
        .zip(mapping.executors.iter())
        .map(|(node, ports)| api_client_from_host_ports(to_http_role(node.role()), ports))
        .collect::<Result<Vec<_>, _>>()?;

    Ok(NodeClients::new(validators, executors))
}

fn api_client_from_host_ports(
    role: HttpNodeRole,
    ports: &NodeHostPorts,
) -> Result<ApiClient, NodeClientError> {
    let base_url = localhost_url(ports.api).map_err(|source| NodeClientError::Endpoint {
        role,
        endpoint: "api",
        port: ports.api,
        source,
    })?;

    let testing_url =
        Some(
            localhost_url(ports.testing).map_err(|source| NodeClientError::Endpoint {
                role,
                endpoint: "testing",
                port: ports.testing,
                source,
            })?,
        );

    Ok(ApiClient::from_urls(base_url, testing_url))
}

const fn to_http_role(role: TopologyNodeRole) -> HttpNodeRole {
    match role {
        TopologyNodeRole::Validator => HttpNodeRole::Validator,
        TopologyNodeRole::Executor => HttpNodeRole::Executor,
    }
}

async fn spawn_block_feed_with(
    node_clients: &NodeClients,
) -> Result<(BlockFeed, BlockFeedTask), ComposeRunnerError> {
    let block_source_client = node_clients
        .random_validator()
        .cloned()
        .ok_or(ComposeRunnerError::BlockFeedMissing)?;

    spawn_block_feed(block_source_client)
        .await
        .map_err(|source| ComposeRunnerError::BlockFeed { source })
}

async fn spawn_block_feed_with_retry(
    node_clients: &NodeClients,
) -> Result<(BlockFeed, BlockFeedTask), ComposeRunnerError> {
    let mut last_err = None;
    for attempt in 1..=BLOCK_FEED_MAX_ATTEMPTS {
        match spawn_block_feed_with(node_clients).await {
            Ok(result) => return Ok(result),
            Err(err) => {
                last_err = Some(err);
                if attempt < BLOCK_FEED_MAX_ATTEMPTS {
                    warn!(attempt, "block feed initialization failed; retrying");
                    sleep(BLOCK_FEED_RETRY_DELAY).await;
                }
            }
        }
    }

    Err(last_err.expect("block feed retry should capture an error"))
}

async fn restart_compose_service(
    compose_file: &Path,
    project_name: &str,
    service: &str,
) -> Result<(), ComposeRunnerError> {
    let mut command = Command::new("docker");
    command
        .arg("compose")
        .arg("-f")
        .arg(compose_file)
        .arg("-p")
        .arg(project_name)
        .arg("restart")
        .arg(service);

    let description = "docker compose restart";
    run_docker_command(command, description, Duration::from_secs(120)).await
}

struct ComposeNodeControl {
    compose_file: PathBuf,
    project_name: String,
}

#[async_trait]
impl NodeControlHandle for ComposeNodeControl {
    async fn restart_validator(&self, index: usize) -> Result<(), DynError> {
        restart_compose_service(
            &self.compose_file,
            &self.project_name,
            &format!("validator-{index}"),
        )
        .await
        .map_err(|err| format!("validator restart failed: {err}").into())
    }

    async fn restart_executor(&self, index: usize) -> Result<(), DynError> {
        restart_compose_service(
            &self.compose_file,
            &self.project_name,
            &format!("executor-{index}"),
        )
        .await
        .map_err(|err| format!("executor restart failed: {err}").into())
    }
}

fn localhost_url(port: u16) -> Result<Url, ParseError> {
    Url::parse(&format!("http://127.0.0.1:{port}/"))
}

async fn discover_host_ports(
    environment: &StackEnvironment,
    descriptors: &GeneratedTopology,
) -> Result<HostPortMapping, ComposeRunnerError> {
    let mut validators = Vec::new();
    for node in descriptors.validators() {
        let service = node_identifier(TopologyNodeRole::Validator, node.index());
        let api = resolve_service_port(environment, &service, node.api_port()).await?;
        let testing = resolve_service_port(environment, &service, node.testing_http_port()).await?;
        validators.push(NodeHostPorts { api, testing });
    }

    let mut executors = Vec::new();
    for node in descriptors.executors() {
        let service = node_identifier(TopologyNodeRole::Executor, node.index());
        let api = resolve_service_port(environment, &service, node.api_port()).await?;
        let testing = resolve_service_port(environment, &service, node.testing_http_port()).await?;
        executors.push(NodeHostPorts { api, testing });
    }

    Ok(HostPortMapping {
        validators,
        executors,
    })
}

async fn resolve_service_port(
    environment: &StackEnvironment,
    service: &str,
    container_port: u16,
) -> Result<u16, ComposeRunnerError> {
    let mut cmd = Command::new("docker");
    cmd.arg("compose")
        .arg("-f")
        .arg(environment.compose_path())
        .arg("-p")
        .arg(environment.project_name())
        .arg("port")
        .arg(service)
        .arg(container_port.to_string())
        .current_dir(environment.root());

    let output = cmd
        .output()
        .await
        .with_context(|| format!("running docker compose port {service} {container_port}"))
        .map_err(|source| ComposeRunnerError::PortDiscovery {
            service: service.to_owned(),
            container_port,
            source,
        })?;

    if !output.status.success() {
        return Err(ComposeRunnerError::PortDiscovery {
            service: service.to_owned(),
            container_port,
            source: anyhow!("docker compose port exited with {}", output.status),
        });
    }

    let stdout = String::from_utf8_lossy(&output.stdout);
    for line in stdout.lines() {
        let line = line.trim();
        if line.is_empty() {
            continue;
        }
        if let Some(port_str) = line.rsplit(':').next()
            && let Ok(port) = port_str.trim().parse::<u16>()
        {
            return Ok(port);
        }
    }

    Err(ComposeRunnerError::PortDiscovery {
        service: service.to_owned(),
        container_port,
        source: anyhow!("unable to parse docker compose port output: {stdout}"),
    })
}

fn ensure_docker_available() -> Result<(), ComposeRunnerError> {
    let available = StdCommand::new("docker")
        .arg("info")
        .stdout(Stdio::null())
        .stderr(Stdio::null())
        .status()
        .map(|status| status.success())
        .unwrap_or(false);
    if available {
        Ok(())
    } else {
        Err(ComposeRunnerError::DockerUnavailable)
    }
}

fn metrics_handle_from_port(port: u16) -> Result<Metrics, MetricsError> {
    let url = localhost_url(port)
        .map_err(|err| MetricsError::new(format!("invalid prometheus url: {err}")))?;
    Metrics::from_prometheus(url)
}

async fn ensure_validators_ready_with_ports(ports: &[u16]) -> Result<(), StackReadinessError> {
    if ports.is_empty() {
        return Ok(());
    }

    wait_for_validators(ports).await.map_err(Into::into)
}

async fn ensure_executors_ready_with_ports(ports: &[u16]) -> Result<(), StackReadinessError> {
    if ports.is_empty() {
        return Ok(());
    }

    wait_for_executors(ports).await.map_err(Into::into)
}

async fn ensure_remote_readiness_with_ports(
    descriptors: &GeneratedTopology,
    mapping: &HostPortMapping,
) -> Result<(), StackReadinessError> {
    let validator_urls = mapping
        .validators
        .iter()
        .map(|ports| readiness_url(HttpNodeRole::Validator, ports.api))
        .collect::<Result<Vec<_>, _>>()?;
    let executor_urls = mapping
        .executors
        .iter()
        .map(|ports| readiness_url(HttpNodeRole::Executor, ports.api))
        .collect::<Result<Vec<_>, _>>()?;

    let validator_membership_urls = mapping
        .validators
        .iter()
        .map(|ports| readiness_url(HttpNodeRole::Validator, ports.testing))
        .collect::<Result<Vec<_>, _>>()?;
    let executor_membership_urls = mapping
        .executors
        .iter()
        .map(|ports| readiness_url(HttpNodeRole::Executor, ports.testing))
        .collect::<Result<Vec<_>, _>>()?;

    descriptors
        .wait_remote_readiness(
            &validator_urls,
            &executor_urls,
            Some(&validator_membership_urls),
            Some(&executor_membership_urls),
        )
        .await
        .map_err(|source| StackReadinessError::Remote { source })
}

fn readiness_url(role: HttpNodeRole, port: u16) -> Result<Url, StackReadinessError> {
    localhost_url(port).map_err(|source| StackReadinessError::Endpoint { role, port, source })
}

fn node_identifier(role: TopologyNodeRole, index: usize) -> String {
    match role {
        TopologyNodeRole::Validator => format!("validator-{index}"),
        TopologyNodeRole::Executor => format!("executor-{index}"),
    }
}

struct WorkspaceState {
    workspace: ComposeWorkspace,
    root: PathBuf,
    cfgsync_path: PathBuf,
    use_kzg: bool,
}

fn ensure_supported_topology(descriptors: &GeneratedTopology) -> Result<(), ComposeRunnerError> {
    let validators = descriptors.validators().len();
    if validators == 0 {
        return Err(ComposeRunnerError::MissingValidator {
            validators,
            executors: descriptors.executors().len(),
        });
    }
    Ok(())
}

async fn prepare_environment(
    descriptors: &GeneratedTopology,
    prometheus_port: u16,
) -> Result<StackEnvironment, ComposeRunnerError> {
    let workspace = prepare_workspace_logged()?;
    update_cfgsync_logged(&workspace, descriptors)?;
    ensure_compose_image().await?;

    let (cfgsync_port, mut cfgsync_handle) = start_cfgsync_stage(&workspace).await?;
    let compose_path =
        render_compose_logged(&workspace, descriptors, cfgsync_port, prometheus_port)?;

    let project_name = format!("nomos-compose-{}", Uuid::new_v4());
    bring_up_stack_logged(
        &compose_path,
        &project_name,
        &workspace.root,
        &mut cfgsync_handle,
    )
    .await?;

    Ok(StackEnvironment::from_workspace(
        workspace,
        compose_path,
        project_name,
        Some(cfgsync_handle),
    ))
}

fn prepare_workspace_state() -> Result<WorkspaceState, WorkspaceError> {
    let workspace = ComposeWorkspace::create().map_err(WorkspaceError::new)?;
    let root = workspace.root_path().to_path_buf();
    let cfgsync_path = workspace.testnet_dir().join("cfgsync.yaml");
    let use_kzg = workspace.root_path().join("kzgrs_test_params").exists();

    Ok(WorkspaceState {
        workspace,
        root,
        cfgsync_path,
        use_kzg,
    })
}

fn prepare_workspace_logged() -> Result<WorkspaceState, ComposeRunnerError> {
    info!("preparing compose workspace");
    prepare_workspace_state().map_err(Into::into)
}

fn update_cfgsync_logged(
    workspace: &WorkspaceState,
    descriptors: &GeneratedTopology,
) -> Result<(), ComposeRunnerError> {
    info!("updating cfgsync configuration");
    configure_cfgsync(workspace, descriptors).map_err(Into::into)
}

async fn start_cfgsync_stage(
    workspace: &WorkspaceState,
) -> Result<(u16, CfgsyncServerHandle), ComposeRunnerError> {
    let cfgsync_port = allocate_cfgsync_port()?;
    info!(cfgsync_port = cfgsync_port, "launching cfgsync server");
    let handle = launch_cfgsync(&workspace.cfgsync_path, cfgsync_port).await?;
    Ok((cfgsync_port, handle))
}

fn configure_cfgsync(
    workspace: &WorkspaceState,
    descriptors: &GeneratedTopology,
) -> Result<(), ConfigError> {
    update_cfgsync_config(&workspace.cfgsync_path, descriptors, workspace.use_kzg).map_err(
        |source| ConfigError::Cfgsync {
            path: workspace.cfgsync_path.clone(),
            source,
        },
    )
}

fn allocate_cfgsync_port() -> Result<u16, ConfigError> {
    let listener = StdTcpListener::bind((Ipv4Addr::LOCALHOST, 0))
        .context("allocating cfgsync port")
        .map_err(|source| ConfigError::Port { source })?;

    let port = listener
        .local_addr()
        .context("reading cfgsync port")
        .map_err(|source| ConfigError::Port { source })?
        .port();
    Ok(port)
}

async fn launch_cfgsync(
    cfgsync_path: &Path,
    port: u16,
) -> Result<CfgsyncServerHandle, ConfigError> {
    start_cfgsync_server(cfgsync_path, port)
        .await
        .map_err(|source| ConfigError::CfgsyncStart { port, source })
}

fn write_compose_artifacts(
    workspace: &WorkspaceState,
    descriptors: &GeneratedTopology,
    cfgsync_port: u16,
    prometheus_port: u16,
) -> Result<PathBuf, ConfigError> {
    let descriptor = ComposeDescriptor::builder(descriptors)
        .with_kzg_mount(workspace.use_kzg)
        .with_cfgsync_port(cfgsync_port)
        .with_prometheus_port(prometheus_port)
        .build()
        .map_err(|source| ConfigError::Descriptor { source })?;

    let compose_path = workspace.root.join("compose.generated.yml");
    write_compose_file(&descriptor, &compose_path)
        .map_err(|source| ConfigError::Template { source })?;
    Ok(compose_path)
}

fn render_compose_logged(
    workspace: &WorkspaceState,
    descriptors: &GeneratedTopology,
    cfgsync_port: u16,
    prometheus_port: u16,
) -> Result<PathBuf, ComposeRunnerError> {
    info!("rendering compose file");
    write_compose_artifacts(workspace, descriptors, cfgsync_port, prometheus_port)
        .map_err(Into::into)
}

async fn bring_up_stack(
    compose_path: &Path,
    project_name: &str,
    workspace_root: &Path,
    cfgsync_handle: &mut CfgsyncServerHandle,
) -> Result<(), ComposeRunnerError> {
    if let Err(err) = compose_up(compose_path, project_name, workspace_root).await {
        cfgsync_handle.shutdown();
        return Err(ComposeRunnerError::Compose(err));
    }
    Ok(())
}

async fn ensure_compose_image() -> Result<(), ComposeRunnerError> {
    let (image, platform) = resolve_image();
    ensure_image_present(&image, platform.as_deref()).await
}

async fn ensure_image_present(
    image: &str,
    platform: Option<&str>,
) -> Result<(), ComposeRunnerError> {
    if docker_image_exists(image).await? {
        return Ok(());
    }

    if image != "nomos-testnet:local" {
        return Err(ComposeRunnerError::MissingImage {
            image: image.to_owned(),
        });
    }

    build_local_image(image, platform).await
}

async fn docker_image_exists(image: &str) -> Result<bool, ComposeRunnerError> {
    let mut cmd = Command::new("docker");
    cmd.arg("image")
        .arg("inspect")
        .arg(image)
        .stdout(Stdio::null())
        .stderr(Stdio::null());

    match cmd.status().await {
        Ok(status) => Ok(status.success()),
        Err(source) => Err(ComposeRunnerError::Compose(ComposeCommandError::Spawn {
            command: format!("docker image inspect {image}"),
            source,
        })),
    }
}

async fn build_local_image(image: &str, platform: Option<&str>) -> Result<(), ComposeRunnerError> {
    let repo_root =
        repository_root().map_err(|source| ComposeRunnerError::ImageBuild { source })?;
    let dockerfile = repo_root.join("testing-framework/runners/docker/runner.Dockerfile");

    info!(image, "building compose runner docker image");

    let mut cmd = Command::new("docker");
    cmd.arg("build");

    if let Some(build_platform) = select_build_platform(platform)? {
        cmd.arg("--platform").arg(&build_platform);
    }

    let circuits_platform = env::var("COMPOSE_CIRCUITS_PLATFORM")
        .ok()
        .filter(|value| !value.is_empty())
        .unwrap_or_else(|| String::from("linux-x86_64"));

    cmd.arg("--build-arg")
        .arg(format!("NOMOS_CIRCUITS_PLATFORM={circuits_platform}"));

    if let Some(value) = env::var("CIRCUITS_OVERRIDE")
        .ok()
        .filter(|val| !val.is_empty())
    {
        cmd.arg("--build-arg")
            .arg(format!("CIRCUITS_OVERRIDE={value}"));
    }

    cmd.arg("-t")
        .arg(image)
        .arg("-f")
        .arg(&dockerfile)
        .arg(&repo_root);

    run_docker_command(cmd, "docker build compose image", IMAGE_BUILD_TIMEOUT).await
}

async fn run_docker_command(
    mut command: Command,
    description: &str,
    timeout_duration: Duration,
) -> Result<(), ComposeRunnerError> {
    match timeout(timeout_duration, command.status()).await {
        Ok(Ok(status)) if status.success() => Ok(()),
        Ok(Ok(status)) => Err(ComposeRunnerError::Compose(ComposeCommandError::Failed {
            command: description.to_owned(),
            status,
        })),
        Ok(Err(source)) => Err(ComposeRunnerError::Compose(ComposeCommandError::Spawn {
            command: description.to_owned(),
            source,
        })),
        Err(_) => Err(ComposeRunnerError::Compose(ComposeCommandError::Timeout {
            command: description.to_owned(),
            timeout: timeout_duration,
        })),
    }
}

fn detect_docker_platform() -> Result<Option<String>, ComposeRunnerError> {
    let output = StdCommand::new("docker")
        .arg("info")
        .arg("-f")
        .arg("{{.Architecture}}")
        .output()
        .map_err(|source| ComposeRunnerError::ImageBuild {
            source: source.into(),
        })?;

    if !output.status.success() {
        return Ok(None);
    }

    let arch = String::from_utf8_lossy(&output.stdout).trim().to_owned();
    if arch.is_empty() {
        return Ok(None);
    }

    Ok(Some(format!("linux/{arch}")))
}

fn select_build_platform(requested: Option<&str>) -> Result<Option<String>, ComposeRunnerError> {
    if let Some(value) = requested {
        return Ok(Some(value.to_owned()));
    }

    detect_docker_platform()?.map_or_else(
        || {
            warn!("docker host architecture unavailable; letting docker choose default platform");
            Ok(None)
        },
        |host_platform| Ok(Some(host_platform)),
    )
}

async fn bring_up_stack_logged(
    compose_path: &Path,
    project_name: &str,
    workspace_root: &Path,
    cfgsync_handle: &mut CfgsyncServerHandle,
) -> Result<(), ComposeRunnerError> {
    info!(project = %project_name, "bringing up docker compose stack");
    bring_up_stack(compose_path, project_name, workspace_root, cfgsync_handle).await
}

struct StackEnvironment {
    compose_path: PathBuf,
    project_name: String,
    root: PathBuf,
    workspace: Option<ComposeWorkspace>,
    cfgsync_handle: Option<CfgsyncServerHandle>,
}

impl StackEnvironment {
    fn from_workspace(
        state: WorkspaceState,
        compose_path: PathBuf,
        project_name: String,
        cfgsync_handle: Option<CfgsyncServerHandle>,
    ) -> Self {
        let WorkspaceState {
            workspace, root, ..
        } = state;

        Self {
            compose_path,
            project_name,
            root,
            workspace: Some(workspace),
            cfgsync_handle,
        }
    }

    fn compose_path(&self) -> &Path {
        &self.compose_path
    }

    fn project_name(&self) -> &str {
        &self.project_name
    }

    fn root(&self) -> &Path {
        &self.root
    }

    fn take_cleanup(&mut self) -> RunnerCleanup {
        RunnerCleanup::new(
            self.compose_path.clone(),
            self.project_name.clone(),
            self.root.clone(),
            self.workspace
                .take()
                .expect("workspace must be available while cleaning up"),
            self.cfgsync_handle.take(),
        )
    }

    fn into_cleanup(self) -> RunnerCleanup {
        RunnerCleanup::new(
            self.compose_path,
            self.project_name,
            self.root,
            self.workspace
                .expect("workspace must be available while cleaning up"),
            self.cfgsync_handle,
        )
    }

    async fn fail(&mut self, reason: &str) {
        error!(
            reason = reason,
            "compose stack failure; dumping docker logs"
        );
        dump_compose_logs(self.compose_path(), self.project_name(), self.root()).await;
        Box::new(self.take_cleanup()).cleanup();
    }
}

struct ComposeCleanupGuard {
    environment: RunnerCleanup,
    block_feed: Option<BlockFeedTask>,
}

impl ComposeCleanupGuard {
    const fn new(environment: RunnerCleanup, block_feed: BlockFeedTask) -> Self {
        Self {
            environment,
            block_feed: Some(block_feed),
        }
    }
}

impl CleanupGuard for ComposeCleanupGuard {
    fn cleanup(mut self: Box<Self>) {
        if let Some(block_feed) = self.block_feed.take() {
            CleanupGuard::cleanup(Box::new(block_feed));
        }
        CleanupGuard::cleanup(Box::new(self.environment));
    }
}

#[cfg(test)]
mod tests {
    use std::{collections::HashMap, net::Ipv4Addr};

    use cfgsync::config::{Host, PortOverrides, create_node_configs};
    use groth16::Fr;
    use nomos_core::{
        mantle::{GenesisTx as GenesisTxTrait, ledger::NoteId},
        sdp::{ProviderId, ServiceType},
    };
    use nomos_ledger::LedgerState;
    use nomos_tracing_service::TracingSettings;
    use testing_framework_core::{
        scenario::ScenarioBuilder,
        topology::{GeneratedNodeConfig, GeneratedTopology, NodeRole as TopologyNodeRole},
    };
    use zksign::PublicKey;

    #[test]
    fn cfgsync_prebuilt_configs_preserve_genesis() {
        let scenario = ScenarioBuilder::with_node_counts(1, 1).build();
        let topology = scenario.topology().clone();
        let hosts = hosts_from_topology(&topology);
        let tracing_settings = tracing_settings(&topology);

        let configs = create_node_configs(
            &topology.config().consensus_params,
            &topology.config().da_params,
            &tracing_settings,
            &topology.config().wallet_config,
            Some(topology.nodes().map(|node| node.id).collect()),
            Some(topology.nodes().map(|node| node.da_port).collect()),
            Some(topology.nodes().map(|node| node.blend_port).collect()),
            hosts,
        );
        let configs_by_identifier: HashMap<_, _> = configs
            .into_iter()
            .map(|(host, config)| (host.identifier, config))
            .collect();

        for node in topology.nodes() {
            let identifier = identifier_for(node.role(), node.index());
            let cfgsync_config = configs_by_identifier
                .get(&identifier)
                .unwrap_or_else(|| panic!("missing cfgsync config for {identifier}"));
            let expected_genesis = &node.general.consensus_config.genesis_tx;
            let actual_genesis = &cfgsync_config.consensus_config.genesis_tx;
            if std::env::var("PRINT_GENESIS").is_ok() {
                println!(
                    "[fingerprint {identifier}] expected={:?}",
                    declaration_fingerprint(expected_genesis)
                );
                println!(
                    "[fingerprint {identifier}] actual={:?}",
                    declaration_fingerprint(actual_genesis)
                );
            }
            assert_eq!(
                expected_genesis.mantle_tx().ledger_tx,
                actual_genesis.mantle_tx().ledger_tx,
                "ledger tx mismatch for {identifier}"
            );
            assert_eq!(
                declaration_fingerprint(expected_genesis),
                declaration_fingerprint(actual_genesis),
                "declaration entries mismatch for {identifier}"
            );
        }
    }

    #[test]
    fn cfgsync_genesis_proofs_verify_against_ledger() {
        let scenario = ScenarioBuilder::with_node_counts(1, 1).build();
        let topology = scenario.topology().clone();
        let hosts = hosts_from_topology(&topology);
        let tracing_settings = tracing_settings(&topology);

        let configs = create_node_configs(
            &topology.config().consensus_params,
            &topology.config().da_params,
            &tracing_settings,
            &topology.config().wallet_config,
            Some(topology.nodes().map(|node| node.id).collect()),
            Some(topology.nodes().map(|node| node.da_port).collect()),
            Some(topology.nodes().map(|node| node.blend_port).collect()),
            hosts,
        );
        let configs_by_identifier: HashMap<_, _> = configs
            .into_iter()
            .map(|(host, config)| (host.identifier, config))
            .collect();

        for node in topology.nodes() {
            let identifier = identifier_for(node.role(), node.index());
            let cfgsync_config = configs_by_identifier
                .get(&identifier)
                .unwrap_or_else(|| panic!("missing cfgsync config for {identifier}"));
            LedgerState::from_genesis_tx::<()>(
                cfgsync_config.consensus_config.genesis_tx.clone(),
                &cfgsync_config.consensus_config.ledger_config,
                Fr::from(0u64),
            )
            .unwrap_or_else(|err| panic!("ledger rejected genesis for {identifier}: {err:?}"));
        }
    }

    #[test]
    fn cfgsync_docker_overrides_produce_valid_genesis() {
        let scenario = ScenarioBuilder::with_node_counts(1, 1).build();
        let topology = scenario.topology().clone();
        let tracing_settings = tracing_settings(&topology);
        let hosts = docker_style_hosts(&topology);

        let configs = create_node_configs(
            &topology.config().consensus_params,
            &topology.config().da_params,
            &tracing_settings,
            &topology.config().wallet_config,
            Some(topology.nodes().map(|node| node.id).collect()),
            Some(topology.nodes().map(|node| node.da_port).collect()),
            Some(topology.nodes().map(|node| node.blend_port).collect()),
            hosts,
        );

        for (host, config) in configs {
            let genesis = &config.consensus_config.genesis_tx;
            LedgerState::from_genesis_tx::<()>(
                genesis.clone(),
                &config.consensus_config.ledger_config,
                Fr::from(0u64),
            )
            .unwrap_or_else(|err| {
                panic!("ledger rejected genesis for {}: {err:?}", host.identifier)
            });
        }
    }

    fn hosts_from_topology(topology: &GeneratedTopology) -> Vec<Host> {
        topology.nodes().map(host_from_node).collect()
    }

    fn docker_style_hosts(topology: &GeneratedTopology) -> Vec<Host> {
        topology
            .nodes()
            .map(|node| docker_host(node, 10 + node.index() as u8))
            .collect()
    }

    fn host_from_node(node: &GeneratedNodeConfig) -> Host {
        let identifier = identifier_for(node.role(), node.index());
        let ip = Ipv4Addr::LOCALHOST;
        let mut host = make_host(node.role(), ip, identifier);
        host.network_port = node.network_port();
        host.da_network_port = node.da_port;
        host.blend_port = node.blend_port;
        host
    }

    fn docker_host(node: &GeneratedNodeConfig, octet: u8) -> Host {
        let identifier = identifier_for(node.role(), node.index());
        let ip = Ipv4Addr::new(172, 23, 0, octet);
        let mut host = make_host(node.role(), ip, identifier);
        host.network_port = node.network_port() + 1000;
        host.da_network_port = node.da_port + 1000;
        host.blend_port = node.blend_port + 1000;
        host
    }

    fn tracing_settings(topology: &GeneratedTopology) -> TracingSettings {
        topology
            .validators()
            .first()
            .or_else(|| topology.executors().first())
            .expect("topology must contain at least one node")
            .general
            .tracing_config
            .tracing_settings
            .clone()
    }

    fn identifier_for(role: TopologyNodeRole, index: usize) -> String {
        match role {
            TopologyNodeRole::Validator => format!("validator-{index}"),
            TopologyNodeRole::Executor => format!("executor-{index}"),
        }
    }

    fn make_host(role: TopologyNodeRole, ip: Ipv4Addr, identifier: String) -> Host {
        let ports = PortOverrides {
            network_port: None,
            da_network_port: None,
            blend_port: None,
            api_port: None,
            testing_http_port: None,
        };
        match role {
            TopologyNodeRole::Validator => Host::validator_from_ip(ip, identifier, ports),
            TopologyNodeRole::Executor => Host::executor_from_ip(ip, identifier, ports),
        }
    }

    fn declaration_fingerprint<G>(genesis: &G) -> Vec<(ServiceType, ProviderId, NoteId, PublicKey)>
    where
        G: GenesisTxTrait,
    {
        genesis
            .sdp_declarations()
            .map(|(op, _)| (op.service_type, op.provider_id, op.locked_note_id, op.zk_id))
            .collect()
    }
}




════════════════════════════════════════════════
FILE: testing-framework/runners/compose/src/wait.rs
────────────────────────────────────────────────
use std::time::Duration;

use testing_framework_core::{
    adjust_timeout,
    scenario::http_probe::{self, HttpReadinessError, NodeRole},
};

const DEFAULT_WAIT: Duration = Duration::from_secs(90);
const POLL_INTERVAL: Duration = Duration::from_millis(250);

pub async fn wait_for_validators(ports: &[u16]) -> Result<(), HttpReadinessError> {
    wait_for_ports(ports, NodeRole::Validator).await
}

pub async fn wait_for_executors(ports: &[u16]) -> Result<(), HttpReadinessError> {
    wait_for_ports(ports, NodeRole::Executor).await
}

async fn wait_for_ports(ports: &[u16], role: NodeRole) -> Result<(), HttpReadinessError> {
    http_probe::wait_for_http_ports(ports, role, adjust_timeout(DEFAULT_WAIT), POLL_INTERVAL).await
}




════════════════════════════════════════════════
FILE: testing-framework/runners/compose/src/workspace.rs
────────────────────────────────────────────────
use std::{
    env, fs,
    path::{Path, PathBuf},
};

use anyhow::{Context as _, Result};
use tempfile::TempDir;

/// Copy the repository `testnet/` directory into a scenario-specific temp dir.
#[derive(Debug)]
pub struct ComposeWorkspace {
    root: TempDir,
}

impl ComposeWorkspace {
    /// Clone the testnet assets into a temporary directory.
    pub fn create() -> Result<Self> {
        let repo_root = env::var("CARGO_WORKSPACE_DIR")
            .map(PathBuf::from)
            .or_else(|_| {
                Path::new(env!("CARGO_MANIFEST_DIR"))
                    .parent()
                    .and_then(Path::parent)
                    .and_then(Path::parent)
                    .map(Path::to_path_buf)
                    .context("resolving workspace root from manifest dir")
            })
            .context("locating repository root")?;
        let temp = tempfile::Builder::new()
            .prefix("nomos-testnet-")
            .tempdir()
            .context("creating testnet temp dir")?;
        let testnet_source = repo_root.join("testnet");
        if !testnet_source.exists() {
            anyhow::bail!(
                "testnet directory not found at {}",
                testnet_source.display()
            );
        }
        copy_dir_recursive(&testnet_source, &temp.path().join("testnet"))?;

        let kzg_source = repo_root.join("tests/kzgrs/kzgrs_test_params");
        if kzg_source.exists() {
            let target = temp.path().join("kzgrs_test_params");
            if kzg_source.is_dir() {
                copy_dir_recursive(&kzg_source, &target)?;
            } else {
                fs::copy(&kzg_source, &target).with_context(|| {
                    format!("copying {} -> {}", kzg_source.display(), target.display())
                })?;
            }
        }

        Ok(Self { root: temp })
    }

    #[must_use]
    pub fn root_path(&self) -> &Path {
        self.root.path()
    }

    #[must_use]
    pub fn testnet_dir(&self) -> PathBuf {
        self.root.path().join("testnet")
    }

    #[must_use]
    pub fn into_inner(self) -> TempDir {
        self.root
    }
}

fn copy_dir_recursive(source: &Path, target: &Path) -> Result<()> {
    fs::create_dir_all(target)
        .with_context(|| format!("creating target dir {}", target.display()))?;
    for entry in fs::read_dir(source).with_context(|| format!("reading {}", source.display()))? {
        let entry = entry?;
        let file_type = entry.file_type()?;
        let dest = target.join(entry.file_name());
        if file_type.is_dir() {
            copy_dir_recursive(&entry.path(), &dest)?;
        } else if !file_type.is_dir() {
            fs::copy(entry.path(), &dest).with_context(|| {
                format!("copying {} -> {}", entry.path().display(), dest.display())
            })?;
        }
    }
    Ok(())
}




════════════════════════════════════════════════
FILE: testing-framework/runners/k8s/src/assets.rs
────────────────────────────────────────────────
use std::{
    collections::BTreeMap,
    env, fs, io,
    path::{Path, PathBuf},
};

use anyhow::{Context as _, Result as AnyResult};
use serde::Serialize;
use tempfile::TempDir;
use testing_framework_core::{
    scenario::cfgsync::{apply_topology_overrides, load_cfgsync_template, render_cfgsync_yaml},
    topology::GeneratedTopology,
};
use thiserror::Error;

pub struct RunnerAssets {
    pub image: String,
    pub kzg_path: PathBuf,
    pub chart_path: PathBuf,
    pub cfgsync_file: PathBuf,
    pub run_cfgsync_script: PathBuf,
    pub run_nomos_node_script: PathBuf,
    pub run_nomos_executor_script: PathBuf,
    pub values_file: PathBuf,
    _tempdir: TempDir,
}

pub const CFGSYNC_PORT: u16 = 4400;

#[derive(Debug, Error)]
pub enum AssetsError {
    #[error("failed to locate workspace root: {source}")]
    WorkspaceRoot {
        #[source]
        source: anyhow::Error,
    },
    #[error("failed to render cfgsync configuration: {source}")]
    Cfgsync {
        #[source]
        source: anyhow::Error,
    },
    #[error("missing required script at {path}")]
    MissingScript { path: PathBuf },
    #[error("missing KZG parameters at {path}; build them with `make kzgrs_test_params`")]
    MissingKzg { path: PathBuf },
    #[error("missing Helm chart at {path}; ensure the repository is up-to-date")]
    MissingChart { path: PathBuf },
    #[error("failed to create temporary directory for rendered assets: {source}")]
    TempDir {
        #[source]
        source: io::Error,
    },
    #[error("failed to write asset at {path}: {source}")]
    Io {
        path: PathBuf,
        #[source]
        source: io::Error,
    },
    #[error("failed to render Helm values: {source}")]
    Values {
        #[source]
        source: serde_yaml::Error,
    },
}

pub fn prepare_assets(topology: &GeneratedTopology) -> Result<RunnerAssets, AssetsError> {
    let root = workspace_root().map_err(|source| AssetsError::WorkspaceRoot { source })?;
    let cfgsync_yaml = render_cfgsync_config(&root, topology)?;

    let tempdir = tempfile::Builder::new()
        .prefix("nomos-helm-")
        .tempdir()
        .map_err(|source| AssetsError::TempDir { source })?;

    let cfgsync_file = write_temp_file(tempdir.path(), "cfgsync.yaml", cfgsync_yaml)?;
    let scripts = validate_scripts(&root)?;
    let kzg_path = validate_kzg_params(&root)?;
    let chart_path = helm_chart_path()?;
    let values_yaml = render_values_yaml(topology)?;
    let values_file = write_temp_file(tempdir.path(), "values.yaml", values_yaml)?;
    let image =
        env::var("NOMOS_TESTNET_IMAGE").unwrap_or_else(|_| String::from("nomos-testnet:local"));

    Ok(RunnerAssets {
        image,
        kzg_path,
        chart_path,
        cfgsync_file,
        run_cfgsync_script: scripts.run_cfgsync,
        run_nomos_node_script: scripts.run_node,
        run_nomos_executor_script: scripts.run_executor,
        values_file,
        _tempdir: tempdir,
    })
}

const CFGSYNC_K8S_TIMEOUT_SECS: u64 = 300;

fn render_cfgsync_config(root: &Path, topology: &GeneratedTopology) -> Result<String, AssetsError> {
    let cfgsync_template_path = root.join("testnet/cfgsync.yaml");
    let mut cfg = load_cfgsync_template(&cfgsync_template_path)
        .map_err(|source| AssetsError::Cfgsync { source })?;
    apply_topology_overrides(&mut cfg, topology, true);
    cfg.timeout = cfg.timeout.max(CFGSYNC_K8S_TIMEOUT_SECS);
    render_cfgsync_yaml(&cfg).map_err(|source| AssetsError::Cfgsync { source })
}

struct ScriptPaths {
    run_cfgsync: PathBuf,
    run_node: PathBuf,
    run_executor: PathBuf,
}

fn validate_scripts(root: &Path) -> Result<ScriptPaths, AssetsError> {
    let scripts_dir = root.join("testnet/scripts");
    let run_cfgsync = scripts_dir.join("run_cfgsync.sh");
    let run_node = scripts_dir.join("run_nomos_node.sh");
    let run_executor = scripts_dir.join("run_nomos_executor.sh");

    for path in [&run_cfgsync, &run_node, &run_executor] {
        if !path.exists() {
            return Err(AssetsError::MissingScript { path: path.clone() });
        }
    }

    Ok(ScriptPaths {
        run_cfgsync,
        run_node,
        run_executor,
    })
}

fn validate_kzg_params(root: &Path) -> Result<PathBuf, AssetsError> {
    let path = root.join("tests/kzgrs/kzgrs_test_params");
    if path.exists() {
        Ok(path)
    } else {
        Err(AssetsError::MissingKzg { path })
    }
}

fn helm_chart_path() -> Result<PathBuf, AssetsError> {
    let path = Path::new(env!("CARGO_MANIFEST_DIR")).join("helm/nomos-runner");
    if path.exists() {
        Ok(path)
    } else {
        Err(AssetsError::MissingChart { path })
    }
}

fn render_values_yaml(topology: &GeneratedTopology) -> Result<String, AssetsError> {
    let values = build_values(topology);
    serde_yaml::to_string(&values).map_err(|source| AssetsError::Values { source })
}

fn write_temp_file(
    dir: &Path,
    name: &str,
    contents: impl AsRef<[u8]>,
) -> Result<PathBuf, AssetsError> {
    let path = dir.join(name);
    fs::write(&path, contents).map_err(|source| AssetsError::Io {
        path: path.clone(),
        source,
    })?;
    Ok(path)
}

pub fn workspace_root() -> AnyResult<PathBuf> {
    if let Ok(var) = env::var("CARGO_WORKSPACE_DIR") {
        return Ok(PathBuf::from(var));
    }
    let manifest_dir = Path::new(env!("CARGO_MANIFEST_DIR"));
    manifest_dir
        .parent()
        .and_then(Path::parent)
        .and_then(Path::parent)
        .map(Path::to_path_buf)
        .context("resolving workspace root from manifest dir")
}

#[derive(Serialize)]
struct HelmValues {
    validators: NodeGroup,
    executors: NodeGroup,
}

#[derive(Serialize)]
struct NodeGroup {
    count: usize,
    nodes: Vec<NodeValues>,
}

#[derive(Serialize)]
struct NodeValues {
    #[serde(rename = "apiPort")]
    api_port: u16,
    #[serde(rename = "testingHttpPort")]
    testing_http_port: u16,
    env: BTreeMap<String, String>,
}

fn build_values(topology: &GeneratedTopology) -> HelmValues {
    let validators = topology
        .validators()
        .iter()
        .enumerate()
        .map(|(index, validator)| {
            let mut env = BTreeMap::new();
            env.insert(
                "CFG_NETWORK_PORT".into(),
                validator.network_port().to_string(),
            );
            env.insert("CFG_DA_PORT".into(), validator.da_port.to_string());
            env.insert("CFG_BLEND_PORT".into(), validator.blend_port.to_string());
            env.insert(
                "CFG_API_PORT".into(),
                validator.general.api_config.address.port().to_string(),
            );
            env.insert(
                "CFG_TESTING_HTTP_PORT".into(),
                validator
                    .general
                    .api_config
                    .testing_http_address
                    .port()
                    .to_string(),
            );
            env.insert("CFG_HOST_KIND".into(), "validator".into());
            env.insert("CFG_HOST_IDENTIFIER".into(), format!("validator-{index}"));

            NodeValues {
                api_port: validator.general.api_config.address.port(),
                testing_http_port: validator.general.api_config.testing_http_address.port(),
                env,
            }
        })
        .collect();

    let executors = topology
        .executors()
        .iter()
        .enumerate()
        .map(|(index, executor)| {
            let mut env = BTreeMap::new();
            env.insert(
                "CFG_NETWORK_PORT".into(),
                executor.network_port().to_string(),
            );
            env.insert("CFG_DA_PORT".into(), executor.da_port.to_string());
            env.insert("CFG_BLEND_PORT".into(), executor.blend_port.to_string());
            env.insert(
                "CFG_API_PORT".into(),
                executor.general.api_config.address.port().to_string(),
            );
            env.insert(
                "CFG_TESTING_HTTP_PORT".into(),
                executor
                    .general
                    .api_config
                    .testing_http_address
                    .port()
                    .to_string(),
            );
            env.insert("CFG_HOST_KIND".into(), "executor".into());
            env.insert("CFG_HOST_IDENTIFIER".into(), format!("executor-{index}"));

            NodeValues {
                api_port: executor.general.api_config.address.port(),
                testing_http_port: executor.general.api_config.testing_http_address.port(),
                env,
            }
        })
        .collect();

    HelmValues {
        validators: NodeGroup {
            count: topology.validators().len(),
            nodes: validators,
        },
        executors: NodeGroup {
            count: topology.executors().len(),
            nodes: executors,
        },
    }
}




════════════════════════════════════════════════
FILE: testing-framework/runners/k8s/src/cleanup.rs
────────────────────────────────────────────────
use std::thread;

use k8s_openapi::api::core::v1::Namespace;
use kube::{Api, Client, api::DeleteParams};
use testing_framework_core::scenario::CleanupGuard;
use tokio::{
    process::Command,
    time::{Duration, sleep},
};
use tracing::warn;

use crate::helm::uninstall_release;

pub struct RunnerCleanup {
    client: Client,
    namespace: String,
    release: String,
    preserve: bool,
}

impl RunnerCleanup {
    pub fn new(client: Client, namespace: String, release: String, preserve: bool) -> Self {
        debug_assert!(
            !namespace.is_empty() && !release.is_empty(),
            "k8s cleanup requires namespace and release"
        );
        Self {
            client,
            namespace,
            release,
            preserve,
        }
    }

    async fn cleanup_async(&self) {
        if self.preserve {
            println!(
                "[k8s-runner] preserving Helm release `{}` in namespace `{}`",
                self.release, self.namespace
            );

            return;
        }

        if let Err(err) = uninstall_release(&self.release, &self.namespace).await {
            println!("[k8s-runner] helm uninstall {} failed: {err}", self.release);
        }

        println!(
            "[k8s-runner] deleting namespace `{}` via k8s API",
            self.namespace
        );
        delete_namespace(&self.client, &self.namespace).await;
        println!(
            "[k8s-runner] delete request for namespace `{}` finished",
            self.namespace
        );
    }

    fn blocking_cleanup_success(&self) -> bool {
        match tokio::runtime::Runtime::new() {
            Ok(rt) => match rt.block_on(async {
                tokio::time::timeout(Duration::from_secs(120), self.cleanup_async()).await
            }) {
                Ok(()) => true,
                Err(err) => {
                    warn!(
                        "[k8s-runner] cleanup timed out after 120s: {err}; falling back to background thread"
                    );
                    false
                }
            },
            Err(err) => {
                warn!(
                    "[k8s-runner] unable to create cleanup runtime: {err}; falling back to background thread"
                );
                false
            }
        }
    }

    fn spawn_cleanup_thread(self: Box<Self>) {
        match thread::Builder::new()
            .name("k8s-runner-cleanup".into())
            .spawn(move || match tokio::runtime::Runtime::new() {
                Ok(rt) => {
                    if let Err(err) = rt.block_on(async {
                        tokio::time::timeout(Duration::from_secs(120), self.cleanup_async()).await
                    }) {
                        warn!("[k8s-runner] background cleanup timed out: {err}");
                    }
                }
                Err(err) => warn!("[k8s-runner] unable to create cleanup runtime: {err}"),
            }) {
            Ok(handle) => {
                if let Err(err) = handle.join() {
                    warn!("[k8s-runner] cleanup thread panicked: {err:?}");
                }
            }
            Err(err) => warn!("[k8s-runner] failed to spawn cleanup thread: {err}"),
        }
    }
}

async fn delete_namespace(client: &Client, namespace: &str) {
    let namespaces: Api<Namespace> = Api::all(client.clone());

    if delete_namespace_via_api(&namespaces, namespace).await {
        wait_for_namespace_termination(&namespaces, namespace).await;
        return;
    }

    if delete_namespace_via_cli(namespace).await {
        wait_for_namespace_termination(&namespaces, namespace).await;
    } else {
        warn!("[k8s-runner] unable to delete namespace `{namespace}` using kubectl fallback");
    }
}

async fn delete_namespace_via_api(namespaces: &Api<Namespace>, namespace: &str) -> bool {
    println!("[k8s-runner] invoking kubernetes API to delete namespace `{namespace}`");
    match tokio::time::timeout(
        Duration::from_secs(10),
        namespaces.delete(namespace, &DeleteParams::default()),
    )
    .await
    {
        Ok(Ok(_)) => {
            println!(
                "[k8s-runner] delete request accepted for namespace `{namespace}`; waiting for termination"
            );
            true
        }
        Ok(Err(err)) => {
            println!("[k8s-runner] failed to delete namespace `{namespace}` via API: {err}");
            warn!("[k8s-runner] api delete failed for namespace {namespace}: {err}");
            false
        }
        Err(_) => {
            println!(
                "[k8s-runner] kubernetes API timed out deleting namespace `{namespace}`; falling back to kubectl"
            );
            false
        }
    }
}

async fn delete_namespace_via_cli(namespace: &str) -> bool {
    println!("[k8s-runner] invoking `kubectl delete namespace {namespace}` fallback");
    let output = Command::new("kubectl")
        .arg("delete")
        .arg("namespace")
        .arg(namespace)
        .arg("--wait=true")
        .output()
        .await;

    match output {
        Ok(result) if result.status.success() => {
            println!("[k8s-runner] `kubectl delete namespace {namespace}` completed successfully");
            true
        }
        Ok(result) => {
            println!(
                "[k8s-runner] `kubectl delete namespace {namespace}` failed: {}\n{}",
                String::from_utf8_lossy(&result.stderr),
                String::from_utf8_lossy(&result.stdout)
            );
            false
        }
        Err(err) => {
            println!("[k8s-runner] failed to spawn kubectl for namespace `{namespace}`: {err}");
            false
        }
    }
}

async fn wait_for_namespace_termination(namespaces: &Api<Namespace>, namespace: &str) {
    for attempt in 0..60 {
        match namespaces.get_opt(namespace).await {
            Ok(Some(ns)) => {
                if attempt == 0 {
                    println!(
                        "[k8s-runner] waiting for namespace `{}` to terminate (phase={:?})",
                        namespace,
                        ns.status
                            .as_ref()
                            .and_then(|status| status.phase.clone())
                            .unwrap_or_else(|| "Unknown".into())
                    );
                }
            }
            Ok(None) => {
                println!("[k8s-runner] namespace `{namespace}` deleted");
                return;
            }
            Err(err) => {
                warn!("[k8s-runner] namespace `{namespace}` poll failed: {err}");
                break;
            }
        }

        sleep(Duration::from_secs(1)).await;
    }

    warn!(
        "[k8s-runner] namespace `{}` still present after waiting for deletion",
        namespace
    );
}

impl CleanupGuard for RunnerCleanup {
    fn cleanup(self: Box<Self>) {
        if tokio::runtime::Handle::try_current().is_err() && self.blocking_cleanup_success() {
            return;
        }
        self.spawn_cleanup_thread();
    }
}




════════════════════════════════════════════════
FILE: testing-framework/runners/k8s/src/helm.rs
────────────────────────────────────────────────
use std::{io, process::Stdio};

use thiserror::Error;
use tokio::process::Command;

use crate::assets::{CFGSYNC_PORT, RunnerAssets, workspace_root};

#[derive(Debug, Error)]
pub enum HelmError {
    #[error("failed to spawn {command}: {source}")]
    Spawn {
        command: String,
        #[source]
        source: io::Error,
    },
    #[error("{command} exited with status {status:?}\nstderr:\n{stderr}\nstdout:\n{stdout}")]
    Failed {
        command: String,
        status: Option<i32>,
        stdout: String,
        stderr: String,
    },
}

pub async fn install_release(
    assets: &RunnerAssets,
    release: &str,
    namespace: &str,
    validators: usize,
    executors: usize,
) -> Result<(), HelmError> {
    let host_path_type = if assets.kzg_path.is_dir() {
        "Directory"
    } else {
        "File"
    };

    let mut cmd = Command::new("helm");
    cmd.arg("install")
        .arg(release)
        .arg(&assets.chart_path)
        .arg("--namespace")
        .arg(namespace)
        .arg("--create-namespace")
        .arg("--wait")
        .arg("--timeout")
        .arg("5m")
        .arg("--set")
        .arg(format!("image={}", assets.image))
        .arg("--set")
        .arg(format!("validators.count={validators}"))
        .arg("--set")
        .arg(format!("executors.count={executors}"))
        .arg("--set")
        .arg(format!("cfgsync.port={CFGSYNC_PORT}"))
        .arg("--set")
        .arg(format!("kzg.hostPath={}", assets.kzg_path.display()))
        .arg("--set")
        .arg(format!("kzg.hostPathType={host_path_type}"))
        .arg("-f")
        .arg(&assets.values_file)
        .arg("--set-file")
        .arg(format!("cfgsync.config={}", assets.cfgsync_file.display()))
        .arg("--set-file")
        .arg(format!(
            "scripts.runCfgsyncSh={}",
            assets.run_cfgsync_script.display()
        ))
        .arg("--set-file")
        .arg(format!(
            "scripts.runNomosNodeSh={}",
            assets.run_nomos_node_script.display()
        ))
        .arg("--set-file")
        .arg(format!(
            "scripts.runNomosExecutorSh={}",
            assets.run_nomos_executor_script.display()
        ))
        .stdout(Stdio::piped())
        .stderr(Stdio::piped());

    if let Ok(root) = workspace_root() {
        cmd.current_dir(root);
    }

    let command = format!("helm install {release}");
    let output = run_helm_command(cmd, &command).await?;

    if std::env::var("K8S_RUNNER_DEBUG").is_ok() {
        println!(
            "[k8s-runner] {command} stdout:\n{}",
            String::from_utf8_lossy(&output.stdout)
        );
        println!(
            "[k8s-runner] {command} stderr:\n{}",
            String::from_utf8_lossy(&output.stderr)
        );
    }

    Ok(())
}

pub async fn uninstall_release(release: &str, namespace: &str) -> Result<(), HelmError> {
    let mut cmd = Command::new("helm");
    cmd.arg("uninstall")
        .arg(release)
        .arg("--namespace")
        .arg(namespace)
        .stdout(Stdio::piped())
        .stderr(Stdio::piped());

    println!("[k8s-runner] issuing `helm uninstall {release}` in namespace `{namespace}`");

    run_helm_command(cmd, &format!("helm uninstall {release}")).await?;
    println!(
        "[k8s-runner] helm uninstall {release} completed successfully (namespace `{namespace}`)"
    );
    Ok(())
}

async fn run_helm_command(
    mut cmd: Command,
    command: &str,
) -> Result<std::process::Output, HelmError> {
    cmd.stdout(Stdio::piped()).stderr(Stdio::piped());
    let output = cmd.output().await.map_err(|source| HelmError::Spawn {
        command: command.to_owned(),
        source,
    })?;

    if output.status.success() {
        Ok(output)
    } else {
        Err(HelmError::Failed {
            command: command.to_owned(),
            status: output.status.code(),
            stdout: String::from_utf8_lossy(&output.stdout).into_owned(),
            stderr: String::from_utf8_lossy(&output.stderr).into_owned(),
        })
    }
}




════════════════════════════════════════════════
FILE: testing-framework/runners/k8s/src/host.rs
────────────────────────────────────────────────
use std::env;

const NODE_HOST_ENV: &str = "K8S_RUNNER_NODE_HOST";
const KUBE_SERVICE_HOST_ENV: &str = "KUBERNETES_SERVICE_HOST";

/// Returns the hostname or IP used to reach `NodePorts` exposed by the cluster.
/// Prefers `K8S_RUNNER_NODE_HOST`, then the standard `KUBERNETES_SERVICE_HOST`
/// (e.g. `kubernetes.docker.internal` on Docker Desktop), and finally falls
/// back to `127.0.0.1`.
pub fn node_host() -> String {
    if let Ok(host) = env::var(NODE_HOST_ENV) {
        return host;
    }
    if let Ok(host) = env::var(KUBE_SERVICE_HOST_ENV)
        && !host.is_empty()
    {
        return host;
    }
    "127.0.0.1".to_owned()
}




════════════════════════════════════════════════
FILE: testing-framework/runners/k8s/src/lib.rs
────────────────────────────────────────────────
mod assets;
mod cleanup;
mod helm;
mod host;
mod logs;
mod runner;
mod wait;

pub use runner::{K8sRunner, K8sRunnerError};




════════════════════════════════════════════════
FILE: testing-framework/runners/k8s/src/logs.rs
────────────────────────────────────────────────
use k8s_openapi::api::core::v1::Pod;
use kube::{
    Api, Client,
    api::{ListParams, LogParams},
};
use tracing::{info, warn};

pub async fn dump_namespace_logs(client: &Client, namespace: &str) {
    let pod_names = match list_pod_names(client, namespace).await {
        Ok(names) => names,
        Err(err) => {
            warn!("[k8s-runner] failed to list pods in namespace {namespace}: {err}");
            return;
        }
    };

    for pod_name in pod_names {
        stream_pod_logs(client, namespace, &pod_name).await;
    }
}

async fn list_pod_names(client: &Client, namespace: &str) -> Result<Vec<String>, kube::Error> {
    let list = Api::<Pod>::namespaced(client.clone(), namespace)
        .list(&ListParams::default())
        .await?;
    Ok(list
        .into_iter()
        .filter_map(|pod| pod.metadata.name)
        .collect())
}

async fn stream_pod_logs(client: &Client, namespace: &str, pod_name: &str) {
    let pods: Api<Pod> = Api::namespaced(client.clone(), namespace);
    let params = LogParams {
        follow: false,
        tail_lines: Some(500),
        ..Default::default()
    };

    match pods.logs(pod_name, &params).await {
        Ok(log) => info!("[k8s-runner] pod {pod_name} logs:\n{log}"),
        Err(err) => warn!("[k8s-runner] failed to fetch logs for pod {pod_name}: {err}"),
    }
}




════════════════════════════════════════════════
FILE: testing-framework/runners/k8s/src/runner.rs
────────────────────────────────────────────────
use std::env;

use anyhow::Error;
use async_trait::async_trait;
use kube::Client;
use reqwest::Url;
use testing_framework_core::{
    nodes::ApiClient,
    scenario::{
        BlockFeed, BlockFeedTask, CleanupGuard, Deployer, Metrics, MetricsError, NodeClients,
        RunContext, Runner, Scenario, http_probe::NodeRole, spawn_block_feed,
    },
    topology::{GeneratedTopology, ReadinessError},
};
use tracing::{error, info};
use url::ParseError;
use uuid::Uuid;

use crate::{
    assets::{AssetsError, RunnerAssets, prepare_assets},
    cleanup::RunnerCleanup,
    helm::{HelmError, install_release},
    host::node_host,
    logs::dump_namespace_logs,
    wait::{ClusterPorts, ClusterReady, ClusterWaitError, NodeConfigPorts, wait_for_cluster_ready},
};

pub struct K8sRunner {
    readiness_checks: bool,
}

impl Default for K8sRunner {
    fn default() -> Self {
        Self::new()
    }
}

impl K8sRunner {
    #[must_use]
    pub const fn new() -> Self {
        Self {
            readiness_checks: true,
        }
    }

    #[must_use]
    pub const fn with_readiness(mut self, enabled: bool) -> Self {
        self.readiness_checks = enabled;
        self
    }
}

#[derive(Default)]
struct PortSpecs {
    validators: Vec<NodeConfigPorts>,
    executors: Vec<NodeConfigPorts>,
}

struct ClusterEnvironment {
    client: Client,
    namespace: String,
    release: String,
    cleanup: Option<RunnerCleanup>,
    validator_api_ports: Vec<u16>,
    validator_testing_ports: Vec<u16>,
    executor_api_ports: Vec<u16>,
    executor_testing_ports: Vec<u16>,
    prometheus_port: u16,
    port_forwards: Vec<std::process::Child>,
}

impl ClusterEnvironment {
    fn new(
        client: Client,
        namespace: String,
        release: String,
        cleanup: RunnerCleanup,
        ports: &ClusterPorts,
        port_forwards: Vec<std::process::Child>,
    ) -> Self {
        Self {
            client,
            namespace,
            release,
            cleanup: Some(cleanup),
            validator_api_ports: ports.validators.iter().map(|ports| ports.api).collect(),
            validator_testing_ports: ports.validators.iter().map(|ports| ports.testing).collect(),
            executor_api_ports: ports.executors.iter().map(|ports| ports.api).collect(),
            executor_testing_ports: ports.executors.iter().map(|ports| ports.testing).collect(),
            prometheus_port: ports.prometheus,
            port_forwards,
        }
    }

    async fn fail(&mut self, reason: &str) {
        error!(
            reason = reason,
            namespace = %self.namespace,
            release = %self.release,
            "k8s stack failure; collecting diagnostics"
        );
        dump_namespace_logs(&self.client, &self.namespace).await;
        kill_port_forwards(&mut self.port_forwards);
        if let Some(guard) = self.cleanup.take() {
            Box::new(guard).cleanup();
        }
    }

    fn into_cleanup(self) -> (RunnerCleanup, Vec<std::process::Child>) {
        (
            self.cleanup.expect("cleanup guard should be available"),
            self.port_forwards,
        )
    }
}

#[derive(Debug, thiserror::Error)]
pub enum NodeClientError {
    #[error("failed to build {endpoint} client URL for {role} port {port}: {source}")]
    Endpoint {
        role: NodeRole,
        endpoint: &'static str,
        port: u16,
        #[source]
        source: ParseError,
    },
}

#[derive(Debug, thiserror::Error)]
pub enum RemoteReadinessError {
    #[error("failed to build readiness URL for {role} port {port}: {source}")]
    Endpoint {
        role: NodeRole,
        port: u16,
        #[source]
        source: ParseError,
    },
    #[error("remote readiness probe failed: {source}")]
    Remote {
        #[source]
        source: ReadinessError,
    },
}

fn readiness_urls(ports: &[u16], role: NodeRole) -> Result<Vec<Url>, RemoteReadinessError> {
    ports
        .iter()
        .copied()
        .map(|port| readiness_url(role, port))
        .collect()
}

fn readiness_url(role: NodeRole, port: u16) -> Result<Url, RemoteReadinessError> {
    cluster_host_url(port).map_err(|source| RemoteReadinessError::Endpoint { role, port, source })
}

fn cluster_host_url(port: u16) -> Result<Url, ParseError> {
    Url::parse(&format!("http://{}:{port}/", node_host()))
}

fn metrics_handle_from_port(port: u16) -> Result<Metrics, MetricsError> {
    let url = cluster_host_url(port)
        .map_err(|err| MetricsError::new(format!("invalid prometheus url: {err}")))?;
    Metrics::from_prometheus(url)
}

async fn spawn_block_feed_with(
    node_clients: &NodeClients,
) -> Result<(BlockFeed, BlockFeedTask), K8sRunnerError> {
    let block_source_client = node_clients
        .any_client()
        .cloned()
        .ok_or(K8sRunnerError::BlockFeedMissing)?;

    spawn_block_feed(block_source_client)
        .await
        .map_err(|source| K8sRunnerError::BlockFeed { source })
}

#[derive(Debug, thiserror::Error)]
pub enum K8sRunnerError {
    #[error(
        "kubernetes runner requires at least one validator and one executor (validators={validators}, executors={executors})"
    )]
    UnsupportedTopology { validators: usize, executors: usize },
    #[error("failed to initialise kubernetes client: {source}")]
    ClientInit {
        #[source]
        source: kube::Error,
    },
    #[error(transparent)]
    Assets(#[from] AssetsError),
    #[error(transparent)]
    Helm(#[from] HelmError),
    #[error(transparent)]
    Cluster(#[from] Box<ClusterWaitError>),
    #[error(transparent)]
    Readiness(#[from] RemoteReadinessError),
    #[error(transparent)]
    NodeClients(#[from] NodeClientError),
    #[error(transparent)]
    Telemetry(#[from] MetricsError),
    #[error("k8s runner requires at least one node client to follow blocks")]
    BlockFeedMissing,
    #[error("failed to initialize block feed: {source}")]
    BlockFeed {
        #[source]
        source: Error,
    },
}

#[async_trait]
impl Deployer for K8sRunner {
    type Error = K8sRunnerError;

    async fn deploy(&self, scenario: &Scenario) -> Result<Runner, Self::Error> {
        let descriptors = scenario.topology().clone();
        ensure_supported_topology(&descriptors)?;

        let client = Client::try_default()
            .await
            .map_err(|source| K8sRunnerError::ClientInit { source })?;
        info!(
            validators = descriptors.validators().len(),
            executors = descriptors.executors().len(),
            "starting k8s deployment"
        );

        let port_specs = collect_port_specs(&descriptors);
        let mut cluster =
            Some(setup_cluster(&client, &port_specs, &descriptors, self.readiness_checks).await?);

        info!("building node clients");
        let node_clients = match build_node_clients(
            cluster
                .as_ref()
                .expect("cluster must be available while building clients"),
        ) {
            Ok(clients) => clients,
            Err(err) => {
                if let Some(env) = cluster.as_mut() {
                    env.fail("failed to construct node api clients").await;
                }
                return Err(err.into());
            }
        };

        let telemetry = match metrics_handle_from_port(
            cluster
                .as_ref()
                .expect("cluster must be available for telemetry")
                .prometheus_port,
        ) {
            Ok(handle) => handle,
            Err(err) => {
                if let Some(env) = cluster.as_mut() {
                    env.fail("failed to configure prometheus metrics handle")
                        .await;
                }
                return Err(err.into());
            }
        };
        let (block_feed, block_feed_guard) = match spawn_block_feed_with(&node_clients).await {
            Ok(pair) => pair,
            Err(err) => {
                if let Some(env) = cluster.as_mut() {
                    env.fail("failed to initialize block feed").await;
                }
                return Err(err);
            }
        };
        let (cleanup, port_forwards) = cluster
            .take()
            .expect("cluster should still be available")
            .into_cleanup();
        let cleanup_guard: Box<dyn CleanupGuard> = Box::new(K8sCleanupGuard::new(
            cleanup,
            block_feed_guard,
            port_forwards,
        ));
        let context = RunContext::new(
            descriptors,
            None,
            node_clients,
            scenario.duration(),
            telemetry,
            block_feed,
            None,
        );
        Ok(Runner::new(context, Some(cleanup_guard)))
    }
}

impl From<ClusterWaitError> for K8sRunnerError {
    fn from(value: ClusterWaitError) -> Self {
        Self::Cluster(Box::new(value))
    }
}

fn ensure_supported_topology(descriptors: &GeneratedTopology) -> Result<(), K8sRunnerError> {
    let validators = descriptors.validators().len();
    let executors = descriptors.executors().len();
    if validators == 0 || executors == 0 {
        return Err(K8sRunnerError::UnsupportedTopology {
            validators,
            executors,
        });
    }
    Ok(())
}

fn kill_port_forwards(handles: &mut Vec<std::process::Child>) {
    for handle in handles.iter_mut() {
        let _ = handle.kill();
        let _ = handle.wait();
    }
    handles.clear();
}

fn collect_port_specs(descriptors: &GeneratedTopology) -> PortSpecs {
    let validators = descriptors
        .validators()
        .iter()
        .map(|node| NodeConfigPorts {
            api: node.general.api_config.address.port(),
            testing: node.general.api_config.testing_http_address.port(),
        })
        .collect();
    let executors = descriptors
        .executors()
        .iter()
        .map(|node| NodeConfigPorts {
            api: node.general.api_config.address.port(),
            testing: node.general.api_config.testing_http_address.port(),
        })
        .collect();

    PortSpecs {
        validators,
        executors,
    }
}

fn build_node_clients(cluster: &ClusterEnvironment) -> Result<NodeClients, NodeClientError> {
    let validators = cluster
        .validator_api_ports
        .iter()
        .copied()
        .zip(cluster.validator_testing_ports.iter().copied())
        .map(|(api_port, testing_port)| {
            api_client_from_ports(NodeRole::Validator, api_port, testing_port)
        })
        .collect::<Result<Vec<_>, _>>()?;
    let executors = cluster
        .executor_api_ports
        .iter()
        .copied()
        .zip(cluster.executor_testing_ports.iter().copied())
        .map(|(api_port, testing_port)| {
            api_client_from_ports(NodeRole::Executor, api_port, testing_port)
        })
        .collect::<Result<Vec<_>, _>>()?;

    Ok(NodeClients::new(validators, executors))
}

fn api_client_from_ports(
    role: NodeRole,
    api_port: u16,
    testing_port: u16,
) -> Result<ApiClient, NodeClientError> {
    let base_endpoint = cluster_host_url(api_port).map_err(|source| NodeClientError::Endpoint {
        role,
        endpoint: "api",
        port: api_port,
        source,
    })?;
    let testing_endpoint =
        Some(
            cluster_host_url(testing_port).map_err(|source| NodeClientError::Endpoint {
                role,
                endpoint: "testing",
                port: testing_port,
                source,
            })?,
        );
    Ok(ApiClient::from_urls(base_endpoint, testing_endpoint))
}

async fn setup_cluster(
    client: &Client,
    specs: &PortSpecs,
    descriptors: &GeneratedTopology,
    readiness_checks: bool,
) -> Result<ClusterEnvironment, K8sRunnerError> {
    let assets = prepare_assets(descriptors)?;
    let validators = descriptors.validators().len();
    let executors = descriptors.executors().len();

    let (namespace, release) = cluster_identifiers();

    let mut cleanup_guard =
        Some(install_stack(client, &assets, &namespace, &release, validators, executors).await?);

    let cluster_ready =
        wait_for_ports_or_cleanup(client, &namespace, &release, specs, &mut cleanup_guard).await?;

    info!(
        prometheus_port = cluster_ready.ports.prometheus,
        "discovered prometheus endpoint"
    );

    let environment = ClusterEnvironment::new(
        client.clone(),
        namespace,
        release,
        cleanup_guard
            .take()
            .expect("cleanup guard must exist after successful cluster startup"),
        &cluster_ready.ports,
        cluster_ready.port_forwards,
    );

    if readiness_checks {
        ensure_cluster_readiness(descriptors, &environment).await?;
    }

    Ok(environment)
}

fn cluster_identifiers() -> (String, String) {
    let run_id = Uuid::new_v4().simple().to_string();
    let namespace = format!("nomos-k8s-{run_id}");
    (namespace.clone(), namespace)
}

async fn install_stack(
    client: &Client,
    assets: &RunnerAssets,
    namespace: &str,
    release: &str,
    validators: usize,
    executors: usize,
) -> Result<RunnerCleanup, K8sRunnerError> {
    info!(
        release = %release,
        namespace = %namespace,
        "installing helm release"
    );
    install_release(assets, release, namespace, validators, executors).await?;
    info!(release = %release, "helm install succeeded");

    let preserve = env::var("K8S_RUNNER_PRESERVE").is_ok();
    Ok(RunnerCleanup::new(
        client.clone(),
        namespace.to_owned(),
        release.to_owned(),
        preserve,
    ))
}

async fn wait_for_ports_or_cleanup(
    client: &Client,
    namespace: &str,
    release: &str,
    specs: &PortSpecs,
    cleanup_guard: &mut Option<RunnerCleanup>,
) -> Result<ClusterReady, K8sRunnerError> {
    match wait_for_cluster_ready(
        client,
        namespace,
        release,
        &specs.validators,
        &specs.executors,
    )
    .await
    {
        Ok(ports) => Ok(ports),
        Err(err) => {
            cleanup_pending(client, namespace, cleanup_guard).await;
            Err(err.into())
        }
    }
}

async fn cleanup_pending(client: &Client, namespace: &str, guard: &mut Option<RunnerCleanup>) {
    dump_namespace_logs(client, namespace).await;
    if let Some(guard) = guard.take() {
        Box::new(guard).cleanup();
    }
}

async fn ensure_cluster_readiness(
    descriptors: &GeneratedTopology,
    cluster: &ClusterEnvironment,
) -> Result<(), RemoteReadinessError> {
    let validator_urls = readiness_urls(&cluster.validator_api_ports, NodeRole::Validator)?;
    let executor_urls = readiness_urls(&cluster.executor_api_ports, NodeRole::Executor)?;
    let validator_membership_urls =
        readiness_urls(&cluster.validator_testing_ports, NodeRole::Validator)?;
    let executor_membership_urls =
        readiness_urls(&cluster.executor_testing_ports, NodeRole::Executor)?;

    descriptors
        .wait_remote_readiness(
            &validator_urls,
            &executor_urls,
            Some(&validator_membership_urls),
            Some(&executor_membership_urls),
        )
        .await
        .map_err(|source| RemoteReadinessError::Remote { source })
}

struct K8sCleanupGuard {
    cleanup: RunnerCleanup,
    block_feed: Option<BlockFeedTask>,
    port_forwards: Vec<std::process::Child>,
}

impl K8sCleanupGuard {
    const fn new(
        cleanup: RunnerCleanup,
        block_feed: BlockFeedTask,
        port_forwards: Vec<std::process::Child>,
    ) -> Self {
        Self {
            cleanup,
            block_feed: Some(block_feed),
            port_forwards,
        }
    }
}

impl CleanupGuard for K8sCleanupGuard {
    fn cleanup(mut self: Box<Self>) {
        if let Some(block_feed) = self.block_feed.take() {
            CleanupGuard::cleanup(Box::new(block_feed));
        }
        kill_port_forwards(&mut self.port_forwards);
        CleanupGuard::cleanup(Box::new(self.cleanup));
    }
}




════════════════════════════════════════════════
FILE: testing-framework/runners/k8s/src/wait.rs
────────────────────────────────────────────────
use std::{
    net::{Ipv4Addr, TcpListener, TcpStream},
    process::{Command as StdCommand, Stdio},
    thread,
    time::Duration,
};

use k8s_openapi::api::{apps::v1::Deployment, core::v1::Service};
use kube::{Api, Client, Error as KubeError};
use testing_framework_core::scenario::http_probe::{self, HttpReadinessError, NodeRole};
use thiserror::Error;
use tokio::time::sleep;

use crate::host::node_host;

const DEPLOYMENT_TIMEOUT: Duration = Duration::from_secs(180);
const NODE_HTTP_TIMEOUT: Duration = Duration::from_secs(240);
const NODE_HTTP_PROBE_TIMEOUT: Duration = Duration::from_secs(30);
const HTTP_POLL_INTERVAL: Duration = Duration::from_secs(1);
const PROMETHEUS_HTTP_PORT: u16 = 9090;
const PROMETHEUS_HTTP_TIMEOUT: Duration = Duration::from_secs(240);
const PROMETHEUS_HTTP_PROBE_TIMEOUT: Duration = Duration::from_secs(30);
const PROMETHEUS_SERVICE_NAME: &str = "prometheus";

#[derive(Clone, Copy)]
pub struct NodeConfigPorts {
    pub api: u16,
    pub testing: u16,
}

#[derive(Clone, Copy)]
pub struct NodePortAllocation {
    pub api: u16,
    pub testing: u16,
}

pub struct ClusterPorts {
    pub validators: Vec<NodePortAllocation>,
    pub executors: Vec<NodePortAllocation>,
    pub prometheus: u16,
}

pub struct ClusterReady {
    pub ports: ClusterPorts,
    pub port_forwards: Vec<std::process::Child>,
}

#[derive(Debug, Error)]
pub enum ClusterWaitError {
    #[error("deployment {name} in namespace {namespace} did not become ready within {timeout:?}")]
    DeploymentTimeout {
        name: String,
        namespace: String,
        timeout: Duration,
    },
    #[error("failed to fetch deployment {name}: {source}")]
    DeploymentFetch {
        name: String,
        #[source]
        source: KubeError,
    },
    #[error("failed to fetch service {service}: {source}")]
    ServiceFetch {
        service: String,
        #[source]
        source: KubeError,
    },
    #[error("service {service} did not allocate a node port for {port}")]
    NodePortUnavailable { service: String, port: u16 },
    #[error("cluster must have at least one validator")]
    MissingValidator,
    #[error("timeout waiting for {role} HTTP endpoint on port {port} after {timeout:?}")]
    NodeHttpTimeout {
        role: NodeRole,
        port: u16,
        timeout: Duration,
    },
    #[error("timeout waiting for prometheus readiness on NodePort {port}")]
    PrometheusTimeout { port: u16 },
    #[error("failed to start port-forward for service {service} port {port}: {source}")]
    PortForward {
        service: String,
        port: u16,
        #[source]
        source: anyhow::Error,
    },
}

pub async fn wait_for_deployment_ready(
    client: &Client,
    namespace: &str,
    name: &str,
    timeout: Duration,
) -> Result<(), ClusterWaitError> {
    let mut elapsed = Duration::ZERO;
    let interval = Duration::from_secs(2);

    while elapsed <= timeout {
        match Api::<Deployment>::namespaced(client.clone(), namespace)
            .get(name)
            .await
        {
            Ok(deployment) => {
                let desired = deployment
                    .spec
                    .as_ref()
                    .and_then(|spec| spec.replicas)
                    .unwrap_or(1);
                let ready = deployment
                    .status
                    .as_ref()
                    .and_then(|status| status.ready_replicas)
                    .unwrap_or(0);
                if ready >= desired {
                    return Ok(());
                }
            }
            Err(err) => {
                return Err(ClusterWaitError::DeploymentFetch {
                    name: name.to_owned(),
                    source: err,
                });
            }
        }

        sleep(interval).await;
        elapsed += interval;
    }

    Err(ClusterWaitError::DeploymentTimeout {
        name: name.to_owned(),
        namespace: namespace.to_owned(),
        timeout,
    })
}

pub async fn find_node_port(
    client: &Client,
    namespace: &str,
    service_name: &str,
    service_port: u16,
) -> Result<u16, ClusterWaitError> {
    let interval = Duration::from_secs(1);
    for _ in 0..120 {
        match Api::<Service>::namespaced(client.clone(), namespace)
            .get(service_name)
            .await
        {
            Ok(service) => {
                if let Some(spec) = service.spec.clone()
                    && let Some(ports) = spec.ports
                {
                    for port in ports {
                        if port.port == i32::from(service_port)
                            && let Some(node_port) = port.node_port
                        {
                            return Ok(node_port as u16);
                        }
                    }
                }
            }
            Err(err) => {
                return Err(ClusterWaitError::ServiceFetch {
                    service: service_name.to_owned(),
                    source: err,
                });
            }
        }
        sleep(interval).await;
    }

    Err(ClusterWaitError::NodePortUnavailable {
        service: service_name.to_owned(),
        port: service_port,
    })
}

pub async fn wait_for_cluster_ready(
    client: &Client,
    namespace: &str,
    release: &str,
    validator_ports: &[NodeConfigPorts],
    executor_ports: &[NodeConfigPorts],
) -> Result<ClusterReady, ClusterWaitError> {
    if validator_ports.is_empty() {
        return Err(ClusterWaitError::MissingValidator);
    }

    let mut validator_allocations = Vec::with_capacity(validator_ports.len());

    for (index, ports) in validator_ports.iter().enumerate() {
        let name = format!("{release}-validator-{index}");
        wait_for_deployment_ready(client, namespace, &name, DEPLOYMENT_TIMEOUT).await?;
        let api_port = find_node_port(client, namespace, &name, ports.api).await?;
        let testing_port = find_node_port(client, namespace, &name, ports.testing).await?;
        validator_allocations.push(NodePortAllocation {
            api: api_port,
            testing: testing_port,
        });
    }

    let mut port_forwards = Vec::new();

    let validator_api_ports: Vec<u16> = validator_allocations
        .iter()
        .map(|ports| ports.api)
        .collect();
    if wait_for_node_http_nodeport(
        &validator_api_ports,
        NodeRole::Validator,
        NODE_HTTP_PROBE_TIMEOUT,
    )
    .await
    .is_err()
    {
        // Fall back to port-forwarding when NodePorts are unreachable from the host.
        validator_allocations.clear();
        port_forwards = port_forward_group(
            namespace,
            release,
            "validator",
            validator_ports,
            &mut validator_allocations,
        )?;
        let validator_api_ports: Vec<u16> = validator_allocations
            .iter()
            .map(|ports| ports.api)
            .collect();
        if let Err(err) =
            wait_for_node_http_port_forward(&validator_api_ports, NodeRole::Validator).await
        {
            kill_port_forwards(&mut port_forwards);
            return Err(err);
        }
    }

    let mut executor_allocations = Vec::with_capacity(executor_ports.len());
    for (index, ports) in executor_ports.iter().enumerate() {
        let name = format!("{release}-executor-{index}");
        wait_for_deployment_ready(client, namespace, &name, DEPLOYMENT_TIMEOUT).await?;
        let api_port = find_node_port(client, namespace, &name, ports.api).await?;
        let testing_port = find_node_port(client, namespace, &name, ports.testing).await?;
        executor_allocations.push(NodePortAllocation {
            api: api_port,
            testing: testing_port,
        });
    }

    let executor_api_ports: Vec<u16> = executor_allocations.iter().map(|ports| ports.api).collect();
    if !executor_allocations.is_empty()
        && wait_for_node_http_nodeport(
            &executor_api_ports,
            NodeRole::Executor,
            NODE_HTTP_PROBE_TIMEOUT,
        )
        .await
        .is_err()
    {
        executor_allocations.clear();
        match port_forward_group(
            namespace,
            release,
            "executor",
            executor_ports,
            &mut executor_allocations,
        ) {
            Ok(forwards) => port_forwards.extend(forwards),
            Err(err) => {
                kill_port_forwards(&mut port_forwards);
                return Err(err);
            }
        }
        let executor_api_ports: Vec<u16> =
            executor_allocations.iter().map(|ports| ports.api).collect();
        if let Err(err) =
            wait_for_node_http_port_forward(&executor_api_ports, NodeRole::Executor).await
        {
            kill_port_forwards(&mut port_forwards);
            return Err(err);
        }
    }

    let mut prometheus_port = find_node_port(
        client,
        namespace,
        PROMETHEUS_SERVICE_NAME,
        PROMETHEUS_HTTP_PORT,
    )
    .await?;
    if wait_for_prometheus_http_nodeport(prometheus_port, PROMETHEUS_HTTP_PROBE_TIMEOUT)
        .await
        .is_err()
    {
        let (local_port, forward) =
            port_forward_service(namespace, PROMETHEUS_SERVICE_NAME, PROMETHEUS_HTTP_PORT)
                .map_err(|err| {
                    kill_port_forwards(&mut port_forwards);
                    err
                })?;
        prometheus_port = local_port;
        port_forwards.push(forward);
        if let Err(err) =
            wait_for_prometheus_http_port_forward(prometheus_port, PROMETHEUS_HTTP_TIMEOUT).await
        {
            kill_port_forwards(&mut port_forwards);
            return Err(err);
        }
    }

    Ok(ClusterReady {
        ports: ClusterPorts {
            validators: validator_allocations,
            executors: executor_allocations,
            prometheus: prometheus_port,
        },
        port_forwards,
    })
}

async fn wait_for_node_http_nodeport(
    ports: &[u16],
    role: NodeRole,
    timeout: Duration,
) -> Result<(), ClusterWaitError> {
    let host = node_host();
    wait_for_node_http_on_host(ports, role, &host, timeout).await
}

async fn wait_for_node_http_port_forward(
    ports: &[u16],
    role: NodeRole,
) -> Result<(), ClusterWaitError> {
    wait_for_node_http_on_host(ports, role, "127.0.0.1", NODE_HTTP_TIMEOUT).await
}

async fn wait_for_node_http_on_host(
    ports: &[u16],
    role: NodeRole,
    host: &str,
    timeout: Duration,
) -> Result<(), ClusterWaitError> {
    http_probe::wait_for_http_ports_with_host(ports, role, host, timeout, HTTP_POLL_INTERVAL)
        .await
        .map_err(map_http_error)
}

const fn map_http_error(error: HttpReadinessError) -> ClusterWaitError {
    ClusterWaitError::NodeHttpTimeout {
        role: error.role(),
        port: error.port(),
        timeout: error.timeout(),
    }
}

pub async fn wait_for_prometheus_http_nodeport(
    port: u16,
    timeout: Duration,
) -> Result<(), ClusterWaitError> {
    let host = node_host();
    wait_for_prometheus_http(&host, port, timeout).await
}

pub async fn wait_for_prometheus_http_port_forward(
    port: u16,
    timeout: Duration,
) -> Result<(), ClusterWaitError> {
    wait_for_prometheus_http("127.0.0.1", port, timeout).await
}

pub async fn wait_for_prometheus_http(
    host: &str,
    port: u16,
    timeout: Duration,
) -> Result<(), ClusterWaitError> {
    let client = reqwest::Client::new();
    let url = format!("http://{host}:{port}/-/ready");

    for _ in 0..timeout.as_secs() {
        if let Ok(resp) = client.get(&url).send().await
            && resp.status().is_success()
        {
            return Ok(());
        }
        sleep(Duration::from_secs(1)).await;
    }

    Err(ClusterWaitError::PrometheusTimeout { port })
}

fn port_forward_group(
    namespace: &str,
    release: &str,
    kind: &str,
    ports: &[NodeConfigPorts],
    allocations: &mut Vec<NodePortAllocation>,
) -> Result<Vec<std::process::Child>, ClusterWaitError> {
    let mut forwards = Vec::new();
    for (index, ports) in ports.iter().enumerate() {
        let service = format!("{release}-{kind}-{index}");
        let (api_port, api_forward) = match port_forward_service(namespace, &service, ports.api) {
            Ok(forward) => forward,
            Err(err) => {
                kill_port_forwards(&mut forwards);
                return Err(err);
            }
        };
        let (testing_port, testing_forward) =
            match port_forward_service(namespace, &service, ports.testing) {
                Ok(forward) => forward,
                Err(err) => {
                    kill_port_forwards(&mut forwards);
                    return Err(err);
                }
            };
        allocations.push(NodePortAllocation {
            api: api_port,
            testing: testing_port,
        });
        forwards.push(api_forward);
        forwards.push(testing_forward);
    }
    Ok(forwards)
}

fn port_forward_service(
    namespace: &str,
    service: &str,
    remote_port: u16,
) -> Result<(u16, std::process::Child), ClusterWaitError> {
    let local_port = allocate_local_port().map_err(|source| ClusterWaitError::PortForward {
        service: service.to_owned(),
        port: remote_port,
        source,
    })?;

    let mut child = StdCommand::new("kubectl")
        .arg("port-forward")
        .arg("-n")
        .arg(namespace)
        .arg(format!("svc/{service}"))
        .arg(format!("{local_port}:{remote_port}"))
        .stdout(Stdio::null())
        .stderr(Stdio::null())
        .spawn()
        .map_err(|source| ClusterWaitError::PortForward {
            service: service.to_owned(),
            port: remote_port,
            source: source.into(),
        })?;

    for _ in 0..20 {
        if let Ok(Some(status)) = child.try_wait() {
            return Err(ClusterWaitError::PortForward {
                service: service.to_owned(),
                port: remote_port,
                source: anyhow::anyhow!("kubectl exited with {status}"),
            });
        }
        if TcpStream::connect((Ipv4Addr::LOCALHOST, local_port)).is_ok() {
            return Ok((local_port, child));
        }
        thread::sleep(Duration::from_millis(250));
    }

    let _ = child.kill();
    Err(ClusterWaitError::PortForward {
        service: service.to_owned(),
        port: remote_port,
        source: anyhow::anyhow!("port-forward did not become ready"),
    })
}

fn allocate_local_port() -> anyhow::Result<u16> {
    let listener = TcpListener::bind((Ipv4Addr::LOCALHOST, 0))?;
    let port = listener.local_addr()?.port();
    drop(listener);
    Ok(port)
}

fn kill_port_forwards(handles: &mut Vec<std::process::Child>) {
    for handle in handles.iter_mut() {
        let _ = handle.kill();
        let _ = handle.wait();
    }
    handles.clear();
}




════════════════════════════════════════════════
FILE: testing-framework/runners/local/src/lib.rs
────────────────────────────────────────────────
mod runner;

pub use runner::{LocalDeployer, LocalDeployerError};




════════════════════════════════════════════════
FILE: testing-framework/runners/local/src/runner.rs
────────────────────────────────────────────────
use async_trait::async_trait;
use testing_framework_core::{
    scenario::{
        BlockFeed, BlockFeedTask, Deployer, DynError, Metrics, NodeClients, RunContext, Runner,
        Scenario, ScenarioError, spawn_block_feed,
    },
    topology::{ReadinessError, Topology},
};
use thiserror::Error;

/// Spawns validators and executors as local processes, reusing the existing
/// integration harness.
#[derive(Clone)]
pub struct LocalDeployer {
    membership_check: bool,
}

#[derive(Debug, Error)]
pub enum LocalDeployerError {
    #[error("readiness probe failed: {source}")]
    ReadinessFailed {
        #[source]
        source: ReadinessError,
    },
    #[error("workload failed: {source}")]
    WorkloadFailed {
        #[source]
        source: DynError,
    },
    #[error("expectations failed: {source}")]
    ExpectationsFailed {
        #[source]
        source: DynError,
    },
}

impl From<ScenarioError> for LocalDeployerError {
    fn from(value: ScenarioError) -> Self {
        match value {
            ScenarioError::Workload(source) => Self::WorkloadFailed { source },
            ScenarioError::ExpectationCapture(source) | ScenarioError::Expectations(source) => {
                Self::ExpectationsFailed { source }
            }
        }
    }
}

#[async_trait]
impl Deployer<()> for LocalDeployer {
    type Error = LocalDeployerError;

    async fn deploy(&self, scenario: &Scenario<()>) -> Result<Runner, Self::Error> {
        let topology = Self::prepare_topology(scenario, self.membership_check).await?;
        let node_clients = NodeClients::from_topology(scenario.topology(), &topology);

        let (block_feed, block_feed_guard) = spawn_block_feed_with(&node_clients).await?;

        let context = RunContext::new(
            scenario.topology().clone(),
            Some(topology),
            node_clients,
            scenario.duration(),
            Metrics::empty(),
            block_feed,
            None,
        );

        Ok(Runner::new(context, Some(Box::new(block_feed_guard))))
    }
}

impl LocalDeployer {
    #[must_use]
    pub fn new() -> Self {
        Self::default()
    }

    #[must_use]
    pub const fn with_membership_check(mut self, enabled: bool) -> Self {
        self.membership_check = enabled;
        self
    }

    async fn prepare_topology(
        scenario: &Scenario<()>,
        membership_check: bool,
    ) -> Result<Topology, LocalDeployerError> {
        let descriptors = scenario.topology();
        let topology = descriptors.clone().spawn_local().await;

        let skip_membership = !membership_check;
        if let Err(source) = wait_for_readiness(&topology, skip_membership).await {
            return Err(LocalDeployerError::ReadinessFailed { source });
        }

        Ok(topology)
    }
}

impl Default for LocalDeployer {
    fn default() -> Self {
        Self {
            membership_check: true,
        }
    }
}

async fn wait_for_readiness(
    topology: &Topology,
    skip_membership: bool,
) -> Result<(), ReadinessError> {
    topology.wait_network_ready().await?;
    if !skip_membership {
        topology.wait_membership_ready().await?;
    }
    topology.wait_da_balancer_ready().await
}

async fn spawn_block_feed_with(
    node_clients: &NodeClients,
) -> Result<(BlockFeed, BlockFeedTask), LocalDeployerError> {
    let block_source_client = node_clients.random_validator().cloned().ok_or_else(|| {
        LocalDeployerError::WorkloadFailed {
            source: "block feed requires at least one validator".into(),
        }
    })?;

    spawn_block_feed(block_source_client)
        .await
        .map_err(|source| LocalDeployerError::WorkloadFailed {
            source: source.into(),
        })
}




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/builder/mod.rs
────────────────────────────────────────────────
use std::{
    num::{NonZeroU64, NonZeroUsize},
    time::Duration,
};

use integration_configs::topology::configs::network::Libp2pNetworkLayout;
use testing_framework_core::{
    scenario::{Builder as CoreScenarioBuilder, NodeControlCapability},
    topology::configs::wallet::WalletConfig,
};

use crate::{
    expectations::ConsensusLiveness,
    workloads::{chaos::RandomRestartWorkload, da, transaction},
};

macro_rules! non_zero_rate_fn {
    ($name:ident, $message:literal) => {
        const fn $name(rate: u64) -> NonZeroU64 {
            match NonZeroU64::new(rate) {
                Some(value) => value,
                None => panic!($message),
            }
        }
    };
}

non_zero_rate_fn!(
    transaction_rate_checked,
    "transaction rate must be non-zero"
);
non_zero_rate_fn!(channel_rate_checked, "channel rate must be non-zero");
non_zero_rate_fn!(blob_rate_checked, "blob rate must be non-zero");

pub trait ScenarioBuilderExt<Caps>: Sized {
    fn topology(self) -> TopologyConfigurator<Caps>;
    fn transactions(self) -> TransactionFlowBuilder<Caps>;
    fn da(self) -> DataAvailabilityFlowBuilder<Caps>;
    #[must_use]
    fn expect_consensus_liveness(self) -> Self;
    #[must_use]
    fn initialize_wallet(self, total_funds: u64, users: usize) -> Self;
}

impl<Caps> ScenarioBuilderExt<Caps> for CoreScenarioBuilder<Caps> {
    fn topology(self) -> TopologyConfigurator<Caps> {
        TopologyConfigurator { builder: self }
    }

    fn transactions(self) -> TransactionFlowBuilder<Caps> {
        TransactionFlowBuilder::new(self)
    }

    fn da(self) -> DataAvailabilityFlowBuilder<Caps> {
        DataAvailabilityFlowBuilder::new(self)
    }

    fn expect_consensus_liveness(self) -> Self {
        self.with_expectation(ConsensusLiveness::default())
    }

    fn initialize_wallet(self, total_funds: u64, users: usize) -> Self {
        let user_count = NonZeroUsize::new(users).expect("wallet user count must be non-zero");
        let wallet = WalletConfig::uniform(total_funds, user_count);
        self.with_wallet_config(wallet)
    }
}

pub struct TopologyConfigurator<Caps> {
    builder: CoreScenarioBuilder<Caps>,
}

impl<Caps> TopologyConfigurator<Caps> {
    #[must_use]
    pub fn validators(mut self, count: usize) -> Self {
        self.builder = self
            .builder
            .map_topology(|topology| topology.with_validator_count(count));
        self
    }

    #[must_use]
    pub fn executors(mut self, count: usize) -> Self {
        self.builder = self
            .builder
            .map_topology(|topology| topology.with_executor_count(count));
        self
    }

    #[must_use]
    pub fn network_star(mut self) -> Self {
        self.builder = self
            .builder
            .map_topology(|topology| topology.with_network_layout(Libp2pNetworkLayout::Star));
        self
    }

    #[must_use]
    pub fn apply(self) -> CoreScenarioBuilder<Caps> {
        self.builder
    }
}

pub struct TransactionFlowBuilder<Caps> {
    builder: CoreScenarioBuilder<Caps>,
    rate: NonZeroU64,
    users: Option<NonZeroUsize>,
}

impl<Caps> TransactionFlowBuilder<Caps> {
    const fn default_rate() -> NonZeroU64 {
        transaction_rate_checked(1)
    }

    const fn new(builder: CoreScenarioBuilder<Caps>) -> Self {
        Self {
            builder,
            rate: Self::default_rate(),
            users: None,
        }
    }

    #[must_use]
    pub const fn rate(mut self, rate: u64) -> Self {
        self.rate = transaction_rate_checked(rate);
        self
    }

    #[must_use]
    pub const fn rate_per_block(mut self, rate: NonZeroU64) -> Self {
        self.rate = rate;
        self
    }

    #[must_use]
    pub const fn users(mut self, users: usize) -> Self {
        match NonZeroUsize::new(users) {
            Some(value) => self.users = Some(value),
            None => panic!("transaction user count must be non-zero"),
        }
        self
    }

    #[must_use]
    pub fn apply(mut self) -> CoreScenarioBuilder<Caps> {
        let workload = transaction::Workload::with_rate(self.rate.get())
            .expect("transaction rate must be non-zero")
            .with_user_limit(self.users);
        self.builder = self.builder.with_workload(workload);
        self.builder
    }
}

pub struct DataAvailabilityFlowBuilder<Caps> {
    builder: CoreScenarioBuilder<Caps>,
    channel_rate: NonZeroU64,
    blob_rate: NonZeroU64,
}

impl<Caps> DataAvailabilityFlowBuilder<Caps> {
    const fn default_channel_rate() -> NonZeroU64 {
        channel_rate_checked(1)
    }

    const fn default_blob_rate() -> NonZeroU64 {
        blob_rate_checked(1)
    }

    const fn new(builder: CoreScenarioBuilder<Caps>) -> Self {
        Self {
            builder,
            channel_rate: Self::default_channel_rate(),
            blob_rate: Self::default_blob_rate(),
        }
    }

    #[must_use]
    pub const fn channel_rate(mut self, rate: u64) -> Self {
        self.channel_rate = channel_rate_checked(rate);
        self
    }

    #[must_use]
    pub const fn channel_rate_per_block(mut self, rate: NonZeroU64) -> Self {
        self.channel_rate = rate;
        self
    }

    #[must_use]
    pub const fn blob_rate(mut self, rate: u64) -> Self {
        self.blob_rate = blob_rate_checked(rate);
        self
    }

    #[must_use]
    pub const fn blob_rate_per_block(mut self, rate: NonZeroU64) -> Self {
        self.blob_rate = rate;
        self
    }

    #[must_use]
    pub fn apply(mut self) -> CoreScenarioBuilder<Caps> {
        let count = (self.channel_rate.get() * self.blob_rate.get()) as usize;
        let workload = da::Workload::with_channel_count(count.max(1));
        self.builder = self.builder.with_workload(workload);
        self.builder
    }
}

pub trait ChaosBuilderExt: Sized {
    fn chaos_random_restart(self) -> ChaosRestartBuilder;
}

impl ChaosBuilderExt for CoreScenarioBuilder<NodeControlCapability> {
    fn chaos_random_restart(self) -> ChaosRestartBuilder {
        ChaosRestartBuilder {
            builder: self,
            min_delay: Duration::from_secs(10),
            max_delay: Duration::from_secs(30),
            target_cooldown: Duration::from_secs(60),
            include_validators: true,
            include_executors: true,
        }
    }
}

pub struct ChaosRestartBuilder {
    builder: CoreScenarioBuilder<NodeControlCapability>,
    min_delay: Duration,
    max_delay: Duration,
    target_cooldown: Duration,
    include_validators: bool,
    include_executors: bool,
}

impl ChaosRestartBuilder {
    #[must_use]
    pub fn min_delay(mut self, delay: Duration) -> Self {
        assert!(!delay.is_zero(), "chaos restart min delay must be non-zero");
        self.min_delay = delay;
        self
    }

    #[must_use]
    pub fn max_delay(mut self, delay: Duration) -> Self {
        assert!(!delay.is_zero(), "chaos restart max delay must be non-zero");
        self.max_delay = delay;
        self
    }

    #[must_use]
    pub fn target_cooldown(mut self, cooldown: Duration) -> Self {
        assert!(
            !cooldown.is_zero(),
            "chaos restart target cooldown must be non-zero"
        );
        self.target_cooldown = cooldown;
        self
    }

    #[must_use]
    pub const fn include_validators(mut self, enabled: bool) -> Self {
        self.include_validators = enabled;
        self
    }

    #[must_use]
    pub const fn include_executors(mut self, enabled: bool) -> Self {
        self.include_executors = enabled;
        self
    }

    #[must_use]
    pub fn apply(mut self) -> CoreScenarioBuilder<NodeControlCapability> {
        assert!(
            self.min_delay <= self.max_delay,
            "chaos restart min delay must not exceed max delay"
        );
        assert!(
            self.target_cooldown >= self.min_delay,
            "chaos restart target cooldown must be >= min delay"
        );
        assert!(
            self.include_validators || self.include_executors,
            "chaos restart requires at least one node group"
        );

        let workload = RandomRestartWorkload::new(
            self.min_delay,
            self.max_delay,
            self.target_cooldown,
            self.include_validators,
            self.include_executors,
        );
        self.builder = self.builder.with_workload(workload);
        self.builder
    }
}




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/expectations/consensus_liveness.rs
────────────────────────────────────────────────
use std::time::Duration;

use async_trait::async_trait;
use testing_framework_core::scenario::{DynError, Expectation, RunContext};
use thiserror::Error;
use tokio::time::sleep;

#[derive(Clone, Copy, Debug)]
pub struct ConsensusLiveness {
    lag_allowance: u64,
}

impl Default for ConsensusLiveness {
    fn default() -> Self {
        Self {
            lag_allowance: LAG_ALLOWANCE,
        }
    }
}

const LAG_ALLOWANCE: u64 = 2;
const MIN_PROGRESS_BLOCKS: u64 = 5;
const REQUEST_RETRIES: usize = 5;
const REQUEST_RETRY_DELAY: Duration = Duration::from_secs(2);

#[async_trait]
impl Expectation for ConsensusLiveness {
    fn name(&self) -> &'static str {
        "consensus_liveness"
    }

    async fn evaluate(&mut self, ctx: &RunContext) -> Result<(), DynError> {
        Self::ensure_participants(ctx)?;
        let target_hint = Self::target_blocks(ctx);
        let check = Self::collect_results(ctx).await;
        (*self).report(target_hint, check)
    }
}

const fn consensus_target_blocks(ctx: &RunContext) -> u64 {
    ctx.expected_blocks()
}

#[derive(Debug, Error)]
enum ConsensusLivenessIssue {
    #[error("{node} height {height} below target {target}")]
    HeightBelowTarget {
        node: String,
        height: u64,
        target: u64,
    },
    #[error("{node} consensus_info failed: {source}")]
    RequestFailed {
        node: String,
        #[source]
        source: DynError,
    },
}

#[derive(Debug, Error)]
enum ConsensusLivenessError {
    #[error("consensus liveness requires at least one validator or executor")]
    MissingParticipants,
    #[error("consensus liveness violated (target={target}):\n{details}")]
    Violations {
        target: u64,
        #[source]
        details: ViolationIssues,
    },
}

#[derive(Debug, Error)]
#[error("{message}")]
struct ViolationIssues {
    issues: Vec<ConsensusLivenessIssue>,
    message: String,
}

impl ConsensusLiveness {
    const fn target_blocks(ctx: &RunContext) -> u64 {
        consensus_target_blocks(ctx)
    }

    fn ensure_participants(ctx: &RunContext) -> Result<(), DynError> {
        if ctx.node_clients().all_clients().count() == 0 {
            Err(Box::new(ConsensusLivenessError::MissingParticipants))
        } else {
            Ok(())
        }
    }

    async fn collect_results(ctx: &RunContext) -> LivenessCheck {
        let participant_count = ctx.node_clients().all_clients().count().max(1);
        let max_attempts = participant_count * REQUEST_RETRIES;
        let mut samples = Vec::with_capacity(participant_count);
        let mut issues = Vec::new();

        for attempt in 0..max_attempts {
            match Self::fetch_cluster_height(ctx).await {
                Ok(height) => {
                    samples.push(NodeSample {
                        label: format!("sample-{attempt}"),
                        height,
                    });
                    if samples.len() >= participant_count {
                        break;
                    }
                }
                Err(err) => issues.push(ConsensusLivenessIssue::RequestFailed {
                    node: format!("sample-{attempt}"),
                    source: err,
                }),
            }

            if samples.len() < participant_count {
                sleep(REQUEST_RETRY_DELAY).await;
            }
        }

        LivenessCheck { samples, issues }
    }

    async fn fetch_cluster_height(ctx: &RunContext) -> Result<u64, DynError> {
        ctx.cluster_client()
            .try_all_clients(|client| {
                Box::pin(async move {
                    client
                        .consensus_info()
                        .await
                        .map(|info| info.height)
                        .map_err(|err| -> DynError { err.into() })
                })
            })
            .await
    }

    #[must_use]
    pub const fn with_lag_allowance(mut self, lag_allowance: u64) -> Self {
        self.lag_allowance = lag_allowance;
        self
    }

    fn report(self, target_hint: u64, mut check: LivenessCheck) -> Result<(), DynError> {
        if check.samples.is_empty() {
            return Err(Box::new(ConsensusLivenessError::MissingParticipants));
        }

        let max_height = check
            .samples
            .iter()
            .map(|sample| sample.height)
            .max()
            .unwrap_or(0);

        let mut target = target_hint;
        if target == 0 || target > max_height {
            target = max_height;
        }

        if max_height < MIN_PROGRESS_BLOCKS {
            check
                .issues
                .push(ConsensusLivenessIssue::HeightBelowTarget {
                    node: "network".to_owned(),
                    height: max_height,
                    target: MIN_PROGRESS_BLOCKS,
                });
        }

        for sample in &check.samples {
            if sample.height + self.lag_allowance < target {
                check
                    .issues
                    .push(ConsensusLivenessIssue::HeightBelowTarget {
                        node: sample.label.clone(),
                        height: sample.height,
                        target,
                    });
            }
        }

        if check.issues.is_empty() {
            tracing::info!(
                target,
                heights = ?check.samples.iter().map(|s| s.height).collect::<Vec<_>>(),
                "consensus liveness expectation satisfied"
            );
            Ok(())
        } else {
            Err(Box::new(ConsensusLivenessError::Violations {
                target,
                details: check.issues.into(),
            }))
        }
    }
}

struct NodeSample {
    label: String,
    height: u64,
}

struct LivenessCheck {
    samples: Vec<NodeSample>,
    issues: Vec<ConsensusLivenessIssue>,
}

impl From<Vec<ConsensusLivenessIssue>> for ViolationIssues {
    fn from(issues: Vec<ConsensusLivenessIssue>) -> Self {
        let mut message = String::new();
        for issue in &issues {
            if !message.is_empty() {
                message.push('\n');
            }
            message.push_str("- ");
            message.push_str(&issue.to_string());
        }
        Self { issues, message }
    }
}




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/expectations/mod.rs
────────────────────────────────────────────────
mod consensus_liveness;

pub use consensus_liveness::ConsensusLiveness;




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/lib.rs
────────────────────────────────────────────────
pub mod builder;
pub mod expectations;
pub mod util;
pub mod workloads;

pub use builder::{ChaosBuilderExt, ScenarioBuilderExt};
pub use expectations::ConsensusLiveness;
pub use workloads::transaction::TxInclusionExpectation;




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/util/mod.rs
────────────────────────────────────────────────
pub mod tx;




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/util/tx.rs
────────────────────────────────────────────────
use ed25519_dalek::{Signer as _, SigningKey};
use nomos_core::mantle::{
    MantleTx, Op, OpProof, SignedMantleTx, Transaction as _,
    ledger::Tx as LedgerTx,
    ops::channel::{ChannelId, MsgId, inscribe::InscriptionOp},
};
use zksign::SecretKey;

#[must_use]
pub fn create_inscription_transaction_with_id(id: ChannelId) -> SignedMantleTx {
    let signing_key = SigningKey::from_bytes(&[0u8; 32]);
    let signer = signing_key.verifying_key();

    let inscription_op = InscriptionOp {
        channel_id: id,
        inscription: format!("Test channel inscription {id:?}").into_bytes(),
        parent: MsgId::root(),
        signer,
    };

    let mantle_tx = MantleTx {
        ops: vec![Op::ChannelInscribe(inscription_op)],
        ledger_tx: LedgerTx::new(vec![], vec![]),
        storage_gas_price: 0,
        execution_gas_price: 0,
    };

    let tx_hash = mantle_tx.hash();
    let signature = signing_key.sign(&tx_hash.as_signing_bytes());

    SignedMantleTx::new(
        mantle_tx,
        vec![OpProof::Ed25519Sig(signature)],
        SecretKey::multi_sign(&[], tx_hash.as_ref()).expect("zk signature generation"),
    )
    .expect("valid transaction")
}




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/workloads/chaos.rs
────────────────────────────────────────────────
use std::{collections::HashMap, time::Duration};

use async_trait::async_trait;
use rand::{Rng as _, seq::SliceRandom as _, thread_rng};
use testing_framework_core::scenario::{DynError, RunContext, Workload};
use tokio::time::{Instant, sleep};
use tracing::info;

pub struct RandomRestartWorkload {
    min_delay: Duration,
    max_delay: Duration,
    target_cooldown: Duration,
    include_validators: bool,
    include_executors: bool,
}

impl RandomRestartWorkload {
    #[must_use]
    pub const fn new(
        min_delay: Duration,
        max_delay: Duration,
        target_cooldown: Duration,
        include_validators: bool,
        include_executors: bool,
    ) -> Self {
        Self {
            min_delay,
            max_delay,
            target_cooldown,
            include_validators,
            include_executors,
        }
    }

    fn targets(&self, ctx: &RunContext) -> Vec<Target> {
        let mut targets = Vec::new();
        let validator_count = ctx.descriptors().validators().len();
        if self.include_validators {
            if validator_count > 1 {
                for index in 0..validator_count {
                    targets.push(Target::Validator(index));
                }
            } else if validator_count == 1 {
                info!("chaos restart skipping validators: only one validator configured");
            }
        }
        if self.include_executors {
            for index in 0..ctx.descriptors().executors().len() {
                targets.push(Target::Executor(index));
            }
        }
        targets
    }

    fn random_delay(&self) -> Duration {
        if self.max_delay <= self.min_delay {
            return self.min_delay;
        }
        let spread = self
            .max_delay
            .checked_sub(self.min_delay)
            .unwrap_or_else(|| Duration::from_millis(1))
            .as_secs_f64();
        let offset = thread_rng().gen_range(0.0..=spread);
        self.min_delay
            .checked_add(Duration::from_secs_f64(offset))
            .unwrap_or(self.max_delay)
    }

    fn initialize_cooldowns(&self, targets: &[Target]) -> HashMap<Target, Instant> {
        let now = Instant::now();
        let ready = now.checked_sub(self.target_cooldown).unwrap_or(now);
        targets
            .iter()
            .copied()
            .map(|target| (target, ready))
            .collect()
    }

    async fn pick_target(
        &self,
        targets: &[Target],
        cooldowns: &HashMap<Target, Instant>,
    ) -> Target {
        loop {
            let now = Instant::now();
            if let Some(next_ready) = cooldowns
                .values()
                .copied()
                .filter(|ready| *ready > now)
                .min()
            {
                let wait = next_ready.saturating_duration_since(now);
                if !wait.is_zero() {
                    sleep(wait).await;
                    continue;
                }
            }

            let available: Vec<Target> = targets
                .iter()
                .copied()
                .filter(|target| cooldowns.get(target).is_none_or(|ready| *ready <= now))
                .collect();

            if let Some(choice) = available.choose(&mut thread_rng()).copied() {
                return choice;
            }

            return targets
                .choose(&mut thread_rng())
                .copied()
                .expect("chaos restart workload has targets");
        }
    }
}

#[async_trait]
impl Workload for RandomRestartWorkload {
    fn name(&self) -> &'static str {
        "chaos_random_restart"
    }

    async fn start(&self, ctx: &RunContext) -> Result<(), DynError> {
        let handle = ctx
            .node_control()
            .ok_or_else(|| "chaos restart workload requires node control".to_owned())?;

        let targets = self.targets(ctx);
        if targets.is_empty() {
            return Err("chaos restart workload has no eligible targets".into());
        }

        let mut cooldowns = self.initialize_cooldowns(&targets);

        loop {
            sleep(self.random_delay()).await;
            let target = self.pick_target(&targets, &cooldowns).await;

            match target {
                Target::Validator(index) => handle
                    .restart_validator(index)
                    .await
                    .map_err(|err| format!("validator restart failed: {err}"))?,
                Target::Executor(index) => handle
                    .restart_executor(index)
                    .await
                    .map_err(|err| format!("executor restart failed: {err}"))?,
            }

            cooldowns.insert(target, Instant::now() + self.target_cooldown);
        }
    }
}

#[derive(Clone, Copy, PartialEq, Eq, Hash)]
enum Target {
    Validator(usize),
    Executor(usize),
}




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/workloads/da/expectation.rs
────────────────────────────────────────────────
use std::{
    collections::HashSet,
    sync::{Arc, Mutex},
};

use async_trait::async_trait;
use nomos_core::mantle::{
    AuthenticatedMantleTx as _,
    ops::{Op, channel::ChannelId},
};
use testing_framework_core::scenario::{BlockRecord, DynError, Expectation, RunContext};
use thiserror::Error;
use tokio::sync::broadcast;

#[derive(Debug)]
pub struct DaWorkloadExpectation {
    planned_channels: Vec<ChannelId>,
    capture_state: Option<CaptureState>,
}

#[derive(Debug)]
struct CaptureState {
    planned: Arc<HashSet<ChannelId>>,
    inscriptions: Arc<Mutex<HashSet<ChannelId>>>,
    blobs: Arc<Mutex<HashSet<ChannelId>>>,
}

const MIN_INCLUSION_RATIO: f64 = 0.8;

#[derive(Debug, Error)]
enum DaExpectationError {
    #[error("da workload expectation not started")]
    NotCaptured,
    #[error("missing inscriptions for {missing:?}")]
    MissingInscriptions { missing: Vec<ChannelId> },
    #[error("missing blobs for {missing:?}")]
    MissingBlobs { missing: Vec<ChannelId> },
}

impl DaWorkloadExpectation {
    pub const fn new(planned_channels: Vec<ChannelId>) -> Self {
        Self {
            planned_channels,
            capture_state: None,
        }
    }
}

#[async_trait]
impl Expectation for DaWorkloadExpectation {
    fn name(&self) -> &'static str {
        "da_workload_inclusions"
    }

    async fn start_capture(&mut self, ctx: &RunContext) -> Result<(), DynError> {
        if self.capture_state.is_some() {
            return Ok(());
        }

        let planned = Arc::new(
            self.planned_channels
                .iter()
                .copied()
                .collect::<HashSet<_>>(),
        );
        let inscriptions = Arc::new(Mutex::new(HashSet::new()));
        let blobs = Arc::new(Mutex::new(HashSet::new()));

        let mut receiver = ctx.block_feed().subscribe();
        let planned_for_task = Arc::clone(&planned);
        let inscriptions_for_task = Arc::clone(&inscriptions);
        let blobs_for_task = Arc::clone(&blobs);

        tokio::spawn(async move {
            loop {
                match receiver.recv().await {
                    Ok(record) => capture_block(
                        record.as_ref(),
                        &planned_for_task,
                        &inscriptions_for_task,
                        &blobs_for_task,
                    ),
                    Err(broadcast::error::RecvError::Lagged(_)) => {}
                    Err(broadcast::error::RecvError::Closed) => break,
                }
            }
        });

        self.capture_state = Some(CaptureState {
            planned,
            inscriptions,
            blobs,
        });

        Ok(())
    }

    async fn evaluate(&mut self, _ctx: &RunContext) -> Result<(), DynError> {
        let state = self
            .capture_state
            .as_ref()
            .ok_or(DaExpectationError::NotCaptured)
            .map_err(DynError::from)?;

        let planned_total = state.planned.len();
        let missing_inscriptions = {
            let inscriptions = state
                .inscriptions
                .lock()
                .expect("inscription lock poisoned");
            missing_channels(&state.planned, &inscriptions)
        };
        let required_inscriptions = minimum_required(planned_total, MIN_INCLUSION_RATIO);
        if planned_total.saturating_sub(missing_inscriptions.len()) < required_inscriptions {
            return Err(DaExpectationError::MissingInscriptions {
                missing: missing_inscriptions,
            }
            .into());
        }

        let missing_blobs = {
            let blobs = state.blobs.lock().expect("blob lock poisoned");
            missing_channels(&state.planned, &blobs)
        };
        let required_blobs = minimum_required(planned_total, MIN_INCLUSION_RATIO);
        if planned_total.saturating_sub(missing_blobs.len()) < required_blobs {
            return Err(DaExpectationError::MissingBlobs {
                missing: missing_blobs,
            }
            .into());
        }

        Ok(())
    }
}

fn capture_block(
    block: &BlockRecord,
    planned: &HashSet<ChannelId>,
    inscriptions: &Arc<Mutex<HashSet<ChannelId>>>,
    blobs: &Arc<Mutex<HashSet<ChannelId>>>,
) {
    let mut new_inscriptions = Vec::new();
    let mut new_blobs = Vec::new();

    for tx in block.block.transactions() {
        for op in &tx.mantle_tx().ops {
            match op {
                Op::ChannelInscribe(inscribe) if planned.contains(&inscribe.channel_id) => {
                    new_inscriptions.push(inscribe.channel_id);
                }
                Op::ChannelBlob(blob) if planned.contains(&blob.channel) => {
                    new_blobs.push(blob.channel);
                }
                _ => {}
            }
        }
    }

    if !new_inscriptions.is_empty() {
        let mut guard = inscriptions.lock().expect("inscription lock poisoned");
        guard.extend(new_inscriptions);
    }

    if !new_blobs.is_empty() {
        let mut guard = blobs.lock().expect("blob lock poisoned");
        guard.extend(new_blobs);
    }
}

fn missing_channels(planned: &HashSet<ChannelId>, observed: &HashSet<ChannelId>) -> Vec<ChannelId> {
    planned.difference(observed).copied().collect()
}

fn minimum_required(total: usize, ratio: f64) -> usize {
    ((total as f64) * ratio).ceil() as usize
}




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/workloads/da/mod.rs
────────────────────────────────────────────────
mod expectation;
mod workload;

pub use workload::Workload;




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/workloads/da/workload.rs
────────────────────────────────────────────────
use std::{sync::Arc, time::Duration};

use async_trait::async_trait;
use ed25519_dalek::SigningKey;
use executor_http_client::ExecutorHttpClient;
use nomos_core::{
    da::BlobId,
    mantle::ops::{
        Op,
        channel::{ChannelId, MsgId},
    },
};
use rand::{Rng as _, RngCore as _, seq::SliceRandom as _, thread_rng};
use testing_framework_core::{
    nodes::ApiClient,
    scenario::{BlockRecord, DynError, Expectation, RunContext, Workload as ScenarioWorkload},
};
use tokio::{sync::broadcast, time::sleep};

use super::expectation::DaWorkloadExpectation;
use crate::{
    util::tx,
    workloads::util::{find_channel_op, submit_transaction_via_cluster},
};

const TEST_KEY_BYTES: [u8; 32] = [0u8; 32];
const DEFAULT_CHANNELS: usize = 1;
const MIN_BLOB_CHUNKS: usize = 1;
const MAX_BLOB_CHUNKS: usize = 8;
const PUBLISH_RETRIES: usize = 5;
const PUBLISH_RETRY_DELAY: Duration = Duration::from_secs(2);

#[derive(Clone)]
pub struct Workload {
    planned_channels: Arc<[ChannelId]>,
}

impl Default for Workload {
    fn default() -> Self {
        Self::with_channel_count(DEFAULT_CHANNELS)
    }
}

impl Workload {
    #[must_use]
    pub fn with_channel_count(count: usize) -> Self {
        assert!(count > 0, "da workload requires positive count");
        Self {
            planned_channels: Arc::from(planned_channel_ids(count)),
        }
    }

    fn plan(&self) -> Arc<[ChannelId]> {
        Arc::clone(&self.planned_channels)
    }
}

#[async_trait]
impl ScenarioWorkload for Workload {
    fn name(&self) -> &'static str {
        "channel_workload"
    }

    fn expectations(&self) -> Vec<Box<dyn Expectation>> {
        let planned = self.plan().to_vec();
        vec![Box::new(DaWorkloadExpectation::new(planned))]
    }

    async fn start(&self, ctx: &RunContext) -> Result<(), DynError> {
        let mut receiver = ctx.block_feed().subscribe();

        for channel_id in self.plan().iter().copied() {
            run_channel_flow(ctx, &mut receiver, channel_id).await?;
        }

        Ok(())
    }
}

async fn run_channel_flow(
    ctx: &RunContext,
    receiver: &mut broadcast::Receiver<Arc<BlockRecord>>,
    channel_id: ChannelId,
) -> Result<(), DynError> {
    let tx = Arc::new(tx::create_inscription_transaction_with_id(channel_id));
    submit_transaction_via_cluster(ctx, Arc::clone(&tx)).await?;

    let inscription_id = wait_for_inscription(receiver, channel_id).await?;
    let blob_id = publish_blob(ctx, channel_id, inscription_id).await?;
    wait_for_blob(receiver, channel_id, blob_id).await?;
    Ok(())
}

async fn wait_for_inscription(
    receiver: &mut broadcast::Receiver<Arc<BlockRecord>>,
    channel_id: ChannelId,
) -> Result<MsgId, DynError> {
    wait_for_channel_op(receiver, move |op| {
        if let Op::ChannelInscribe(inscribe) = op
            && inscribe.channel_id == channel_id
        {
            Some(inscribe.id())
        } else {
            None
        }
    })
    .await
}

async fn wait_for_blob(
    receiver: &mut broadcast::Receiver<Arc<BlockRecord>>,
    channel_id: ChannelId,
    blob_id: BlobId,
) -> Result<MsgId, DynError> {
    wait_for_channel_op(receiver, move |op| {
        if let Op::ChannelBlob(blob_op) = op
            && blob_op.channel == channel_id
            && blob_op.blob == blob_id
        {
            Some(blob_op.id())
        } else {
            None
        }
    })
    .await
}

async fn wait_for_channel_op<F>(
    receiver: &mut broadcast::Receiver<Arc<BlockRecord>>,
    mut matcher: F,
) -> Result<MsgId, DynError>
where
    F: FnMut(&Op) -> Option<MsgId>,
{
    loop {
        match receiver.recv().await {
            Ok(record) => {
                if let Some(msg_id) = find_channel_op(record.block.as_ref(), &mut matcher) {
                    return Ok(msg_id);
                }
            }
            Err(broadcast::error::RecvError::Lagged(_)) => {}
            Err(broadcast::error::RecvError::Closed) => {
                return Err("block feed closed while waiting for channel operations".into());
            }
        }
    }
}

async fn publish_blob(
    ctx: &RunContext,
    channel_id: ChannelId,
    parent_msg: MsgId,
) -> Result<BlobId, DynError> {
    let executors = ctx.node_clients().executor_clients();
    if executors.is_empty() {
        return Err("da workload requires at least one executor".into());
    }

    let signer = SigningKey::from_bytes(&TEST_KEY_BYTES).verifying_key();
    let data = random_blob_payload();
    let client = ExecutorHttpClient::new(None);

    let mut candidates: Vec<&ApiClient> = executors.iter().collect();
    let mut last_err = None;
    for attempt in 1..=PUBLISH_RETRIES {
        candidates.shuffle(&mut thread_rng());
        for executor in &candidates {
            let executor_url = executor.base_url().clone();
            match client
                .publish_blob(executor_url, channel_id, parent_msg, signer, data.clone())
                .await
            {
                Ok(blob_id) => return Ok(blob_id),
                Err(err) => last_err = Some(err.into()),
            }
        }

        if attempt < PUBLISH_RETRIES {
            sleep(PUBLISH_RETRY_DELAY).await;
        }
    }

    Err(last_err.unwrap_or_else(|| "da workload could not publish blob".into()))
}

fn random_blob_payload() -> Vec<u8> {
    let mut rng = thread_rng();
    let chunks = rng.gen_range(MIN_BLOB_CHUNKS..=MAX_BLOB_CHUNKS);
    let mut data = vec![0u8; 31 * chunks];
    rng.fill_bytes(&mut data);
    data
}

fn planned_channel_ids(total: usize) -> Vec<ChannelId> {
    (0..total as u64)
        .map(deterministic_channel_id)
        .collect::<Vec<_>>()
}

fn deterministic_channel_id(index: u64) -> ChannelId {
    let mut bytes = [0u8; 32];
    bytes[..8].copy_from_slice(b"chn_wrkd");
    bytes[24..].copy_from_slice(&index.to_be_bytes());
    ChannelId::from(bytes)
}




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/workloads/mod.rs
────────────────────────────────────────────────
pub mod chaos;
pub mod da;
pub mod transaction;
pub mod util;

pub use transaction::TxInclusionExpectation;




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/workloads/transaction/expectation.rs
────────────────────────────────────────────────
use std::{
    collections::HashSet,
    num::{NonZeroU64, NonZeroUsize},
    sync::{
        Arc,
        atomic::{AtomicU64, Ordering},
    },
};

use async_trait::async_trait;
use nomos_core::{header::HeaderId, mantle::AuthenticatedMantleTx as _};
use testing_framework_core::scenario::{DynError, Expectation, RunContext};
use thiserror::Error;
use tokio::sync::broadcast;
use zksign::PublicKey;

use super::workload::{limited_user_count, submission_plan};

const MIN_INCLUSION_RATIO: f64 = 0.5;

#[derive(Clone)]
pub struct TxInclusionExpectation {
    txs_per_block: NonZeroU64,
    user_limit: Option<NonZeroUsize>,
    capture_state: Option<CaptureState>,
}

#[derive(Clone)]
struct CaptureState {
    observed: Arc<AtomicU64>,
    expected: u64,
}

#[derive(Debug, Error)]
enum TxExpectationError {
    #[error("transaction workload requires seeded accounts")]
    MissingAccounts,
    #[error("transaction workload planned zero transactions")]
    NoPlannedTransactions,
    #[error("transaction inclusion expectation not captured")]
    NotCaptured,
    #[error("transaction inclusion observed {observed} below required {required}")]
    InsufficientInclusions { observed: u64, required: u64 },
}

impl TxInclusionExpectation {
    pub const NAME: &'static str = "tx_inclusion_expectation";

    #[must_use]
    pub const fn new(txs_per_block: NonZeroU64, user_limit: Option<NonZeroUsize>) -> Self {
        Self {
            txs_per_block,
            user_limit,
            capture_state: None,
        }
    }
}

#[async_trait]
impl Expectation for TxInclusionExpectation {
    fn name(&self) -> &'static str {
        Self::NAME
    }

    async fn start_capture(&mut self, ctx: &RunContext) -> Result<(), DynError> {
        if self.capture_state.is_some() {
            return Ok(());
        }

        let wallet_accounts = ctx.descriptors().config().wallet().accounts.clone();
        if wallet_accounts.is_empty() {
            return Err(TxExpectationError::MissingAccounts.into());
        }

        let available = limited_user_count(self.user_limit, wallet_accounts.len());
        let (planned, _) = submission_plan(self.txs_per_block, ctx, available)?;
        if planned == 0 {
            return Err(TxExpectationError::NoPlannedTransactions.into());
        }

        let wallet_pks = wallet_accounts
            .into_iter()
            .take(planned)
            .map(|account| account.secret_key.to_public_key())
            .collect::<HashSet<PublicKey>>();

        let observed = Arc::new(AtomicU64::new(0));
        let receiver = ctx.block_feed().subscribe();
        let tracked_accounts = Arc::new(wallet_pks);
        let spawn_accounts = Arc::clone(&tracked_accounts);
        let spawn_observed = Arc::clone(&observed);

        tokio::spawn(async move {
            let mut receiver = receiver;
            let genesis_parent = HeaderId::from([0; 32]);
            loop {
                match receiver.recv().await {
                    Ok(record) => {
                        if record.block.header().parent_block() == genesis_parent {
                            continue;
                        }

                        for tx in record.block.transactions() {
                            for note in &tx.mantle_tx().ledger_tx.outputs {
                                if spawn_accounts.contains(&note.pk) {
                                    spawn_observed.fetch_add(1, Ordering::Relaxed);
                                    break;
                                }
                            }
                        }
                    }
                    Err(broadcast::error::RecvError::Lagged(_)) => {}
                    Err(broadcast::error::RecvError::Closed) => break,
                }
            }
        });

        self.capture_state = Some(CaptureState {
            observed,
            expected: planned as u64,
        });

        Ok(())
    }

    async fn evaluate(&mut self, _ctx: &RunContext) -> Result<(), DynError> {
        let state = self
            .capture_state
            .as_ref()
            .ok_or(TxExpectationError::NotCaptured)?;

        let observed = state.observed.load(Ordering::Relaxed);
        let required = ((state.expected as f64) * MIN_INCLUSION_RATIO).ceil() as u64;

        if observed >= required {
            Ok(())
        } else {
            Err(TxExpectationError::InsufficientInclusions { observed, required }.into())
        }
    }
}




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/workloads/transaction/mod.rs
────────────────────────────────────────────────
mod expectation;
mod workload;

pub use expectation::TxInclusionExpectation;
pub use workload::Workload;




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/workloads/transaction/workload.rs
────────────────────────────────────────────────
use std::{
    collections::{HashMap, VecDeque},
    num::{NonZeroU64, NonZeroUsize},
    sync::Arc,
    time::Duration,
};

use async_trait::async_trait;
use integration_configs::topology::configs::wallet::WalletAccount;
use nomos_core::mantle::{
    GenesisTx as _, Note, SignedMantleTx, Transaction as _, Utxo, tx_builder::MantleTxBuilder,
};
use testing_framework_core::{
    scenario::{DynError, Expectation, RunContext, RunMetrics, Workload as ScenarioWorkload},
    topology::{GeneratedNodeConfig, GeneratedTopology},
};
use tokio::time::sleep;
use zksign::{PublicKey, SecretKey};

use super::expectation::TxInclusionExpectation;
use crate::workloads::util::submit_transaction_via_cluster;

#[derive(Clone)]
pub struct Workload {
    txs_per_block: NonZeroU64,
    user_limit: Option<NonZeroUsize>,
    accounts: Vec<WalletInput>,
}

#[derive(Clone)]
struct WalletInput {
    account: WalletAccount,
    utxo: Utxo,
}

#[async_trait]
impl ScenarioWorkload for Workload {
    fn name(&self) -> &'static str {
        "tx_workload"
    }

    fn expectations(&self) -> Vec<Box<dyn Expectation>> {
        vec![Box::new(TxInclusionExpectation::new(
            self.txs_per_block,
            self.user_limit,
        ))]
    }

    fn init(
        &mut self,
        descriptors: &GeneratedTopology,
        _run_metrics: &RunMetrics,
    ) -> Result<(), DynError> {
        let wallet_accounts = descriptors.config().wallet().accounts.clone();
        if wallet_accounts.is_empty() {
            return Err("transaction workload requires seeded accounts".into());
        }

        let reference_node = descriptors
            .validators()
            .first()
            .or_else(|| descriptors.executors().first())
            .ok_or("transaction workload requires at least one node in the topology")?;

        let utxo_map = wallet_utxo_map(reference_node);
        let mut accounts = wallet_accounts
            .into_iter()
            .filter_map(|account| {
                utxo_map
                    .get(&account.public_key())
                    .copied()
                    .map(|utxo| WalletInput { account, utxo })
            })
            .collect::<Vec<_>>();

        apply_user_limit(&mut accounts, self.user_limit);

        if accounts.is_empty() {
            return Err(
                "transaction workload could not match any accounts to genesis UTXOs".into(),
            );
        }

        self.accounts = accounts;
        Ok(())
    }

    async fn start(&self, ctx: &RunContext) -> Result<(), DynError> {
        Submission::new(self, ctx)?.execute().await
    }
}

impl Workload {
    #[must_use]
    pub const fn new(txs_per_block: NonZeroU64) -> Self {
        Self {
            txs_per_block,
            user_limit: None,
            accounts: Vec::new(),
        }
    }

    #[must_use]
    pub fn with_rate(txs_per_block: u64) -> Option<Self> {
        NonZeroU64::new(txs_per_block).map(Self::new)
    }

    #[must_use]
    pub const fn txs_per_block(&self) -> NonZeroU64 {
        self.txs_per_block
    }

    #[must_use]
    pub const fn with_user_limit(mut self, user_limit: Option<NonZeroUsize>) -> Self {
        self.user_limit = user_limit;
        self
    }
}

impl Default for Workload {
    fn default() -> Self {
        Self::new(NonZeroU64::new(1).expect("non-zero"))
    }
}

struct Submission<'a> {
    plan: VecDeque<WalletInput>,
    ctx: &'a RunContext,
    interval: Duration,
}

impl<'a> Submission<'a> {
    fn new(workload: &Workload, ctx: &'a RunContext) -> Result<Self, DynError> {
        if workload.accounts.is_empty() {
            return Err("transaction workload has no available accounts".into());
        }

        let (planned, interval) =
            submission_plan(workload.txs_per_block, ctx, workload.accounts.len())?;

        let plan = workload
            .accounts
            .iter()
            .take(planned)
            .cloned()
            .collect::<VecDeque<_>>();

        Ok(Self {
            plan,
            ctx,
            interval,
        })
    }

    async fn execute(mut self) -> Result<(), DynError> {
        while let Some(input) = self.plan.pop_front() {
            submit_wallet_transaction(self.ctx, &input).await?;

            if !self.interval.is_zero() {
                sleep(self.interval).await;
            }
        }

        Ok(())
    }
}

async fn submit_wallet_transaction(ctx: &RunContext, input: &WalletInput) -> Result<(), DynError> {
    let signed_tx = Arc::new(build_wallet_transaction(input)?);
    submit_transaction_via_cluster(ctx, signed_tx).await
}

fn build_wallet_transaction(input: &WalletInput) -> Result<SignedMantleTx, DynError> {
    let builder = MantleTxBuilder::new()
        .add_ledger_input(input.utxo)
        .add_ledger_output(Note::new(input.utxo.note.value, input.account.public_key()));

    let mantle_tx = builder.build();
    let tx_hash = mantle_tx.hash();

    let signature = SecretKey::multi_sign(
        std::slice::from_ref(&input.account.secret_key),
        tx_hash.as_ref(),
    )
    .map_err(|err| format!("transaction workload could not sign transaction: {err}"))?;

    SignedMantleTx::new(mantle_tx, Vec::new(), signature).map_err(|err| {
        format!("transaction workload constructed invalid transaction: {err}").into()
    })
}

fn wallet_utxo_map(node: &GeneratedNodeConfig) -> HashMap<PublicKey, Utxo> {
    let genesis_tx = node.general.consensus_config.genesis_tx.clone();
    let ledger_tx = genesis_tx.mantle_tx().ledger_tx.clone();
    let tx_hash = ledger_tx.hash();

    ledger_tx
        .outputs
        .iter()
        .enumerate()
        .map(|(idx, note)| (note.pk, Utxo::new(tx_hash, idx, *note)))
        .collect()
}

fn apply_user_limit<T>(items: &mut Vec<T>, user_limit: Option<NonZeroUsize>) {
    if let Some(limit) = user_limit {
        let allowed = limit.get().min(items.len());
        items.truncate(allowed);
    }
}

pub(super) fn limited_user_count(user_limit: Option<NonZeroUsize>, available: usize) -> usize {
    user_limit.map_or(available, |limit| limit.get().min(available))
}

pub(super) fn submission_plan(
    txs_per_block: NonZeroU64,
    ctx: &RunContext,
    available_accounts: usize,
) -> Result<(usize, Duration), DynError> {
    if available_accounts == 0 {
        return Err("transaction workload scheduled zero transactions".into());
    }

    let run_secs = ctx.run_duration().as_secs_f64();
    let block_secs = ctx
        .run_metrics()
        .block_interval_hint()
        .unwrap_or_else(|| ctx.run_duration())
        .as_secs_f64();

    let expected_blocks = run_secs / block_secs;
    let requested = (expected_blocks * txs_per_block.get() as f64)
        .floor()
        .clamp(0.0, u64::MAX as f64) as u64;

    let planned = requested.min(available_accounts as u64) as usize;
    if planned == 0 {
        return Err("transaction workload scheduled zero transactions".into());
    }

    let interval = Duration::from_secs_f64(run_secs / planned as f64);
    Ok((planned, interval))
}




════════════════════════════════════════════════
FILE: testing-framework/workflows/src/workloads/util.rs
────────────────────────────────────────────────
use std::sync::Arc;

use nomos_core::{
    block::Block,
    mantle::{
        AuthenticatedMantleTx as _, SignedMantleTx,
        ops::{Op, channel::MsgId},
    },
};
use testing_framework_core::scenario::{DynError, RunContext};

/// Scans a block and invokes the matcher for every operation until it returns
/// `Some(...)`. Returns `None` when no matching operation is found.
pub fn find_channel_op<F>(block: &Block<SignedMantleTx>, matcher: &mut F) -> Option<MsgId>
where
    F: FnMut(&Op) -> Option<MsgId>,
{
    for tx in block.transactions() {
        for op in &tx.mantle_tx().ops {
            if let Some(msg_id) = matcher(op) {
                return Some(msg_id);
            }
        }
    }

    None
}

pub async fn submit_transaction_via_cluster(
    ctx: &RunContext,
    tx: Arc<SignedMantleTx>,
) -> Result<(), DynError> {
    ctx.cluster_client()
        .try_all_clients(|client| {
            let tx = Arc::clone(&tx);
            Box::pin(async move {
                client
                    .submit_transaction(&tx)
                    .await
                    .map_err(|err| -> DynError { err.into() })
            })
        })
        .await
}




════════════════════════════════════════════════
FILE: testnet/cfgsync/src/bin/cfgsync-client.rs
────────────────────────────────────────────────
use std::{
    collections::{HashMap, HashSet},
    env, fs,
    net::Ipv4Addr,
    process,
    str::FromStr,
};

use cfgsync::{
    client::{FetchedConfig, get_config},
    server::ClientIp,
};
use nomos_executor::config::Config as ExecutorConfig;
use nomos_libp2p::PeerId;
use nomos_node::Config as ValidatorConfig;
use serde::{Serialize, de::DeserializeOwned};
use subnetworks_assignations::{MembershipCreator, MembershipHandler, SubnetworkId};

fn parse_ip(ip_str: &str) -> Ipv4Addr {
    ip_str.parse().unwrap_or_else(|_| {
        eprintln!("Invalid IP format, defaulting to 127.0.0.1");
        Ipv4Addr::LOCALHOST
    })
}

fn parse_assignations(raw: &serde_json::Value) -> Option<HashMap<SubnetworkId, HashSet<PeerId>>> {
    let assignations = raw
        .pointer("/da_network/membership/assignations")?
        .as_object()?;
    let mut result = HashMap::new();

    for (subnetwork, peers) in assignations {
        let subnetwork_id = SubnetworkId::from_str(subnetwork).ok()?;
        let mut members = HashSet::new();

        for peer in peers.as_array()? {
            if let Some(peer) = peer.as_str().and_then(|p| PeerId::from_str(p).ok()) {
                members.insert(peer);
            }
        }

        result.insert(subnetwork_id, members);
    }

    Some(result)
}

fn apply_da_assignations<
    Membership: MembershipCreator<Id = PeerId> + MembershipHandler<NetworkId = SubnetworkId>,
>(
    membership: &Membership,
    assignations: HashMap<SubnetworkId, HashSet<PeerId>>,
) -> Membership {
    let session_id = membership.session_id();
    membership.init(session_id, assignations)
}

async fn pull_to_file<Config, F>(
    payload: ClientIp,
    url: &str,
    config_file: &str,
    apply_membership: F,
) -> Result<(), String>
where
    Config: Serialize + DeserializeOwned,
    F: FnOnce(&mut Config, HashMap<SubnetworkId, HashSet<PeerId>>),
{
    let FetchedConfig { mut config, raw } = get_config::<Config>(payload, url).await?;

    if let Some(assignations) = parse_assignations(&raw) {
        apply_membership(&mut config, assignations);
    }

    let yaml = serde_yaml::to_string(&config)
        .map_err(|err| format!("Failed to serialize config to YAML: {err}"))?;

    fs::write(config_file, yaml).map_err(|err| format!("Failed to write config to file: {err}"))?;

    println!("Config saved to {config_file}");
    Ok(())
}

#[tokio::main]
async fn main() {
    let config_file_path = env::var("CFG_FILE_PATH").unwrap_or_else(|_| "config.yaml".to_owned());
    let server_addr =
        env::var("CFG_SERVER_ADDR").unwrap_or_else(|_| "http://127.0.0.1:4400".to_owned());
    let ip = parse_ip(&env::var("CFG_HOST_IP").unwrap_or_else(|_| "127.0.0.1".to_owned()));
    let identifier =
        env::var("CFG_HOST_IDENTIFIER").unwrap_or_else(|_| "unidentified-node".to_owned());

    let host_kind = env::var("CFG_HOST_KIND").unwrap_or_else(|_| "validator".to_owned());

    let network_port = env::var("CFG_NETWORK_PORT")
        .ok()
        .and_then(|v| v.parse().ok());
    let da_port = env::var("CFG_DA_PORT").ok().and_then(|v| v.parse().ok());
    let blend_port = env::var("CFG_BLEND_PORT").ok().and_then(|v| v.parse().ok());
    let api_port = env::var("CFG_API_PORT").ok().and_then(|v| v.parse().ok());
    let testing_http_port = env::var("CFG_TESTING_HTTP_PORT")
        .ok()
        .and_then(|v| v.parse().ok());

    let payload = ClientIp {
        ip,
        identifier,
        network_port,
        da_port,
        blend_port,
        api_port,
        testing_http_port,
    };

    let node_config_endpoint = match host_kind.as_str() {
        "executor" => format!("{server_addr}/executor"),
        _ => format!("{server_addr}/validator"),
    };

    let config_result = match host_kind.as_str() {
        "executor" => {
            pull_to_file::<ExecutorConfig, _>(
                payload,
                &node_config_endpoint,
                &config_file_path,
                |config, assignations| {
                    config.da_network.membership =
                        apply_da_assignations(&config.da_network.membership, assignations);
                },
            )
            .await
        }
        _ => {
            pull_to_file::<ValidatorConfig, _>(
                payload,
                &node_config_endpoint,
                &config_file_path,
                |config, assignations| {
                    config.da_network.membership =
                        apply_da_assignations(&config.da_network.membership, assignations);
                },
            )
            .await
        }
    };

    // Handle error if the config request fails
    if let Err(err) = config_result {
        eprintln!("Error: {err}");
        process::exit(1);
    }
}




════════════════════════════════════════════════
FILE: testnet/cfgsync/src/bin/cfgsync-server.rs
────────────────────────────────────────────────
use std::{path::PathBuf, process};

use cfgsync::server::{CfgSyncConfig, cfgsync_app};
use clap::Parser;
use tokio::net::TcpListener;

#[derive(Parser, Debug)]
#[command(about = "CfgSync")]
struct Args {
    config: PathBuf,
}

#[tokio::main]
async fn main() {
    let cli = Args::parse();

    let config = CfgSyncConfig::load_from_file(&cli.config).unwrap_or_else(|err| {
        eprintln!("{err}");
        process::exit(1);
    });

    let port = config.port;
    let app = cfgsync_app(config.into());

    println!("Server running on http://0.0.0.0:{port}");
    let listener = TcpListener::bind(&format!("0.0.0.0:{port}")).await.unwrap();

    axum::serve(listener, app).await.unwrap();
}




════════════════════════════════════════════════
FILE: testnet/cfgsync/src/client.rs
────────────────────────────────────────────────
use reqwest::{Client, Response};
use serde::de::DeserializeOwned;

use crate::server::ClientIp;

#[derive(Debug)]
pub struct FetchedConfig<Config> {
    pub config: Config,
    pub raw: serde_json::Value,
}

async fn deserialize_response<Config: DeserializeOwned>(
    response: Response,
) -> Result<FetchedConfig<Config>, String> {
    let body = response
        .text()
        .await
        .map_err(|error| format!("Failed to read response body: {error}"))?;
    let raw: serde_json::Value =
        serde_json::from_str(&body).map_err(|error| format!("Failed to parse body: {error}"))?;
    let mut json_deserializer = serde_json::Deserializer::from_str(&body);
    let config = serde_path_to_error::deserialize(&mut json_deserializer)
        .map_err(|error| format!("Failed to deserialize body: {error}, raw body: {body}"))?;

    Ok(FetchedConfig { config, raw })
}

pub async fn get_config<Config: DeserializeOwned>(
    payload: ClientIp,
    url: &str,
) -> Result<FetchedConfig<Config>, String> {
    let client = Client::new();

    let response = client
        .post(url)
        .json(&payload)
        .send()
        .await
        .map_err(|err| format!("Failed to send IP announcement: {err}"))?;

    if !response.status().is_success() {
        return Err(format!("Server error: {:?}", response.status()));
    }

    deserialize_response(response).await
}




════════════════════════════════════════════════
FILE: testnet/cfgsync/src/config.rs
────────────────────────────────────────────────
use std::{collections::HashMap, net::Ipv4Addr, str::FromStr as _};

use groth16::fr_to_bytes;
use hex;
use integration_configs::topology::configs::{
    GeneralConfig,
    api::GeneralApiConfig,
    blend::{GeneralBlendConfig, create_blend_configs},
    bootstrap::{SHORT_PROLONGED_BOOTSTRAP_PERIOD, create_bootstrap_configs},
    consensus::{
        ConsensusParams, GeneralConsensusConfig, ProviderInfo, create_consensus_configs,
        create_genesis_tx_with_declarations,
    },
    da::{DaParams, GeneralDaConfig, create_da_configs},
    network::{NetworkParams, create_network_configs},
    time::default_time_config,
    tracing::GeneralTracingConfig,
    wallet::WalletConfig,
};
use key_management_system::{
    backend::preload::PreloadKMSBackendSettings,
    keys::{Ed25519Key, Key, ZkKey},
};
use nomos_core::{
    mantle::GenesisTx as _,
    sdp::{Locator, ServiceType},
};
use nomos_libp2p::{Multiaddr, PeerId, Protocol, ed25519};
use nomos_tracing_service::{LoggerLayer, MetricsLayer, TracingLayer, TracingSettings};
use nomos_utils::net::get_available_udp_port;
use rand::{Rng as _, thread_rng};

const DEFAULT_LIBP2P_NETWORK_PORT: u16 = 3000;
const DEFAULT_DA_NETWORK_PORT: u16 = 3300;
const DEFAULT_BLEND_PORT: u16 = 3400;
const DEFAULT_API_PORT: u16 = 18080;

#[derive(Copy, Clone, Eq, PartialEq, Hash)]
pub enum HostKind {
    Validator,
    Executor,
}

#[derive(Eq, PartialEq, Hash, Clone)]
pub struct Host {
    pub kind: HostKind,
    pub ip: Ipv4Addr,
    pub identifier: String,
    pub network_port: u16,
    pub da_network_port: u16,
    pub blend_port: u16,
    pub api_port: u16,
    pub testing_http_port: u16,
}

#[derive(Clone, Copy)]
pub struct PortOverrides {
    pub network_port: Option<u16>,
    pub da_network_port: Option<u16>,
    pub blend_port: Option<u16>,
    pub api_port: Option<u16>,
    pub testing_http_port: Option<u16>,
}

impl Host {
    fn from_parts(kind: HostKind, ip: Ipv4Addr, identifier: String, ports: PortOverrides) -> Self {
        Self {
            kind,
            ip,
            identifier,
            network_port: ports.network_port.unwrap_or(DEFAULT_LIBP2P_NETWORK_PORT),
            da_network_port: ports.da_network_port.unwrap_or(DEFAULT_DA_NETWORK_PORT),
            blend_port: ports.blend_port.unwrap_or(DEFAULT_BLEND_PORT),
            api_port: ports.api_port.unwrap_or(DEFAULT_API_PORT),
            testing_http_port: ports.testing_http_port.unwrap_or(DEFAULT_API_PORT + 1),
        }
    }

    #[must_use]
    pub fn validator_from_ip(ip: Ipv4Addr, identifier: String, ports: PortOverrides) -> Self {
        Self::from_parts(HostKind::Validator, ip, identifier, ports)
    }

    #[must_use]
    pub fn executor_from_ip(ip: Ipv4Addr, identifier: String, ports: PortOverrides) -> Self {
        Self::from_parts(HostKind::Executor, ip, identifier, ports)
    }
}

#[must_use]
pub fn create_node_configs(
    consensus_params: &ConsensusParams,
    da_params: &DaParams,
    tracing_settings: &TracingSettings,
    wallet_config: &WalletConfig,
    ids: Option<Vec<[u8; 32]>>,
    da_ports: Option<Vec<u16>>,
    blend_ports: Option<Vec<u16>>,
    hosts: Vec<Host>,
) -> HashMap<Host, GeneralConfig> {
    let mut hosts = hosts;
    hosts.sort_by_key(|host| {
        let index = host
            .identifier
            .rsplit('-')
            .next()
            .and_then(|raw| raw.parse::<usize>().ok())
            .unwrap_or(0);
        let kind = match host.kind {
            HostKind::Validator => 0,
            HostKind::Executor => 1,
        };
        (kind, index)
    });

    assert_eq!(
        hosts.len(),
        consensus_params.n_participants,
        "host count must match consensus participants"
    );

    let ids = ids.unwrap_or_else(|| {
        let mut generated = vec![[0; 32]; consensus_params.n_participants];
        for id in &mut generated {
            thread_rng().fill(id);
        }
        generated
    });
    assert_eq!(
        ids.len(),
        consensus_params.n_participants,
        "pre-generated ids must match participant count"
    );

    let ports = da_ports.unwrap_or_else(|| {
        (0..consensus_params.n_participants)
            .map(|_| get_available_udp_port().unwrap())
            .collect()
    });
    assert_eq!(
        ports.len(),
        consensus_params.n_participants,
        "da port list must match participant count"
    );

    let blend_ports = blend_ports.unwrap_or_else(|| hosts.iter().map(|h| h.blend_port).collect());
    assert_eq!(
        blend_ports.len(),
        consensus_params.n_participants,
        "blend port list must match participant count"
    );

    let mut consensus_configs = create_consensus_configs(&ids, consensus_params, wallet_config);
    let bootstrap_configs = create_bootstrap_configs(&ids, SHORT_PROLONGED_BOOTSTRAP_PERIOD);
    let da_configs = create_da_configs(&ids, da_params, &ports);
    let network_configs = create_network_configs(&ids, &NetworkParams::default());
    let blend_configs = create_blend_configs(&ids, &blend_ports);
    let api_configs = hosts
        .iter()
        .map(|host| GeneralApiConfig {
            address: format!("0.0.0.0:{}", host.api_port).parse().unwrap(),
            testing_http_address: format!("0.0.0.0:{}", host.testing_http_port)
                .parse()
                .unwrap(),
        })
        .collect::<Vec<_>>();
    let mut configured_hosts = HashMap::new();

    let initial_peer_templates: Vec<Vec<Multiaddr>> = network_configs
        .iter()
        .map(|cfg| cfg.backend.initial_peers.clone())
        .collect();
    let original_network_ports: Vec<u16> = network_configs
        .iter()
        .map(|cfg| cfg.backend.inner.port)
        .collect();
    let peer_ids: Vec<PeerId> = ids
        .iter()
        .map(|bytes| {
            let mut key_bytes = *bytes;
            let secret =
                ed25519::SecretKey::try_from_bytes(&mut key_bytes).expect("valid ed25519 key");
            PeerId::from_public_key(&ed25519::Keypair::from(secret).public().into())
        })
        .collect();

    let host_network_init_peers = rewrite_initial_peers(
        &initial_peer_templates,
        &original_network_ports,
        &hosts,
        &peer_ids,
    );

    let providers = create_providers(&hosts, &consensus_configs, &blend_configs, &da_configs);

    // Update genesis TX to contain Blend and DA providers.
    let ledger_tx = consensus_configs[0]
        .genesis_tx
        .mantle_tx()
        .ledger_tx
        .clone();
    let genesis_tx = create_genesis_tx_with_declarations(ledger_tx, providers);
    for c in &mut consensus_configs {
        c.genesis_tx = genesis_tx.clone();
    }

    // Set Blend and DA keys in KMS of each node config.
    let kms_configs = create_kms_configs(&blend_configs, &da_configs);

    for (i, host) in hosts.into_iter().enumerate() {
        let consensus_config = consensus_configs[i].clone();
        let api_config = api_configs[i].clone();

        // DA Libp2p network config.
        let mut da_config = da_configs[i].clone();
        da_config.listening_address = Multiaddr::from_str(&format!(
            "/ip4/0.0.0.0/udp/{}/quic-v1",
            host.da_network_port,
        ))
        .unwrap();
        if matches!(host.kind, HostKind::Validator) {
            da_config.policy_settings.min_dispersal_peers = 0;
        }

        // Libp2p network config.
        let mut network_config = network_configs[i].clone();
        network_config.backend.inner.host = Ipv4Addr::from_str("0.0.0.0").unwrap();
        network_config.backend.inner.port = host.network_port;
        network_config.backend.initial_peers = host_network_init_peers[i].clone();
        network_config.backend.inner.nat_config = nomos_libp2p::NatSettings::Static {
            external_address: Multiaddr::from_str(&format!(
                "/ip4/{}/udp/{}/quic-v1",
                host.ip, host.network_port
            ))
            .unwrap(),
        };

        // Blend network config.
        let mut blend_config = blend_configs[i].clone();
        blend_config.backend_core.listening_address =
            Multiaddr::from_str(&format!("/ip4/0.0.0.0/udp/{}/quic-v1", host.blend_port)).unwrap();

        // Tracing config.
        let tracing_config =
            update_tracing_identifier(tracing_settings.clone(), host.identifier.clone());

        // Time config
        let time_config = default_time_config();

        configured_hosts.insert(
            host.clone(),
            GeneralConfig {
                consensus_config,
                bootstrapping_config: bootstrap_configs[i].clone(),
                da_config,
                network_config,
                blend_config,
                api_config,
                tracing_config,
                time_config,
                kms_config: kms_configs[i].clone(),
            },
        );
    }

    configured_hosts
}

fn create_providers(
    hosts: &[Host],
    consensus_configs: &[GeneralConsensusConfig],
    blend_configs: &[GeneralBlendConfig],
    da_configs: &[GeneralDaConfig],
) -> Vec<ProviderInfo> {
    let mut providers: Vec<_> = da_configs
        .iter()
        .enumerate()
        .map(|(i, da_conf)| ProviderInfo {
            service_type: ServiceType::DataAvailability,
            provider_sk: da_conf.signer.clone(),
            zk_sk: da_conf.secret_zk_key.clone(),
            locator: Locator(
                Multiaddr::from_str(&format!(
                    "/ip4/{}/udp/{}/quic-v1",
                    hosts[i].ip, hosts[i].da_network_port
                ))
                .unwrap(),
            ),
            note: consensus_configs[0].da_notes[i].clone(),
        })
        .collect();
    providers.extend(blend_configs.iter().enumerate().map(|(i, blend_conf)| {
        ProviderInfo {
            service_type: ServiceType::BlendNetwork,
            provider_sk: blend_conf.signer.clone(),
            zk_sk: blend_conf.secret_zk_key.clone(),
            locator: Locator(
                Multiaddr::from_str(&format!(
                    "/ip4/{}/udp/{}/quic-v1",
                    hosts[i].ip, hosts[i].blend_port
                ))
                .unwrap(),
            ),
            note: consensus_configs[0].blend_notes[i].clone(),
        }
    }));

    providers
}

fn rewrite_initial_peers(
    templates: &[Vec<Multiaddr>],
    original_ports: &[u16],
    hosts: &[Host],
    peer_ids: &[PeerId],
) -> Vec<Vec<Multiaddr>> {
    templates
        .iter()
        .enumerate()
        .map(|(node_idx, peers)| {
            peers
                .iter()
                .filter_map(|addr| find_matching_host(addr, original_ports))
                .filter(|&peer_idx| peer_idx != node_idx)
                .map(|peer_idx| {
                    Multiaddr::from_str(&format!(
                        "/ip4/{}/udp/{}/quic-v1/p2p/{}",
                        hosts[peer_idx].ip, hosts[peer_idx].network_port, peer_ids[peer_idx]
                    ))
                    .expect("valid peer multiaddr")
                })
                .collect()
        })
        .collect()
}

fn find_matching_host(addr: &Multiaddr, original_ports: &[u16]) -> Option<usize> {
    extract_udp_port(addr).and_then(|port| {
        original_ports
            .iter()
            .position(|candidate| *candidate == port)
    })
}

fn extract_udp_port(addr: &Multiaddr) -> Option<u16> {
    addr.iter().find_map(|protocol| {
        if let Protocol::Udp(port) = protocol {
            Some(port)
        } else {
            None
        }
    })
}

fn update_tracing_identifier(
    settings: TracingSettings,
    identifier: String,
) -> GeneralTracingConfig {
    GeneralTracingConfig {
        tracing_settings: TracingSettings {
            logger: match settings.logger {
                LoggerLayer::Loki(mut config) => {
                    config.host_identifier.clone_from(&identifier);
                    LoggerLayer::Loki(config)
                }
                other => other,
            },
            tracing: match settings.tracing {
                TracingLayer::Otlp(mut config) => {
                    config.service_name.clone_from(&identifier);
                    TracingLayer::Otlp(config)
                }
                other @ TracingLayer::None => other,
            },
            filter: settings.filter,
            metrics: match settings.metrics {
                MetricsLayer::Otlp(mut config) => {
                    config.host_identifier = identifier;
                    MetricsLayer::Otlp(config)
                }
                other @ MetricsLayer::None => other,
            },
            console: settings.console,
            level: settings.level,
        },
    }
}

fn create_kms_configs(
    blend_configs: &[GeneralBlendConfig],
    da_configs: &[GeneralDaConfig],
) -> Vec<PreloadKMSBackendSettings> {
    da_configs
        .iter()
        .zip(blend_configs.iter())
        .map(|(da_conf, blend_conf)| PreloadKMSBackendSettings {
            keys: [
                (
                    hex::encode(blend_conf.signer.verifying_key().as_bytes()),
                    Key::Ed25519(Ed25519Key::new(blend_conf.signer.clone())),
                ),
                (
                    hex::encode(fr_to_bytes(
                        &blend_conf.secret_zk_key.to_public_key().into_inner(),
                    )),
                    Key::Zk(ZkKey::new(blend_conf.secret_zk_key.clone())),
                ),
                (
                    hex::encode(da_conf.signer.verifying_key().as_bytes()),
                    Key::Ed25519(Ed25519Key::new(da_conf.signer.clone())),
                ),
                (
                    hex::encode(fr_to_bytes(
                        &da_conf.secret_zk_key.to_public_key().into_inner(),
                    )),
                    Key::Zk(ZkKey::new(da_conf.secret_zk_key.clone())),
                ),
            ]
            .into(),
        })
        .collect()
}

#[cfg(test)]
mod cfgsync_tests {
    use std::{net::Ipv4Addr, num::NonZero, str::FromStr as _, time::Duration};

    use integration_configs::topology::configs::{
        consensus::ConsensusParams, da::DaParams, wallet::WalletConfig,
    };
    use nomos_da_network_core::swarm::{
        DAConnectionMonitorSettings, DAConnectionPolicySettings, ReplicationConfig,
    };
    use nomos_libp2p::{Multiaddr, Protocol};
    use nomos_tracing_service::{
        ConsoleLayer, FilterLayer, LoggerLayer, MetricsLayer, TracingLayer, TracingSettings,
    };
    use tracing::Level;

    use super::{Host, HostKind, create_node_configs};

    #[test]
    fn basic_ip_list() {
        let hosts = (0..10)
            .map(|i| Host {
                kind: HostKind::Validator,
                ip: Ipv4Addr::from_str(&format!("10.1.1.{i}")).unwrap(),
                identifier: "node".into(),
                network_port: 3000,
                da_network_port: 4044,
                blend_port: 5000,
                api_port: 18080,
                testing_http_port: 18081,
            })
            .collect();

        let configs = create_node_configs(
            &ConsensusParams {
                n_participants: 10,
                security_param: NonZero::new(10).unwrap(),
                active_slot_coeff: 0.9,
            },
            &DaParams {
                subnetwork_size: 2,
                dispersal_factor: 1,
                num_samples: 1,
                num_subnets: 2,
                old_blobs_check_interval: Duration::from_secs(5),
                blobs_validity_duration: Duration::from_secs(u64::MAX),
                global_params_path: String::new(),
                policy_settings: DAConnectionPolicySettings::default(),
                monitor_settings: DAConnectionMonitorSettings::default(),
                balancer_interval: Duration::ZERO,
                redial_cooldown: Duration::ZERO,
                replication_settings: ReplicationConfig {
                    seen_message_cache_size: 0,
                    seen_message_ttl: Duration::ZERO,
                },
                subnets_refresh_interval: Duration::from_secs(1),
                retry_shares_limit: 1,
                retry_commitments_limit: 1,
            },
            &TracingSettings {
                logger: LoggerLayer::None,
                tracing: TracingLayer::None,
                filter: FilterLayer::None,
                metrics: MetricsLayer::None,
                console: ConsoleLayer::None,
                level: Level::DEBUG,
            },
            &WalletConfig::default(),
            None,
            None,
            None,
            hosts,
        );

        for (host, config) in &configs {
            let network_port = config.network_config.backend.inner.port;
            let da_network_port = extract_port(&config.da_config.listening_address);
            let blend_port = extract_port(&config.blend_config.backend_core.listening_address);

            assert_eq!(network_port, host.network_port);
            assert_eq!(da_network_port, host.da_network_port);
            assert_eq!(blend_port, host.blend_port);
        }
    }

    fn extract_port(multiaddr: &Multiaddr) -> u16 {
        multiaddr
            .iter()
            .find_map(|protocol| match protocol {
                Protocol::Udp(port) => Some(port),
                _ => None,
            })
            .unwrap()
    }
}




════════════════════════════════════════════════
FILE: testnet/cfgsync/src/lib.rs
────────────────────────────────────────────────
pub mod client;
pub mod config;
pub mod repo;
pub mod server;




════════════════════════════════════════════════
FILE: testnet/cfgsync/src/repo.rs
────────────────────────────────────────────────
use std::{
    collections::HashMap,
    sync::{Arc, Mutex},
    time::Duration,
};

use integration_configs::topology::configs::{
    GeneralConfig, consensus::ConsensusParams, da::DaParams, wallet::WalletConfig,
};
use nomos_tracing_service::TracingSettings;
use tokio::{sync::oneshot::Sender, time::timeout};

use crate::{
    config::{Host, create_node_configs},
    server::CfgSyncConfig,
};

pub enum RepoResponse {
    Config(Box<GeneralConfig>),
    Timeout,
}

pub struct ConfigRepo {
    waiting_hosts: Mutex<HashMap<Host, Sender<RepoResponse>>>,
    n_hosts: usize,
    consensus_params: ConsensusParams,
    da_params: DaParams,
    tracing_settings: TracingSettings,
    wallet_config: WalletConfig,
    timeout_duration: Duration,
    ids: Option<Vec<[u8; 32]>>,
    da_ports: Option<Vec<u16>>,
    blend_ports: Option<Vec<u16>>,
}

impl From<CfgSyncConfig> for Arc<ConfigRepo> {
    fn from(config: CfgSyncConfig) -> Self {
        let consensus_params = config.to_consensus_params();
        let da_params = config.to_da_params();
        let tracing_settings = config.to_tracing_settings();
        let wallet_config = config.wallet_config();
        let ids = config.ids;
        let da_ports = config.da_ports;
        let blend_ports = config.blend_ports;

        ConfigRepo::new(
            config.n_hosts,
            consensus_params,
            da_params,
            tracing_settings,
            wallet_config,
            ids,
            da_ports,
            blend_ports,
            Duration::from_secs(config.timeout),
        )
    }
}

impl ConfigRepo {
    #[must_use]
    pub fn new(
        n_hosts: usize,
        consensus_params: ConsensusParams,
        da_params: DaParams,
        tracing_settings: TracingSettings,
        wallet_config: WalletConfig,
        ids: Option<Vec<[u8; 32]>>,
        da_ports: Option<Vec<u16>>,
        blend_ports: Option<Vec<u16>>,
        timeout_duration: Duration,
    ) -> Arc<Self> {
        let repo = Arc::new(Self {
            waiting_hosts: Mutex::new(HashMap::new()),
            n_hosts,
            consensus_params,
            da_params,
            tracing_settings,
            wallet_config,
            ids,
            da_ports,
            blend_ports,
            timeout_duration,
        });

        let repo_clone = Arc::clone(&repo);
        tokio::spawn(async move {
            repo_clone.run().await;
        });

        repo
    }

    pub fn register(&self, host: Host, reply_tx: Sender<RepoResponse>) {
        let mut waiting_hosts = self.waiting_hosts.lock().unwrap();
        waiting_hosts.insert(host, reply_tx);
    }

    async fn run(&self) {
        let timeout_duration = self.timeout_duration;

        if timeout(timeout_duration, self.wait_for_hosts()).await == Ok(()) {
            println!("All hosts have announced their IPs");

            let mut waiting_hosts = self.waiting_hosts.lock().unwrap();
            let hosts = waiting_hosts.keys().cloned().collect();

            let configs = create_node_configs(
                &self.consensus_params,
                &self.da_params,
                &self.tracing_settings,
                &self.wallet_config,
                self.ids.clone(),
                self.da_ports.clone(),
                self.blend_ports.clone(),
                hosts,
            );

            for (host, sender) in waiting_hosts.drain() {
                let config = configs.get(&host).expect("host should have a config");
                let _ = sender.send(RepoResponse::Config(Box::new(config.to_owned())));
            }
        } else {
            println!("Timeout: Not all hosts announced within the time limit");

            let mut waiting_hosts = self.waiting_hosts.lock().unwrap();
            for (_, sender) in waiting_hosts.drain() {
                let _ = sender.send(RepoResponse::Timeout);
            }
        }
    }

    async fn wait_for_hosts(&self) {
        loop {
            if self.waiting_hosts.lock().unwrap().len() >= self.n_hosts {
                break;
            }
            tokio::time::sleep(Duration::from_secs(1)).await;
        }
    }
}




════════════════════════════════════════════════
FILE: testnet/cfgsync/src/server.rs
────────────────────────────────────────────────
use std::{fs, net::Ipv4Addr, num::NonZero, path::PathBuf, sync::Arc, time::Duration};

use axum::{Json, Router, extract::State, http::StatusCode, response::IntoResponse, routing::post};
use integration_configs::{
    nodes::{executor::create_executor_config, validator::create_validator_config},
    topology::configs::{consensus::ConsensusParams, da::DaParams, wallet::WalletConfig},
};
use nomos_da_network_core::swarm::{
    DAConnectionMonitorSettings, DAConnectionPolicySettings, ReplicationConfig,
};
use nomos_tracing_service::TracingSettings;
use nomos_utils::bounded_duration::{MinimalBoundedDuration, SECOND};
use serde::{Deserialize, Serialize};
use serde_json::json;
use serde_with::serde_as;
use subnetworks_assignations::MembershipHandler;
use tokio::sync::oneshot::channel;

use crate::{
    config::{Host, PortOverrides},
    repo::{ConfigRepo, RepoResponse},
};

#[serde_as]
#[derive(Debug, Deserialize)]
pub struct CfgSyncConfig {
    pub port: u16,
    pub n_hosts: usize,
    pub timeout: u64,

    // ConsensusConfig related parameters
    pub security_param: NonZero<u32>,
    pub active_slot_coeff: f64,
    pub wallet: WalletConfig,
    #[serde(default)]
    pub ids: Option<Vec<[u8; 32]>>,
    #[serde(default)]
    pub da_ports: Option<Vec<u16>>,
    #[serde(default)]
    pub blend_ports: Option<Vec<u16>>,

    // DaConfig related parameters
    pub subnetwork_size: usize,
    pub dispersal_factor: usize,
    pub num_samples: u16,
    pub num_subnets: u16,
    #[serde_as(as = "MinimalBoundedDuration<0, SECOND>")]
    pub old_blobs_check_interval: Duration,
    #[serde_as(as = "MinimalBoundedDuration<0, SECOND>")]
    pub blobs_validity_duration: Duration,
    pub global_params_path: String,
    pub min_dispersal_peers: usize,
    pub min_replication_peers: usize,
    #[serde_as(as = "MinimalBoundedDuration<0, SECOND>")]
    pub monitor_failure_time_window: Duration,
    #[serde_as(as = "MinimalBoundedDuration<0, SECOND>")]
    pub balancer_interval: Duration,
    pub replication_settings: ReplicationConfig,
    pub retry_shares_limit: usize,
    pub retry_commitments_limit: usize,

    // Tracing params
    pub tracing_settings: TracingSettings,
}

impl CfgSyncConfig {
    pub fn load_from_file(file_path: &PathBuf) -> Result<Self, String> {
        let config_content = fs::read_to_string(file_path)
            .map_err(|err| format!("Failed to read config file: {err}"))?;
        serde_yaml::from_str(&config_content)
            .map_err(|err| format!("Failed to parse config file: {err}"))
    }

    #[must_use]
    pub const fn to_consensus_params(&self) -> ConsensusParams {
        ConsensusParams {
            n_participants: self.n_hosts,
            security_param: self.security_param,
            active_slot_coeff: self.active_slot_coeff,
        }
    }

    #[must_use]
    pub fn to_da_params(&self) -> DaParams {
        DaParams {
            subnetwork_size: self.subnetwork_size,
            dispersal_factor: self.dispersal_factor,
            num_samples: self.num_samples,
            num_subnets: self.num_subnets,
            old_blobs_check_interval: self.old_blobs_check_interval,
            blobs_validity_duration: self.blobs_validity_duration,
            global_params_path: self.global_params_path.clone(),
            policy_settings: DAConnectionPolicySettings {
                min_dispersal_peers: self.min_dispersal_peers,
                min_replication_peers: self.min_replication_peers,
                max_dispersal_failures: 3,
                max_sampling_failures: 3,
                max_replication_failures: 3,
                malicious_threshold: 10,
            },
            monitor_settings: DAConnectionMonitorSettings {
                failure_time_window: self.monitor_failure_time_window,
                ..Default::default()
            },
            balancer_interval: self.balancer_interval,
            redial_cooldown: Duration::ZERO,
            replication_settings: self.replication_settings,
            subnets_refresh_interval: Duration::from_secs(30),
            retry_shares_limit: self.retry_shares_limit,
            retry_commitments_limit: self.retry_commitments_limit,
        }
    }

    #[must_use]
    pub fn to_tracing_settings(&self) -> TracingSettings {
        self.tracing_settings.clone()
    }

    #[must_use]
    pub fn wallet_config(&self) -> WalletConfig {
        self.wallet.clone()
    }
}

#[derive(Serialize, Deserialize)]
pub struct ClientIp {
    pub ip: Ipv4Addr,
    pub identifier: String,
    #[serde(default)]
    pub network_port: Option<u16>,
    #[serde(default)]
    pub da_port: Option<u16>,
    #[serde(default)]
    pub blend_port: Option<u16>,
    #[serde(default)]
    pub api_port: Option<u16>,
    #[serde(default)]
    pub testing_http_port: Option<u16>,
}

async fn validator_config(
    State(config_repo): State<Arc<ConfigRepo>>,
    Json(payload): Json<ClientIp>,
) -> impl IntoResponse {
    let ClientIp {
        ip,
        identifier,
        network_port,
        da_port,
        blend_port,
        api_port,
        testing_http_port,
    } = payload;
    let ports = PortOverrides {
        network_port,
        da_network_port: da_port,
        blend_port,
        api_port,
        testing_http_port,
    };

    let (reply_tx, reply_rx) = channel();
    config_repo.register(Host::validator_from_ip(ip, identifier, ports), reply_tx);

    (reply_rx.await).map_or_else(
        |_| (StatusCode::INTERNAL_SERVER_ERROR, "Error receiving config").into_response(),
        |config_response| match config_response {
            RepoResponse::Config(config) => {
                let config = create_validator_config(*config);
                let mut value =
                    serde_json::to_value(&config).expect("validator config should serialize");
                inject_defaults(&mut value);
                override_api_ports(&mut value, &ports);
                inject_da_assignations(&mut value, &config.da_network.membership);
                override_min_session_members(&mut value);
                (StatusCode::OK, Json(value)).into_response()
            }
            RepoResponse::Timeout => (StatusCode::REQUEST_TIMEOUT).into_response(),
        },
    )
}

async fn executor_config(
    State(config_repo): State<Arc<ConfigRepo>>,
    Json(payload): Json<ClientIp>,
) -> impl IntoResponse {
    let ClientIp {
        ip,
        identifier,
        network_port,
        da_port,
        blend_port,
        api_port,
        testing_http_port,
    } = payload;
    let ports = PortOverrides {
        network_port,
        da_network_port: da_port,
        blend_port,
        api_port,
        testing_http_port,
    };

    let (reply_tx, reply_rx) = channel();
    config_repo.register(Host::executor_from_ip(ip, identifier, ports), reply_tx);

    (reply_rx.await).map_or_else(
        |_| (StatusCode::INTERNAL_SERVER_ERROR, "Error receiving config").into_response(),
        |config_response| match config_response {
            RepoResponse::Config(config) => {
                let config = create_executor_config(*config);
                let mut value =
                    serde_json::to_value(&config).expect("executor config should serialize");
                inject_defaults(&mut value);
                override_api_ports(&mut value, &ports);
                inject_da_assignations(&mut value, &config.da_network.membership);
                override_min_session_members(&mut value);
                (StatusCode::OK, Json(value)).into_response()
            }
            RepoResponse::Timeout => (StatusCode::REQUEST_TIMEOUT).into_response(),
        },
    )
}

pub fn cfgsync_app(config_repo: Arc<ConfigRepo>) -> Router {
    Router::new()
        .route("/validator", post(validator_config))
        .route("/executor", post(executor_config))
        .with_state(config_repo)
}

fn override_api_ports(config: &mut serde_json::Value, ports: &PortOverrides) {
    if let Some(api_port) = ports.api_port {
        if let Some(address) = config.pointer_mut("/http/backend_settings/address") {
            *address = json!(format!("0.0.0.0:{api_port}"));
        }
    }

    if let Some(testing_port) = ports.testing_http_port {
        if let Some(address) = config.pointer_mut("/testing_http/backend_settings/address") {
            *address = json!(format!("0.0.0.0:{testing_port}"));
        }
    }
}

fn inject_da_assignations(
    config: &mut serde_json::Value,
    membership: &nomos_node::NomosDaMembership,
) {
    let assignations: std::collections::HashMap<String, Vec<String>> = membership
        .subnetworks()
        .into_iter()
        .map(|(subnet_id, members)| {
            (
                subnet_id.to_string(),
                members.into_iter().map(|peer| peer.to_string()).collect(),
            )
        })
        .collect();

    if let Some(membership) = config.pointer_mut("/da_network/membership") {
        if let Some(map) = membership.as_object_mut() {
            map.insert("assignations".to_string(), serde_json::json!(assignations));
        }
    }
}

fn override_min_session_members(config: &mut serde_json::Value) {
    if let Some(value) = config.pointer_mut("/da_network/min_session_members") {
        *value = serde_json::json!(1);
    }
}

fn inject_defaults(config: &mut serde_json::Value) {
    if let Some(cryptarchia) = config
        .get_mut("cryptarchia")
        .and_then(|v| v.as_object_mut())
    {
        let bootstrap = cryptarchia
            .entry("bootstrap")
            .or_insert_with(|| serde_json::json!({}));
        if let Some(bootstrap_map) = bootstrap.as_object_mut() {
            bootstrap_map
                .entry("ibd")
                .or_insert_with(|| serde_json::json!({ "peers": [], "delay_before_new_download": { "secs": 10, "nanos": 0 } }));
        }

        cryptarchia
            .entry("network_adapter_settings")
            .or_insert_with(|| serde_json::json!({ "topic": "/cryptarchia/proto" }));
        cryptarchia.entry("sync").or_insert_with(|| {
            serde_json::json!({
                "orphan": { "max_orphan_cache_size": 5 }
            })
        });
    }
}




════════════════════════════════════════════════
FILE: tests/workflows/src/bin/compose_runner_ci.rs
────────────────────────────────────────────────
use std::time::Duration;

use anyhow::{Context as _, Result};
use testing_framework_core::scenario::{Deployer as _, Runner, ScenarioBuilder};
use testing_framework_runner_compose::{ComposeRunner, ComposeRunnerError};
use tests_workflows::{
    ChaosBuilderExt as _, ScenarioBuilderExt as _, expectations::ConsensusLiveness,
};

const RUN_DURATION: Duration = Duration::from_secs(120);
const VALIDATORS: usize = 2;
const EXECUTORS: usize = 2;
const MIXED_TXS_PER_BLOCK: u64 = 5;
const TOTAL_WALLETS: usize = 64;
const TRANSACTION_WALLETS: usize = 8;

#[tokio::main]
async fn main() -> Result<()> {
    let topology = ScenarioBuilder::with_node_counts(VALIDATORS, EXECUTORS)
        .enable_node_control()
        .chaos_random_restart()
        .min_delay(Duration::from_secs(45))
        .max_delay(Duration::from_secs(75))
        .target_cooldown(Duration::from_secs(120))
        .apply()
        .topology()
        .validators(VALIDATORS)
        .executors(EXECUTORS)
        .network_star()
        .apply();

    let workloads = topology
        .wallets(TOTAL_WALLETS)
        .transactions()
        .rate(MIXED_TXS_PER_BLOCK)
        .users(TRANSACTION_WALLETS)
        .apply()
        .da()
        .channel_rate(1)
        .blob_rate(1)
        .apply();

    let lag_allowance = 2 + (VALIDATORS + EXECUTORS) as u64;
    let mut plan = workloads
        .with_expectation(ConsensusLiveness::default().with_lag_allowance(lag_allowance))
        .with_run_duration(RUN_DURATION)
        .build();

    let deployer = ComposeRunner::new().with_readiness(false);
    let runner: Runner = match deployer.deploy(&plan).await {
        Ok(runner) => runner,
        Err(ComposeRunnerError::DockerUnavailable) => {
            anyhow::bail!("Docker is required for compose runner CI binary");
        }
        Err(err) => return Err(err.into()),
    };

    if !runner.context().telemetry().is_configured() {
        anyhow::bail!("compose runner should expose prometheus metrics");
    }

    runner
        .run(&mut plan)
        .await
        .context("compose scenario execution failed")?;

    Ok(())
}




════════════════════════════════════════════════
FILE: tests/workflows/src/lib.rs
────────────────────────────────────────────────
use testing_framework_core::scenario::Metrics;
pub use testing_framework_workflows::{
    builder::{ChaosBuilderExt, ScenarioBuilderExt},
    expectations, util, workloads,
};

/// Metrics are currently disabled in this branch; return a stub handle.
#[must_use]
pub const fn configure_prometheus_metrics() -> Metrics {
    Metrics::empty()
}




════════════════════════════════════════════════
FILE: tests/workflows/tests/compose_runner.rs
────────────────────────────────────────────────
use std::time::Duration;

use serial_test::serial;
use testing_framework_core::scenario::{Deployer as _, Runner, ScenarioBuilder};
use testing_framework_runner_compose::{ComposeRunner, ComposeRunnerError};
use tests_workflows::{ChaosBuilderExt as _, ScenarioBuilderExt as _};

const VALIDATORS: usize = 1;
const EXECUTORS: usize = 1;
const RUN_DURATION: Duration = Duration::from_secs(60);
const MIXED_TXS_PER_BLOCK: u64 = 5;
const TOTAL_WALLETS: usize = 64;
const TRANSACTION_WALLETS: usize = 8;

#[tokio::test]
#[serial]
async fn compose_runner_mixed_workloads() {
    run_compose_case(VALIDATORS, EXECUTORS).await;
}

async fn run_compose_case(validators: usize, executors: usize) {
    println!(
        "running compose chaos test with {validators} validator(s) and {executors} executor(s)"
    );

    let mut plan = ScenarioBuilder::with_node_counts(validators, executors)
        .enable_node_control()
        .chaos_random_restart()
        // Keep chaos restarts outside the test run window to avoid crash loops on restart.
        .min_delay(Duration::from_secs(120))
        .max_delay(Duration::from_secs(180))
        .target_cooldown(Duration::from_secs(240))
        .apply()
        .topology()
        .network_star()
        .validators(validators)
        .executors(executors)
        .apply()
        .wallets(TOTAL_WALLETS)
        .transactions()
        .rate(MIXED_TXS_PER_BLOCK)
        .users(TRANSACTION_WALLETS)
        .apply()
        .da()
        .channel_rate(1)
        .blob_rate(1)
        .apply()
        .with_run_duration(RUN_DURATION)
        .expect_consensus_liveness()
        .build();

    let deployer = ComposeRunner::new();
    let runner: Runner = match deployer.deploy(&plan).await {
        Ok(runner) => runner,
        Err(ComposeRunnerError::DockerUnavailable) => {
            eprintln!("Skipping compose_runner_mixed_workloads: Docker is unavailable");
            return;
        }
        Err(err) => panic!("scenario deployment: {err}"),
    };
    let context = runner.context();
    assert!(
        context.telemetry().is_configured(),
        "compose runner should expose prometheus metrics"
    );

    let _handle = runner.run(&mut plan).await.expect("scenario executed");
}




════════════════════════════════════════════════
FILE: tests/workflows/tests/framework_demo.rs
────────────────────────────────────────────────
//! # Test Framework Demo Topology
//!
//! The demo showcases how the testing framework composes deployments:
//!
//! ```text
//! ┌────────────────────────────────────────────────────────┐
//! │ ScenarioBuilder                                        │
//! │   ├─ plan() ───────────▶ Runner::new(plan)             │
//! │   ├─ enable_node_control → chaos workloads             │
//! │   ├─ topology() → network layout → validators/executors│
//! │   └─ workloads (transactions + DA)                     │
//! └────────────────────────────────────────────────────────┘
//!
//! ┌─────────────────────────────┐
//! │ Deployers                   │
//! │   ├─ LocalDeployer          │
//! │   ├─ ComposeRunner          │
//! │   └─ K8sRunner              │
//! │                             │
//! │ Runner                      │
//! │   ├─ execute plan           │
//! │   ├─ telemetry              │
//! │   └─ control handles        │
//! └─────────────────────────────┘
//! ```
//!
//! Component responsibilities:
//!
//! ┌─────────────────────────────────────────────────────────────┐
//! │ Component    │ Role                                         │
//! │--------------│----------------------------------------------│
//! │ Workloads    │ drive traffic (tx, DA blobs, chaos restarts) │
//! │ Expectations │ assert cluster health                       │
//! │ Deployers    │ provision env (host, Docker, k8s)            │
//! │ Runner       │ drives workloads/expectations, telemetry     │
//! └─────────────────────────────────────────────────────────────┘
//!
//! Execution flow:
//!
//! ```text
//! ┌──────┐     ┌───────────────┐     ┌──────────────┐     ┌────────┐
//! │ 1.   │ ─▶ │ 2. Workloads/ │ ─▶ │ 3. Deployers  │ ─▶ │ Runner │
//! │ Plan │     │ Expectations  │     │ Environment   │     │        │
//! └──────┘     └───────────────┘     └──────────────┘     └────────┘
//!                                                           ├─ orchestrate
//!                                                           ├─ telemetry
//!                                                           └─ control
//! ```
//!
//! Cluster interaction:
//!
//! ```text
//!           ┌───────────────┐
//!           │ Deployers     │  provision VMs/containers
//!           └──────┬────────┘
//!                  │
//!        ┌─────────▼─────────┐
//!        │ Cluster Nodes     │ (validators, executors)
//!        └───────┬───────────┘
//!                │
//!      ┌─────────▼──────────┐
//!      │ Runner             │  command/control + telemetry
//!      └──────┬────────┬────┘
//!             │        │
//!        workloads   expectations
//! ```
//!
//! Each runner consumes the same scenario plan; only the deployment backend
//! changes. `full_plan` shows the high-level builder-style DSL, while
//! `explicit_workload_plan` wires the same components explicitly through
//! `with_workload` calls.

use std::{num::NonZeroUsize, time::Duration};

use testing_framework_core::scenario::{
    Deployer as _, NodeControlCapability, Runner, ScenarioBuilder,
};
use testing_framework_runner_compose::ComposeRunner;
use testing_framework_runner_k8s::K8sRunner;
use testing_framework_runner_local::LocalDeployer;
use testing_framework_workflows::ConsensusLiveness;
use tests_workflows::{ChaosBuilderExt as _, ScenarioBuilderExt as _};

const RUN_DURATION: Duration = Duration::from_secs(60);
const VALIDATORS: usize = 1;
const EXECUTORS: usize = 1;
const MIXED_TXS_PER_BLOCK: u64 = 5;
const TOTAL_WALLETS: usize = 64;
const TRANSACTION_WALLETS: usize = 8;

#[rustfmt::skip]
fn explicit_workload_plan() -> testing_framework_core::scenario::Builder<NodeControlCapability> {
    use testing_framework_workflows::workloads::{chaos::RandomRestartWorkload, da, transaction};

    let builder = ScenarioBuilder::with_node_counts(VALIDATORS, EXECUTORS).enable_node_control();

    let topology = builder
        .topology()
            .network_star()
            .validators(VALIDATORS)
            .executors(EXECUTORS)
        .apply();

    let chaos = RandomRestartWorkload::new(
        Duration::from_secs(45),
        Duration::from_secs(75),
        Duration::from_secs(120),
        true,
        true,
    );
    let tx = transaction::Workload::with_rate(MIXED_TXS_PER_BLOCK)
        .expect("transaction rate must be non-zero")
        .with_user_limit(Some(NonZeroUsize::new(TRANSACTION_WALLETS).unwrap()));

    let da_workload = da::Workload::with_channel_count(1);

    topology
        .with_workload(chaos)
        .with_workload(tx)
        .with_workload(da_workload)
        .with_run_duration(RUN_DURATION)
        .expect_consensus_liveness()
        .with_expectation(ConsensusLiveness::default())
}

#[rustfmt::skip]
fn full_plan() -> testing_framework_core::scenario::Builder<NodeControlCapability> {
    ScenarioBuilder::
         with_node_counts(VALIDATORS, EXECUTORS)
        .enable_node_control()
        // configure random restarts and schedule
        .chaos_random_restart()
            // earliest interval between restarts
            .min_delay(Duration::from_secs(45))
            // latest interval between restarts
            .max_delay(Duration::from_secs(75))
            // avoid restarting same node too soon
            .target_cooldown(Duration::from_secs(120))
        .apply()
        // shape the network layout
        .topology()
            // star network layout for libp2p topology
            .network_star()
            // validator count in the plan
            .validators(VALIDATORS)
            // executor count in the plan
            .executors(EXECUTORS)
            .apply()
        // seed wallet accounts
        .wallets(TOTAL_WALLETS)
        // transaction workload configuration
        .transactions()
            // submissions per block
           .rate(MIXED_TXS_PER_BLOCK)
            // number of unique wallet actors
           .users(TRANSACTION_WALLETS)
           .apply()
        // data-availability workload configuration
        .da()
            // channel operations per block
           .channel_rate(1)
            // number of blobs per channel
           .blob_rate(1)
           .apply()
        // run window and expectation
        .with_run_duration(RUN_DURATION)
        // assert consensus keeps up with workload
        .expect_consensus_liveness()
}

#[rustfmt::skip]
fn demo_plan() -> ScenarioBuilder {
    ScenarioBuilder::
         with_node_counts(VALIDATORS, EXECUTORS)
        .topology()
           .network_star()
           .validators(VALIDATORS)
           .executors(EXECUTORS)
           .apply()
        .wallets(TOTAL_WALLETS)
        .transactions()
           .rate(MIXED_TXS_PER_BLOCK)
           .users(TRANSACTION_WALLETS)
           .apply()
        .da()
           .channel_rate(1)
           .blob_rate(1)
           .apply()
        .with_run_duration(RUN_DURATION)
        .expect_consensus_liveness()
}

#[tokio::test]
async fn demo_local_runner_mixed_workloads() {
    let mut plan = demo_plan().build();

    let deployer = LocalDeployer::default();

    let runner: Runner = deployer.deploy(&plan).await.expect("scenario deployment");

    let _handle = runner
        .run(&mut plan)
        .await
        .expect("scenario should execute");
}

#[tokio::test]
async fn demo_compose_runner_tx_workload() {
    // Keep the explicit wiring example compiled and linted.
    let _ = explicit_workload_plan();

    let mut plan = full_plan().build();

    let deployer = ComposeRunner::default();

    let runner: Runner = deployer.deploy(&plan).await.expect("scenario deployment");

    let _handle = runner
        .run(&mut plan)
        .await
        .expect("compose scenario should execute");
}

#[tokio::test]
async fn demo_k8s_runner_tx_workload() {
    let mut plan = demo_plan().build();

    let deployer = K8sRunner::default();
    let runner: Runner = deployer.deploy(&plan).await.expect("scenario deployment");

    let _handle = runner
        .run(&mut plan)
        .await
        .expect("k8s scenario should execute");
}




════════════════════════════════════════════════
FILE: tests/workflows/tests/k8s_runner.rs
────────────────────────────────────────────────
use std::time::Duration;

use serial_test::serial;
use testing_framework_core::scenario::{Deployer as _, Runner, ScenarioBuilder};
use testing_framework_runner_k8s::{K8sRunner, K8sRunnerError};
use tests_workflows::ScenarioBuilderExt as _;

const RUN_DURATION: Duration = Duration::from_secs(60);
const VALIDATORS: usize = 1;
const EXECUTORS: usize = 1;
const MIXED_TXS_PER_BLOCK: u64 = 5;
const TOTAL_WALLETS: usize = 64;
const TRANSACTION_WALLETS: usize = 8;

#[tokio::test]
#[ignore = "requires access to a Kubernetes cluster"]
#[serial]
async fn k8s_runner_tx_workload() {
    let mut plan = ScenarioBuilder::with_node_counts(VALIDATORS, EXECUTORS)
        .topology()
        .network_star()
        .validators(VALIDATORS)
        .executors(EXECUTORS)
        .apply()
        .wallets(TOTAL_WALLETS)
        .transactions()
        .rate(MIXED_TXS_PER_BLOCK)
        .users(TRANSACTION_WALLETS)
        .apply()
        .da()
        .channel_rate(1)
        .blob_rate(1)
        .apply()
        .with_run_duration(RUN_DURATION)
        .expect_consensus_liveness()
        .build();

    let deployer = K8sRunner::new();
    let runner: Runner = match deployer.deploy(&plan).await {
        Ok(runner) => runner,
        Err(K8sRunnerError::ClientInit { source }) => {
            eprintln!("Skipping k8s_runner_tx_workload: Kubernetes cluster unavailable ({source})");
            return;
        }
        Err(err) => panic!("scenario deployment failed: {err}"),
    };

    let context = runner.context();
    assert!(
        context.telemetry().is_configured(),
        "k8s runner should expose prometheus metrics"
    );
    let validator_clients = context.node_clients().validator_clients().to_vec();

    let _handle = runner
        .run(&mut plan)
        .await
        .expect("k8s scenario should execute");

    for (idx, client) in validator_clients.iter().enumerate() {
        let info = client
            .consensus_info()
            .await
            .unwrap_or_else(|err| panic!("validator {idx} consensus_info failed: {err}"));
        assert!(
            info.height >= 5,
            "validator {idx} height {} should reach at least 5 blocks",
            info.height
        );
    }
}




════════════════════════════════════════════════
FILE: tests/workflows/tests/local_runner.rs
────────────────────────────────────────────────
use std::time::Duration;

use serial_test::serial;
use testing_framework_core::scenario::{Deployer as _, Runner, ScenarioBuilder};
use testing_framework_runner_local::LocalDeployer;
use tests_workflows::ScenarioBuilderExt as _;

const RUN_DURATION: Duration = Duration::from_secs(60);
const VALIDATORS: usize = 1;
const EXECUTORS: usize = 1;
const MIXED_TXS_PER_BLOCK: u64 = 5;
const TOTAL_WALLETS: usize = 64;
const TRANSACTION_WALLETS: usize = 8;

fn build_plan() -> testing_framework_core::scenario::Scenario {
    ScenarioBuilder::with_node_counts(VALIDATORS, EXECUTORS)
        .topology()
        .network_star()
        .validators(VALIDATORS)
        .executors(EXECUTORS)
        .apply()
        .wallets(TOTAL_WALLETS)
        .transactions()
        .rate(MIXED_TXS_PER_BLOCK)
        .users(TRANSACTION_WALLETS)
        .apply()
        .da()
        .channel_rate(1)
        .blob_rate(1)
        .apply()
        .with_run_duration(RUN_DURATION)
        .expect_consensus_liveness()
        .build()
}

#[tokio::test]
#[serial]
/// Drives both workloads concurrently to mimic a user mixing transaction flow
/// with blob publishing on the same topology.
async fn local_runner_mixed_workloads() {
    let mut plan = build_plan();
    let deployer = LocalDeployer::default();
    let runner: Runner = deployer.deploy(&plan).await.expect("scenario deployment");
    let _handle = runner.run(&mut plan).await.expect("scenario executed");
}



